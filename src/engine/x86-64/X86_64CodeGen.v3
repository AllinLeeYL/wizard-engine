// Copyright 2021 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

def R: X86_64Regs, G = X86_64Regs.GPRs, C: X86_64Conds;
// Configuration of live variables for the intepreter and baseline compiler.
enum IVar(entryParam: int, gpr: X86_64Gpr, frameOffset: int, baseline: bool) {
	INTERPRETER	(0, 	null,	0,	false),	// Interpreter object
	WFUNC		(1, 	null,	0,	false),	// WasmFunction object
	MEM0_BASE	(-1,	R.R10,	8,	true),	// base of memory #0
	TABLE0_BASE	(-1,	R.R11,	16,	true),	// base of table #0
	VFP		(2,	R.R11,	24,	false),	// value stack frame pointer
	VSP		(3,	R.RSI,	32,	false),	// value stack stack pointer
	XIP		(-1,	R.RBX,	40,	false),	// extended instruction pointer
	IP		(-1,	R.RAX,	48,	false),	// instruction pointer
	EIP		(-1,	R.R13,	56,	false),	// instruction pointer
	FUNC		(-1,	R.R12,	64,	true),	// FuncDecl
	INSTANCE	(-1,	R.RDI,	72,	true)	// Instance
}

// Register configuration and shorthand for codegen.
component I {
	def V3_PARAM_GPRS = [R.RDI, R.RSI, R.RDX, R.RCX, R.R8, R.R9]; 		// System-V
	def V3_RET_GPRS = [R.RAX, R.RDX, R.RCX, R.RSI]; 			// System-V + 2

	def INTERPRETER_GPRS = filterGprs(isTrue);	// allocatable interpreter registers
	def BASELINE_GPRS = filterGprs(IVar.baseline);	// allocatable baseline registers


	def filterGprs(cond: IVar -> bool) -> Array<X86_64Gpr> {
		var gprs = G;
		var used = Array<bool>.new(gprs.length);
		used[R.RSP.regnum] = true;
		for (v in IVar) {
			if (v.gpr != null && cond(v)) used[v.gpr.regnum] = true;
		}
		var v = Vector<X86_64Gpr>.new().grow(gprs.length);
		for (i < gprs.length) {
			if (!used[i]) v.put(gprs[i]);
		}
		return v.extract();
	}
	def isTrue(v: IVar) -> bool {
		return true;
	}

	// Shorthand for individual registers and addresses.
	def ip = R.RAX;
	def eip = R.R13;
	def ip_ptr = ip.indirect();
	def vsp = R.RSI;
	def vsp_ptr = vsp.indirect();
	def vsp_ptr2 = vsp.plus(PTR_SIZE);
	def vsp_ptr_m1 = vsp.plus(-PTR_SIZE);
	def vsp_ptr_m2 = vsp.plus(-SLOT_SIZE);
	def vsp_ptr_m3 = vsp.plus(-PTR_SIZE - SLOT_SIZE);
	def vsp_ptr_m4 = vsp.plus(-2 * SLOT_SIZE);
	def r0 = I.INTERPRETER_GPRS[0];
	def r1 = I.INTERPRETER_GPRS[1];
	def r2 = I.INTERPRETER_GPRS[2];
	def xmm0 = X86_64Regs.XMM0;
	def scratch = R.RBP;
}

def PTR_SIZE = Pointer.SIZE;
def SLOT_SIZE = 2 * Pointer.SIZE;
def SLOT_SIZE_LOG = u6.view(4);
// Code generator for X86-64, including the fast interpreter and baseline JIT.
class X86_64CodeGen {
	def w = DataWriter.new();
	def asm = X86_64Assemblers.create64(w);
	def frameSize = IVar.INSTANCE.frameOffset + PTR_SIZE;
	def offsets = V3Offsets.new();

	var threadedDispatch = false;

	new() { asm.patcher = asm.d.patcher = Patcher.new(w); }

	def genInterpreter() -> InterpreterCode {
		// Map some executable memory for the interpreter.
		def SIZE = 4096u;
		var mapping = Mmap.reserve(SIZE, Mmap.PROT_READ | Mmap.PROT_WRITE |  Mmap.PROT_EXEC);
		if (mapping == null) return null;
		var ic = InterpreterCode.new(mapping);
		// Reserve space for a 2x256-entry signed-offset dispatch table.
		asm.w.align(2);
		ic.dispatchTable.offset = w.pos;
		asm.w.skipN(256 * 2);
		// Then the interpreter entry
		genInterpreterEntry(ic);
		// Then opcode handlers
		genOpcodeHandlers(ic);
		ic.handlerEndOffset = w.pos;
		// Generate out-of-line code
		genOutOfLineLEBs(ic);
		ic.codeEndOffset = w.atEnd().pos;
		// Finished. Copy code from Datawriter into memory range.
		var p = mapping.range.start;
		w.atEnd();
		if (w.pos > SIZE) System.error("X86_64CodeGen", Strings.format2("need %d bytes for interpreter code, only allocated %d", w.pos, SIZE));
		for (i < w.pos) {
			p.store<u8>(w.data[i]);
			p++;
		}
		// Write-protect the executable code for security and debugging
		Mmap.protect(mapping.range.start, mapping.range.size(), Mmap.PROT_READ | Mmap.PROT_EXEC);
		return ic;
	}

	def genInterpreterEntry(ic: InterpreterCode) {
		// Generate code that can be entered directly from V3.
		ic.v3EntryOffset = w.atEnd().pos;
		// Allocate and initialize interpreter stack frame from incoming V3 params.
		asm.q.sub_r_i(R.RSP, frameSize);

		var iv: IVar;

		// Spill interpreter object
		iv = IVar.INTERPRETER;
		asm.movq_m_r(R.RSP.plus(iv.frameOffset), I.V3_PARAM_GPRS[iv.entryParam]);
		// Spill vfp (value frame pointer)
		iv = IVar.VFP;
		asm.movq_m_r(R.RSP.plus(iv.frameOffset), I.V3_PARAM_GPRS[iv.entryParam]);
		// Spill vsp (value stack pointer);
		iv = IVar.VSP;
		asm.movq_m_r(R.RSP.plus(iv.frameOffset), I.V3_PARAM_GPRS[iv.entryParam]);

		// Load wf.instance, wf.decl and spill
		var wfReg = I.V3_PARAM_GPRS[IVar.WFUNC.entryParam];
		asm.movq_r_m(I.scratch, X86_64Addr.new(wfReg, null, 1, offsets.WasmFunction_instance));
		asm.movq_m_r(R.RSP.plus(IVar.INSTANCE.frameOffset), I.scratch);
		asm.movq_r_m(I.scratch, X86_64Addr.new(wfReg, null, 1, offsets.WasmFunction_decl));
		asm.movq_m_r(R.RSP.plus(IVar.FUNC.frameOffset), I.scratch);

		// Load &func.code.code[0] into IP
		asm.movq_r_m(I.scratch, X86_64Addr.new(I.scratch, null, 1, offsets.FuncDecl_code));
		asm.movq_r_m(I.scratch, X86_64Addr.new(I.scratch, null, 1, offsets.Code_code));
		asm.lea(IVar.IP.gpr, X86_64Addr.new(I.scratch, null, 1, offsets.Array_contents));
		asm.movq_m_r(R.RSP.plus(IVar.IP.frameOffset), IVar.IP.gpr);
		// Load IP + code.length into EIP
		asm.movd_r_m(IVar.EIP.gpr, X86_64Addr.new(I.scratch, null, 1, offsets.Array_length));
		asm.q.add_r_r(IVar.EIP.gpr, IVar.IP.gpr);
		asm.movq_m_r(R.RSP.plus(IVar.EIP.frameOffset), IVar.EIP.gpr);

		// Move vals into correct registers.
		for (iv in [IVar.VSP, IVar.VFP, IVar.FUNC]) {
			asm.movq_r_m(iv.gpr, R.RSP.plus(iv.frameOffset));
		}

		ic.callEntryOffset = w.pos;
		// Decode locals and initialize them. (XXX: special-case 0 locals)
		var countGpr = I.r0;
		genReadUleb32(ic, countGpr, I.ip, I.scratch);
		var start = X86_64Label.new(), done = X86_64Label.new();
		// gen: if (count != 0) do
		asm.d.cmp_r_i(countGpr, 0);
		asm.jc_rel_near(C.Z, done);
		asm.bind(start);
		// gen: var num = read_uleb32()
		var numGpr = I.r1;
		genReadUleb32(ic, numGpr, I.ip, I.scratch);
		// gen: var type = read_type();
		var type_done = X86_64Label.new();
		var typeGpr = I.r2;
		asm.d.movbzx_r_m(typeGpr, I.ip_ptr);		// load first byte
		asm.q.inc_r(I.ip);				// increment pointer
		asm.d.test_r_i(typeGpr, 0x80);			// test most-significant bit
		asm.jc_rel_near(C.Z, type_done);	// more type LEB bytes?
		genSkipLeb(I.ip, I.scratch);			// only first byte matters
		asm.bind(type_done);

		// TODO: for RefNullT, AbstractT, skip additional ULEB

		// gen: if(num != 0) do
		var start2 = X86_64Label.new(), done2 = X86_64Label.new();
		asm.d.cmp_r_i(numGpr, 0);
		asm.jc_rel_near(C.Z, done2);
		asm.bind(start2);
		asm.movq_m_r(I.vsp_ptr, typeGpr);	// *(sp) = type
		asm.movq_m_i(I.vsp_ptr2, 0);		// *(sp + 8) = 0
		asm.add_r_i(I.vsp, SLOT_SIZE);		// sp += 16
		// gen: while (--num != 0)
		asm.d.dec_r(numGpr);
		asm.jc_rel_near(C.NZ, start2);

		// gen: while (--count != 0)
		asm.d.dec_r(countGpr);
		asm.jc_rel_near(C.NZ, start);
		asm.bind(done);

		// execute first instruction
		ic.firstDispatchOffset = w.atEnd().pos;
		genDispatch(ic);
	}

	// Generate all the opcode handlers.
	def genOpcodeHandlers(ic: InterpreterCode) {
		// Generate the default handler and initialize the table
		var pos = w.atEnd().pos;
		genDefaultOpcodeHandler(ic);
		for (i < 256) patchDispatchTable(ic, i, pos);

		// Generate handlers for all short opcodes
		for (opcode in Opcode) {  // XXX: order opcodes by frequency
			if (opcode.prefix != 0) continue;
			var pos = w.atEnd().pos;
			// try to generate the handler for the opcode
			var result = genOpcodeHandler(ic, opcode);
			match (result) {
				UNHANDLED => ; // already points to default handler
				HANDLED => {
					patchDispatchTable(ic, opcode.code, pos);
					if (threadedDispatch) {
						genDispatch(ic); // generates inline copy of dispatch
					} else {
						asm.jmp_rel(ic.firstDispatchOffset - w.atEnd().pos); // jump to dispatch (loop)
					}
				}
				END => {
					patchDispatchTable(ic, opcode.code, pos);
				}
			}
		}
		if (ic.returnSequenceOffset != 0) {
			patchDispatchTable(ic, Opcode.RETURN.code, ic.returnSequenceOffset);
		}
	}

	// Generate code for a unknown / unimplemented bytecode.
	def genDefaultOpcodeHandler(ic: InterpreterCode) {
		genAbruptReturn(ExecState.TRAPPED, TrapReason.UNIMPLEMENTED);
	}

	// Generate the code of a single opcode.
	def genOpcodeHandler(ic: InterpreterCode, opcode: Opcode) -> HandlerGenResult {
		match (opcode) {
			UNREACHABLE => {
				genAbruptReturn(ExecState.TRAPPED, TrapReason.UNREACHABLE);
				return HandlerGenResult.END;
			}
			NOP => {
				// nothing to do
			}
			BLOCK, LOOP, TRY => {
				// XXX: block, loop, and try can share a handler
				genSkipLeb(I.ip, I.scratch);
			}
			END => {
				asm.q.cmp_r_r(I.ip, I.eip);
				asm.jc_rel(C.L, ic.firstDispatchOffset - w.atEnd().pos); // jump to dispatch (loop)
				ic.returnSequenceOffset = w.pos; // Opcode.RETURN is patched to jump directly to here.
				// Load return count from func.sig.results.length
				var tmp = IVar.FUNC.gpr;
				asm.movd_r_m(tmp, X86_64Addr.new(IVar.FUNC.gpr, null, 1, offsets.FuncDecl_sig));
				asm.movd_r_m(tmp, X86_64Addr.new(tmp, null, 1, offsets.SigDecl_results));
				asm.movd_r_m(tmp, X86_64Addr.new(tmp, null, 1, offsets.Array_length));
				var done = X86_64Label.new();
				// Copy return value(s) from VSP to VFP.
				asm.cmp_r_i(tmp, 0);
				asm.jc_rel_near(C.Z, done);
				asm.d.shl_r_i(tmp, SLOT_SIZE_LOG);
				asm.q.sub_r_r(IVar.VSP.gpr, tmp);
				var loop = X86_64Label.new();
				// loop until tmp == 0, copying stack slots down to frame pointer.
				// XXX: use rep movsq with rcx, rsi and rdi?
				asm.bind(loop);
				asm.movdqu_s_m(I.xmm0, X86_64Addr.new(IVar.VSP.gpr, tmp, 1, - SLOT_SIZE));
				asm.movdqu_m_s(X86_64Addr.new(IVar.VFP.gpr, tmp, 1, - SLOT_SIZE), I.xmm0);
				asm.q.sub_r_i(tmp, SLOT_SIZE);
				asm.jc_rel_near(C.NZ, loop);
				asm.bind(done);
				// Deallocate interpreter frame and return to calling code.
				asm.q.add_r_i(R.RSP, frameSize);
				asm.movd_r_i(I.V3_RET_GPRS[0], ExecState.FINISHED.tag);
				asm.ret();
				return HandlerGenResult.END;
			}
			RETURN => {
				return HandlerGenResult.END; // patched manually
			}
			DROP => {
				asm.sub_r_i(I.vsp, SLOT_SIZE);
			}
			LOCAL_GET => {
				genReadUleb32(ic, I.r0, I.ip, I.scratch);
				asm.d.shl_r_i(I.r0, SLOT_SIZE_LOG);
				asm.movdqu_s_m(I.xmm0, X86_64Addr.new(IVar.VFP.gpr, I.r0, 1, 0));
				asm.movdqu_m_s(I.vsp_ptr, I.xmm0);
				asm.add_r_i(I.vsp, SLOT_SIZE);
			}
			LOCAL_SET => {
				genReadUleb32(ic, I.r0, I.ip, I.scratch);
				asm.d.shl_r_i(I.r0, SLOT_SIZE_LOG);
				asm.sub_r_i(I.vsp, SLOT_SIZE);
				asm.movq_r_m(I.r1, I.vsp_ptr2);
				asm.movq_m_r(X86_64Addr.new(IVar.VFP.gpr, I.r0, 1, 8), I.r1);
			}
			LOCAL_TEE => {
				genReadUleb32(ic, I.r0, I.ip, I.scratch);
				asm.d.shl_r_i(I.r0, SLOT_SIZE_LOG);
				asm.movq_r_m(I.r1, X86_64Addr.new(IVar.VSP.gpr, null, 1, 8 - SLOT_SIZE));
				asm.movq_m_r(X86_64Addr.new(IVar.VFP.gpr, I.r0, 1, 8), I.r1);
			}
			I32_CONST => {
				genReadSleb32_inline(ic, I.r1, I.ip, I.scratch); // TODO: r0 == ecx, which is overwritten
				asm.movq_m_i(I.vsp_ptr, i7.view(BpTypecon.I32.code));
				asm.movq_m_r(I.vsp_ptr2, I.r1);
				asm.add_r_i(I.vsp, SLOT_SIZE);
			}
			I64_CONST => {
				genReadSleb64_inline(ic, I.r1, I.ip, I.scratch); // TODO: r0 == ecx, which is overwritten
				asm.movq_m_i(I.vsp_ptr, i7.view(BpTypecon.I64.code));
				asm.movq_m_r(I.vsp_ptr2, I.r1);
				asm.add_r_i(I.vsp, SLOT_SIZE);
			}
			F32_CONST => {
				asm.movd_r_m(I.r0, I.ip_ptr);
				asm.add_r_i(I.ip, 4);
				asm.movq_m_i(I.vsp_ptr, i7.view(BpTypecon.F32.code));
				asm.movq_m_r(I.vsp_ptr2, I.r0);
				asm.add_r_i(I.vsp, SLOT_SIZE);
			}
			F64_CONST => {
				asm.movq_r_m(I.r0, I.ip_ptr);
				asm.add_r_i(I.ip, 8);
				asm.movq_m_i(I.vsp_ptr, i7.view(BpTypecon.F64.code));
				asm.movq_m_r(I.vsp_ptr2, I.r0);
				asm.add_r_i(I.vsp, SLOT_SIZE);
			}
			I32_EQZ => {
				asm.d.test_m_i(I.vsp_ptr_m1, -1);
				asm.set_r(C.Z, I.r0);
				asm.movbzx_r_r(I.r0, I.r0);
				asm.movd_m_r(I.vsp_ptr_m1, I.r0);
			}
			I32_EQ => genCmpI32(C.Z);
			I32_NE => genCmpI32(C.NZ);
			I32_LT_S => genCmpI32(C.L);
			I32_LT_U => genCmpI32(C.C);
			I32_GT_S => genCmpI32(C.G);
			I32_GT_U => genCmpI32(C.A);
			I32_LE_S => genCmpI32(C.LE);
			I32_LE_U => genCmpI32(C.NA);
			I32_GE_S => genCmpI32(C.GE);
			I32_GE_U => genCmpI32(C.NC);
			I64_EQZ => {
				asm.q.test_m_i(I.vsp_ptr_m1, -1);
				asm.set_r(C.Z, I.r0);
				asm.movbzx_r_r(I.r0, I.r0);
				asm.movd_m_r(I.vsp_ptr_m1, I.r0);
				asm.movd_m_i(I.vsp_ptr_m2, BpTypecon.I32.code);
			}
			I64_EQ => genCmpI64(C.Z);
			I64_NE => genCmpI64(C.NZ);
			I64_LT_S => genCmpI64(C.L);
			I64_LT_U => genCmpI64(C.C);
			I64_GT_S => genCmpI64(C.G);
			I64_GT_U => genCmpI64(C.A);
			I64_LE_S => genCmpI64(C.LE);
			I64_LE_U => genCmpI64(C.NA);
			I64_GE_S => genCmpI64(C.GE);
			I64_GE_U => genCmpI64(C.NC);
			I32_CLZ => {
				asm.movd_r_i(I.r1, -1);
				asm.d.bsr_r_m(I.r0, I.vsp_ptr_m1);
				asm.d.cmov_r(C.Z, I.r0, I.r1);
				asm.movd_r_i(I.r1, 31);
				asm.d.sub_r_r(I.r1, I.r0);
				asm.movd_m_r(I.vsp_ptr_m1, I.r1);
			}
			I32_CTZ => {
				asm.d.bsf_r_m(I.r0, I.vsp_ptr_m1);
				asm.movd_r_i(I.r1, 32);
				asm.d.cmov_r(C.Z, I.r0, I.r1);
				asm.movd_m_r(I.vsp_ptr_m1, I.r0);
			}
			I32_POPCNT => {
				asm.d.popcnt_r_m(I.r0, I.vsp_ptr_m1);
				asm.movd_m_r(I.vsp_ptr_m1, I.r0);
			}
			I32_ADD => genBinop32_m_r(asm.d.add_m_r);
			I32_SUB => genBinop32_m_r(asm.d.sub_m_r);
			I32_MUL => {
				asm.movd_r_m(I.r0, I.vsp_ptr_m1);
				asm.d.imul_r_m(I.r0, I.vsp_ptr_m3);
				asm.movd_m_r(I.vsp_ptr_m3, I.r0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			I32_AND => genBinop32_m_r(asm.d.and_m_r);
			I32_OR => genBinop32_m_r(asm.d.or_m_r);
			I32_XOR => genBinop32_m_r(asm.d.xor_m_r);
			I32_SHL => genBinop32_m_cl(asm.d.shl_m_cl);
			I32_SHR_S => genBinop32_m_cl(asm.d.sar_m_cl);
			I32_SHR_U => genBinop32_m_cl(asm.d.shr_m_cl);
			I32_ROTL => genBinop32_m_cl(asm.d.rol_m_cl);
			I32_ROTR => genBinop32_m_cl(asm.d.ror_m_cl);

			I64_CLZ => {
				asm.movq_r_i(I.r1, -1);
				asm.q.bsr_r_m(I.r0, I.vsp_ptr_m1);
				asm.q.cmov_r(C.Z, I.r0, I.r1);
				asm.movd_r_i(I.r1, 63);
				asm.q.sub_r_r(I.r1, I.r0);
				asm.movq_m_r(I.vsp_ptr_m1, I.r1);
			}
			I64_CTZ => {
				asm.q.bsf_r_m(I.r0, I.vsp_ptr_m1);
				asm.movd_r_i(I.r1, 64);
				asm.q.cmov_r(C.Z, I.r0, I.r1);
				asm.movq_m_r(I.vsp_ptr_m1, I.r0);
			}
			I64_POPCNT => {
				asm.q.popcnt_r_m(I.r0, I.vsp_ptr_m1);
				asm.movq_m_r(I.vsp_ptr_m1, I.r0);
			}
			I64_ADD => genBinop64_m_r(asm.q.add_m_r);
			I64_SUB => genBinop64_m_r(asm.q.sub_m_r);
			I64_MUL => {
				asm.movq_r_m(I.r0, I.vsp_ptr_m1);
				asm.q.imul_r_m(I.r0, I.vsp_ptr_m3);
				asm.movq_m_r(I.vsp_ptr_m3, I.r0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			I64_AND => genBinop64_m_r(asm.q.and_m_r);
			I64_OR => genBinop64_m_r(asm.q.or_m_r);
			I64_XOR => genBinop64_m_r(asm.q.xor_m_r);
			I64_SHL => genBinop64_m_cl(asm.q.shl_m_cl);
			I64_SHR_S => genBinop64_m_cl(asm.q.sar_m_cl);
			I64_SHR_U => genBinop64_m_cl(asm.q.shr_m_cl);
			I64_ROTL => genBinop64_m_cl(asm.q.rol_m_cl);
			I64_ROTR => genBinop64_m_cl(asm.q.ror_m_cl);
			F32_ADD => {
				asm.movss_s_m(I.xmm0, I.vsp_ptr_m3);
				asm.addss_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movss_m_s(I.vsp_ptr_m3, I.xmm0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			F32_SUB => {
				asm.movss_s_m(I.xmm0, I.vsp_ptr_m3);
				asm.subss_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movss_m_s(I.vsp_ptr_m3, I.xmm0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			F32_MUL => {
				asm.movss_s_m(I.xmm0, I.vsp_ptr_m3);
				asm.mulss_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movss_m_s(I.vsp_ptr_m3, I.xmm0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			F32_DIV => {
				asm.movss_s_m(I.xmm0, I.vsp_ptr_m3);
				asm.divss_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movss_m_s(I.vsp_ptr_m3, I.xmm0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			F64_ADD => {
				asm.movsd_s_m(I.xmm0, I.vsp_ptr_m3);
				asm.addsd_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movsd_m_s(I.vsp_ptr_m3, I.xmm0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			F64_SUB => {
				asm.movsd_s_m(I.xmm0, I.vsp_ptr_m3);
				asm.subsd_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movsd_m_s(I.vsp_ptr_m3, I.xmm0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			F64_MUL => {
				asm.movsd_s_m(I.xmm0, I.vsp_ptr_m3);
				asm.mulsd_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movsd_m_s(I.vsp_ptr_m3, I.xmm0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			F64_DIV => {
				asm.movsd_s_m(I.xmm0, I.vsp_ptr_m3);
				asm.divsd_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movsd_m_s(I.vsp_ptr_m3, I.xmm0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			I32_REINTERPRET_F32 => {
				asm.movq_m_i(I.vsp_ptr_m2, BpTypecon.I32.code); // update tag
			}
			I64_REINTERPRET_F64 => {
				asm.movq_m_i(I.vsp_ptr_m2, BpTypecon.I64.code); // update tag
			}
			F32_REINTERPRET_I32 => {
				asm.movq_m_i(I.vsp_ptr_m2, BpTypecon.F32.code); // update tag
			}
			F64_REINTERPRET_I64 => {
				asm.movq_m_i(I.vsp_ptr_m2, BpTypecon.F64.code); // update tag
			}
			I32_EXTEND8_S => {
				asm.d.movbsx_r_m(I.r0, I.vsp_ptr_m1);
				asm.movd_m_r(I.vsp_ptr_m1, I.r0);
			}
			I32_EXTEND16_S => {
				asm.d.movwsx_r_m(I.r0, I.vsp_ptr_m1);
				asm.movd_m_r(I.vsp_ptr_m1, I.r0);
			}
			I64_EXTEND8_S => {
				asm.q.movbsx_r_m(I.r0, I.vsp_ptr_m1);
				asm.movq_m_r(I.vsp_ptr_m1, I.r0);
			}
			I64_EXTEND16_S => {
				asm.q.movwsx_r_m(I.r0, I.vsp_ptr_m1);
				asm.movq_m_r(I.vsp_ptr_m1, I.r0);
			}
			I64_EXTEND32_S => {
				asm.movd_r_m(I.r0, I.vsp_ptr_m1);
				asm.q.shl_r_i(I.r0, 32);
				asm.q.sar_r_i(I.r0, 32);
				asm.movq_m_r(I.vsp_ptr_m1, I.r0);
			}

			_ => return HandlerGenResult.UNHANDLED;
		}
		return HandlerGenResult.HANDLED;
	}
	def genBinop32_m_r(gen: (X86_64Addr, X86_64Gpr) -> X86_64Assembler) {
		asm.movd_r_m(I.r0, I.vsp_ptr_m1);
		gen(I.vsp_ptr_m3, I.r0);
		asm.q.sub_r_i(I.vsp, SLOT_SIZE);
	}
	def genBinop32_m_cl(gen: X86_64Addr -> X86_64Assembler) {
		asm.movd_r_m(I.r0, I.vsp_ptr_m1);
		gen(I.vsp_ptr_m3);
		asm.q.sub_r_i(I.vsp, SLOT_SIZE);
	}
	def genBinop64_m_r(gen: (X86_64Addr, X86_64Gpr) -> X86_64Assembler) {
		asm.movq_r_m(I.r0, I.vsp_ptr_m1);
		gen(I.vsp_ptr_m3, I.r0);
		asm.q.sub_r_i(I.vsp, SLOT_SIZE);
	}
	def genBinop64_m_cl(gen: X86_64Addr -> X86_64Assembler) {
		asm.movq_r_m(I.r0, I.vsp_ptr_m1);
		gen(I.vsp_ptr_m3);
		asm.q.sub_r_i(I.vsp, SLOT_SIZE);
	}
	def genCmpI32(cond: X86_64Cond) {
		asm.movd_r_m(I.r0, I.vsp_ptr_m1);
		asm.d.cmp_m_r(I.vsp_ptr_m3, I.r0);
		asm.set_r(cond, I.r0);
		asm.movbzx_r_r(I.r0, I.r0);
		asm.movd_m_r(I.vsp_ptr_m3, I.r0);
		asm.q.sub_r_i(I.vsp, SLOT_SIZE);
	}
	def genCmpI64(cond: X86_64Cond) {
		asm.movq_r_m(I.r0, I.vsp_ptr_m1);
		asm.q.cmp_m_r(I.vsp_ptr_m3, I.r0);
		asm.set_r(cond, I.r0);
		asm.movbzx_r_r(I.r0, I.r0);
		asm.movq_m_r(I.vsp_ptr_m3, I.r0);
		asm.movq_m_i(I.vsp_ptr_m4, BpTypecon.I32.code);
		asm.q.sub_r_i(I.vsp, SLOT_SIZE);
	}
	def genAbruptReturn(state: ExecState, reason: TrapReason) {
		asm.q.add_r_i(R.RSP, frameSize);
		asm.movd_r_i(I.V3_RET_GPRS[0], state.tag);
		asm.movd_r_i(I.V3_RET_GPRS[1], reason.tag);
		asm.ret();
	}

	// Generate a read of a 32-bit unsigned LEB.
	def genReadUleb32(ic: InterpreterCode, dest: X86_64Gpr, ptr: X86_64Gpr, scratch: X86_64Gpr) {
		var ool_leb = OutOfLineLEB.new(dest);
		ic.oolULeb32Sites.put(ool_leb);
		var asm = this.asm.d;
		asm.movbzx_r_m(dest, ptr.indirect());	// load first byte
		asm.q.inc_r(ptr);			// increment pointer
		asm.test_r_i(dest, 0x80);		// test most-significant bit
		asm.jc_rel_addr(C.NZ, ool_leb);
		ool_leb.retOffset = asm.pos();
	}
	// Generate a read of a 32-bit signed LEB.
	def genReadSleb32_inline(ic: InterpreterCode, dest: X86_64Gpr, ptr: X86_64Gpr, scratch: X86_64Gpr) {
		var done = X86_64Label.new(), sext = X86_64Label.new(), loop = X86_64Label.new();
		asm.movd_r_i(R.RCX, 0);
		asm.movd_r_i(dest, 0);

		asm.bind(loop);
		asm.movbzx_r_m(scratch, ptr.indirect());	// load byte
		asm.q.inc_r(ptr);				// increment pointer
		asm.d.test_r_i(scratch, 0x80);			// test most-significant bit
		asm.jc_rel_near(C.Z, sext);		// break if not set
		asm.d.and_r_i(scratch, 0x7F);			// mask off upper bit
		asm.d.shl_r_cl(scratch);			// shift byte into correct bit pos
		asm.d.or_r_r(dest, scratch);			// merge byte into val
		asm.d.add_r_i(R.RCX, 7);				// compute next bit pos
		asm.jmp_rel_near(loop);				// loop

		asm.bind(sext);
		asm.d.shl_r_cl(scratch);			// shift byte into correct bit pos
		asm.d.or_r_r(dest, scratch);			// merge byte into val
		asm.d.sub_r_i(R.RCX, 25);				// compute 32 - shift
		asm.d.neg_r(R.RCX);
		asm.jc_rel_near(C.S, done);		// if shift > 32, done
		asm.d.shl_r_cl(dest);				// sign extension
		asm.d.sar_r_cl(dest);
		asm.bind(done);
	}
	// Generate a read of a 32-bit signed LEB.
	def genReadSleb64_inline(ic: InterpreterCode, dest: X86_64Gpr, ptr: X86_64Gpr, scratch: X86_64Gpr) {
		var done = X86_64Label.new(), sext = X86_64Label.new(), loop = X86_64Label.new();
		asm.movd_r_i(R.RCX, 0);
		asm.movd_r_i(dest, 0);

		asm.bind(loop);
		asm.movbzx_r_m(scratch, ptr.indirect());	// load byte
		asm.q.inc_r(ptr);				// increment pointer
		asm.d.test_r_i(scratch, 0x80);			// test most-significant bit
		asm.jc_rel_near(C.Z, sext);		// break if not set
		asm.d.and_r_i(scratch, 0x7F);			// mask off upper bit
		asm.q.shl_r_cl(scratch);			// shift byte into correct bit pos
		asm.q.or_r_r(dest, scratch);			// merge byte into val
		asm.d.add_r_i(R.RCX, 7);			// compute next bit pos
		asm.jmp_rel_near(loop);				// loop

		asm.bind(sext);
		asm.q.shl_r_cl(scratch);			// shift byte into correct bit pos
		asm.q.or_r_r(dest, scratch);			// merge byte into val
		asm.d.sub_r_i(R.RCX, 57);			// compute 67 - shift
		asm.d.neg_r(R.RCX);
		asm.jc_rel_near(C.S, done);		// if shift > 64, done
		asm.q.shl_r_cl(dest);				// sign extension
		asm.q.sar_r_cl(dest);
		asm.bind(done);
	}
	// Generate code which skips over an LEB.
	def genSkipLeb(ptr: X86_64Gpr, scratch: X86_64Gpr) {
		var more = X86_64Label.new();
		asm.bind(more);
		asm.movbzx_r_m(scratch, ptr.indirect());	// load first byte
		asm.q.inc_r(ptr);				// increment pointer
		asm.test_r_i(scratch, 0x80);			// test most-significant bit
		asm.jc_rel_near(C.NZ, more);
	}
	// Generate a load of the next bytecode and a dispatch through the dispatch table.
	def genDispatch(ic: InterpreterCode) {
		var opcode = I.r0;
		var base = I.r1;
		asm.movbzx_r_m(opcode, I.ip_ptr);
		asm.inc_r(I.ip);
		asm.lea(base, ic.dispatchTable); // RIP-relative LEA
		asm.movwsx_r_m(opcode, X86_64Addr.new(base, opcode, 2, 0)); // load 16-bit offset
		asm.add_r_r(base, opcode);
		asm.ijmp_r(base);
	}
	// Patch the dispatch table for the given opcode to go to the given position.
	def patchDispatchTable(ic: InterpreterCode, opcode: int, pos: int) {
		var offset = pos - ic.dispatchTable.offset;
		w.at(ic.dispatchTable.offset + 2 * opcode).put_b16(offset);
		w.atEnd();
	}
	// Generate the out-of-line LEB decoding code.
	def genOutOfLineLEBs(ic: InterpreterCode) {
		for (i < ic.oolULeb32Sites.length) {
			var o = ic.oolULeb32Sites[i];
			var pos = w.atEnd().pos;
			w.at(o.pos).put_b32(pos - (o.pos + o.delta));
			// TODO: gen code
		}
		ic.oolULeb32Sites = null;
		for (i < ic.oolSLeb32Sites.length) {
			var o = ic.oolSLeb32Sites[i];
			var pos = w.atEnd().pos;
			w.at(o.pos).put_b32(pos - (o.pos + o.delta));
			// TODO: gen code
		}
		ic.oolSLeb32Sites = null;
	}
}
enum HandlerGenResult { UNHANDLED, HANDLED, END }

// Assembler patching support for out-of-line LEBs and the dispatch table.
def ABS_MARKER = 0x55443322;
def REL_MARKER = 0x44332211;
class OutOfLineLEB(dest: X86_64Gpr) extends X86_64Addr {
	var retOffset: int; // where OOB code should "return"
	var pos: int = -1;
	var delta: int;

	new() super(null, null, 1, REL_MARKER) { }
}
class DispatchTableRef extends X86_64Addr {
	var offset: int = -1;

	new() super(null, null, 1, REL_MARKER) { }
}
class Patcher(w: DataWriter) extends X86_64AddrPatcher {
	new() super(ABS_MARKER, REL_MARKER) { }
	def recordRel32(pos: int, delta: int, addr: X86_64Addr) {
		match (addr) {
			x: OutOfLineLEB => {
				x.pos = pos;
				x.delta = delta;
			}
			x: DispatchTableRef => {
				if (x.offset < 0) System.error("InterpreterGen", "dispatch table not fixed");
				w.at(pos).put_b32(x.offset - (pos + delta));
				w.atEnd();
			}
		}
	}
}