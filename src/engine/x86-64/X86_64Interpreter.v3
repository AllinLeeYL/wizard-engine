// Copyright 2021 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

// Implements a Wasm interpreter by running handwritten x86-64 interpreter loop.
def GLOBAL_BUFFER_MARKER = 0x7ACEBEEF778899AA;
def INTERPRETER_CODE_MARKER = 0x7FAACCEE;
def GLOBAL_BUFFER_HEADER_SIZE = 64;
def tuning = X86_64InterpreterTuning.new();
def DEBUG = false;
def FINISHED: AbruptReturn = null;
component X86_64Interpreter {
	var interpreterCode: X86_64InterpreterCode; // Dynamically-generated interpreter code.
	var asmEntry: (/*wf: */ WasmFunction, /*sp: */ Pointer) -> AbruptReturn;
	var valueStack: ValueStack;
	var dispatchTable: Pointer;

	def run(f: Function, args: Array<Value>) -> Result {
		if (interpreterCode == null) genInterpreterCode();
		// Unpack arguments into value stack format.
		if (args != null) for (v in args) valueStack.push(v);

		// Call the main loop which handles tail calls.
		var ret = runWithTailCalls(f, Pointer.NULL);
		// Unpack the abrupt return value into a {Result}.
		match (ret) {
			null => return popResult(f.sig);
			x: Trap => return Result.Trap(x);
			_ => return Result.Break;
		}
	}
	def serializeInterpreterCodeIntoExecutable(executable: Array<byte>) -> bool {
		if (interpreterCode == null) genInterpreterCode();
		// try to find {global_buffer} in {data}
		var d = DataReader.new(executable);
		// the global buffer contents will be at the same page alignment in the executable
		var page_offset = int.!((Pointer.atContents(global_buffer) - Pointer.NULL) & (PAGE_SIZE - 1));
		var found = -1;
		for (pos = page_offset; pos < d.limit; pos += PAGE_SIZE) {
			var val = d.at(pos).read_u64();
			if (val == GLOBAL_BUFFER_MARKER) { found = pos; break; }
		}
		if (found < 0) return false;
		// Write the executable code and the offsets of {InterpreterCode} into the executable.
		var w = DataWriter.new().reset(executable, found, found);
		w.puta(global_buffer); // write machine code
		interpreterCode.serialize(w.at(found + 8)); // write interpreter offsets
		return true;
	}
	private def genInterpreterCode() {
		valueStack = ValueStack.new(EngineOptions.stackSize);
		var start = System.ticksUs();
		interpreterCode = deserializeOrGenerateInterpreterCode();
		RiRuntime.registerUserCode(interpreterCode);
		var diff = System.ticksUs() - start;
		if (DEBUG) Trace.OUT.put1("Created interpreter in %d \xCE\xBCs.\n", diff).outln();
		asmEntry = CiRuntime.forgeClosure<
			X86_64Interpreter,				// closure type
			(/*wf: */ WasmFunction, /*sp: */ Pointer),	// parameter types
			AbruptReturn>(			// return types
				interpreterCode.start + interpreterCode.v3EntryOffset, this);
		dispatchTable = interpreterCode.start +
			if(Execute.probes.elem != null,
				interpreterCode.probedDispatchTableOffset,
				interpreterCode.fastDispatchTableOffset);
	}
	def onProbeEnable() {
		if (interpreterCode != null) dispatchTable = interpreterCode.start + interpreterCode.probedDispatchTableOffset;
	}
	def onProbeDisable() {
		if (interpreterCode != null) dispatchTable = interpreterCode.start + interpreterCode.fastDispatchTableOffset;
	}
	def reset() {
		if (valueStack != null) valueStack.sp = valueStack.mapping.range.start;
	}
	def getFrameAccessor(sp: Pointer) -> X86_64InterpreterFrameAccessor {
		var retip = (sp + -Pointer.SIZE).load<Pointer>();
		if (!inCode(retip)) return null;
		if (tuning.cacheFrameAccessor) {
			var prev = (sp + IVarConfig.frame.ACCESSOR.disp).load<X86_64InterpreterFrameAccessor>();
			if (prev != null) return prev;
			var n = X86_64InterpreterFrameAccessor.new(sp);
			(sp + IVarConfig.frame.ACCESSOR.disp).store<X86_64InterpreterFrameAccessor>(n);
			return n;
		}
		return X86_64InterpreterFrameAccessor.new(sp);
	}
	def inCode(p: Pointer) -> bool {
		return p >= interpreterCode.start && p < interpreterCode.end;
	}
	private def runWithTailCalls(f: Function, sp: Pointer) -> AbruptReturn {
		var state = FINISHED;
		while (true) { // handle repeated tail calls
			var result: HostResult;
			match (f) {
				wf: WasmFunction => {
					var sp = valueStack.sp;
					var ret = invoke(wf, sp);
					if (ret == null) {
						valueStack.sp = sp + ((wf.sig.results.length - wf.sig.params.length) * valueStack.valuerep.slot_size);
					}
					return ret;
				}
				hf: HostFunction => {
					if (Trace.interpreter) Execute.traceCallHostFunction(hf);
					var result = doInvokeHostFunction(hf);
					match (result) {
						Trap(trap) => {
							if (sp != Pointer.NULL) prependFrames(trap, hf, sp);
							state = trap;
							break;
						}
						Value0 => {
							break;
						}
						Value1(val) => {
							valueStack.push(val);
							break;
						}
						ValueN(vals) => {
							for (a in vals) valueStack.push(a);
							break;
						}
						TailCall(target, args) => {
							for (a in args) valueStack.push(a);
							f = target;
							continue; // continue with next tail call
						}
					}
				}
			}
		}
		return state;
	}
	private def doInvokeHostFunction(hf: HostFunction) -> HostResult {
		var result: HostResult;
		match (hf) {
			hf: HostFunction0 => {
				result = hf.invoke0();
			}
			hf: HostFunction1 => {
				var a0 = valueStack.pop(hf.sig.params[0]);
				result = hf.invoke1(a0);
			}
			hf: HostFunction2 => {
				var a1 = valueStack.pop(hf.sig.params[1]);
				var a0 = valueStack.pop(hf.sig.params[0]);
				result = hf.invoke2(a0, a1);
			}
			hf: HostFunction3 => {
				var a2 = valueStack.pop(hf.sig.params[2]);
				var a1 = valueStack.pop(hf.sig.params[1]);
				var a0 = valueStack.pop(hf.sig.params[0]);
				result = hf.invoke3(a0, a1, a2);
			}
			hf: HostFunctionN => {
				var aN = valueStack.popN(hf.sig.params);
				result = hf.invokeN(aN);
			}
			_ => {
				return HostResult.Trap(Trap.new(TrapReason.ERROR, "invalid host function", null));
			}
		}
		return result;
	}
	private def invoke(wf: WasmFunction, sp: Pointer) -> AbruptReturn {
		if (wf.decl.target_code.spc_entry != Pointer.NULL) {
			return Target.callSpcEntry(wf);
		}
		return asmEntry(wf, sp);
	}
	private def popResult(sig: SigDecl) -> Result {
		var rt = sig.results;
		var r = Array<Value>.new(rt.length);
		for (i = r.length - 1; i >= 0; i--) r[i] = valueStack.pop(rt[i]);
		return Result.Value(r);
	}
	// =======================================================================================
	// INTERPRETER RUNTIME CALLBACKS
	// =======================================================================================
	def runtime_callHost(f: Function) -> AbruptReturn {
		var state = runWithTailCalls(f, CiRuntime.callerSp());
		return state;
	}
	def runtime_MEMORY_GROW(instance: Instance, index: u32) {
		var memory = instance.memories[index];
		var pages = Values.v_u(valueStack.pop(ValueType.I32));
		var result = memory.grow(pages);
		valueStack.push(Values.i_v(result));
	}
	def runtime_MEMORY_INIT(instance: Instance, dindex: u32, mindex: u32) -> AbruptReturn {
		var memory = instance.memories[mindex];
		var ddecl = if(!instance.dropped_data[dindex], instance.module.data[int.!(dindex)]);
		var size = valueStack.popu();
		var src_offset = valueStack.popu();
		var dst_offset = valueStack.popu();
		var t = memory.copyIn(dst_offset, if(ddecl != null, ddecl.data), src_offset, size);
		if (t != TrapReason.NONE) return doTrap(t, CiRuntime.callerSp());
		return FINISHED;
	}
	def runtime_MEMORY_COPY(instance: Instance, mindex1: u32, mindex2: u32) -> AbruptReturn { // XXX: inline
		var dst = instance.memories[mindex1];
		var src = instance.memories[mindex2];
		var size = valueStack.popu();
		var src_offset = valueStack.popu();
		var dst_offset = valueStack.popu();
		var t = dst.copyM(dst_offset, src, src_offset, size);
		if (t != TrapReason.NONE) return doTrap(t, CiRuntime.callerSp());
		return FINISHED;
	}
	def runtime_MEMORY_FILL(instance: Instance, mindex: u32) -> AbruptReturn {
		var memory = instance.memories[mindex];
		var size = valueStack.popu();
		var val = valueStack.popu();
		var dest = valueStack.popu();
		var t = memory.fill(dest, u8.view(val), size);
		if (t != TrapReason.NONE) return doTrap(t, CiRuntime.callerSp());
		return FINISHED;
	}
	def runtime_GLOBAL_GET(instance: Instance, index: u32) { // XXX: inline when Value rep known
		var val = instance.globals[index].value;
		valueStack.push(val);
	}
	def runtime_GLOBAL_SET(instance: Instance, index: u32) { // XXX: inline when Value rep known
		var g = instance.globals[index];
		var val = valueStack.pop(g.valtype);
		g.value = val;
	}
	def runtime_TABLE_GET(instance: Instance, index: u32) -> AbruptReturn { // XXX: inline when Value rep known
		var table = instance.tables[index];
		var elem = Values.v_u(valueStack.pop(ValueType.I32));
		if (elem >= table.elems.length) return doTrap(TrapReason.TABLE_OUT_OF_BOUNDS, CiRuntime.callerSp());
		var val = table.elems[elem];
		valueStack.push(val);
		return FINISHED;
	}
	def runtime_TABLE_SET(instance: Instance, index: u32) -> AbruptReturn {
		var table = instance.tables[index];
		var val = valueStack.pop(table.elemtype);
		var elem = Values.v_u(valueStack.pop(ValueType.I32));
		if (elem >= table.elems.length) return doTrap(TrapReason.TABLE_OUT_OF_BOUNDS, CiRuntime.callerSp());
		table[int.view(elem)] = val;
		return FINISHED;
	}
	def runtime_TABLE_INIT(instance: Instance, eindex: u32, tindex: u32) -> AbruptReturn {
		var elem = if (!instance.dropped_elems[eindex], instance.module.elems[int.!(eindex)]);
		var table = instance.tables[tindex];
		var size = valueStack.popu();
		var src_offset = valueStack.popu();
		var dst_offset = valueStack.popu();
		var t = table.copyE(instance, dst_offset, elem, src_offset, size);
		if (t != TrapReason.NONE) return doTrap(t, CiRuntime.callerSp());
		return FINISHED;
	}
	def runtime_TABLE_COPY(instance: Instance, t1: u32, t2: u32) -> AbruptReturn {
		var dst = instance.tables[t1];
		var src = instance.tables[t2];
		var size = valueStack.popu(), src_offset = valueStack.popu(), dst_offset = valueStack.popu();
		var t = dst.copyT(dst_offset, src, src_offset, size);
		if (t != TrapReason.NONE) return doTrap(t, CiRuntime.callerSp());
		return FINISHED;
	}
	def runtime_TABLE_GROW(instance: Instance, tindex: u32) {
		var table = instance.tables[tindex];
		var size = valueStack.popu();
		var val = valueStack.pop(table.elemtype);
		var r = table.grow(size, val);
		valueStack.push(Values.i_v(r));
	}
	def runtime_TABLE_FILL(instance: Instance, tindex: u32) -> AbruptReturn { // XXX: inline when Value rep known
		var table = instance.tables[tindex];
		var size = valueStack.popu();
		var val = valueStack.pop(table.elemtype);
		var dest = valueStack.popu();
		var t = table.fill(dest, val, size);
		if (t != TrapReason.NONE) return doTrap(t, CiRuntime.callerSp());
		return FINISHED;
	}
	def runtime_PROBE_loop(func: WasmFunction, pc: int) -> AbruptReturn {
		var frame = TargetFrame(CiRuntime.callerSp());
		var ret = Execute.fireProbes(func, pc, frame);
		if (Trap.?(ret)) return prependFrames(Trap.!(ret), null, CiRuntime.callerSp());
		return ret;
	}
	def runtime_PROBE_instr(func: WasmFunction, pc: int) -> AbruptReturn {
		var frame = TargetFrame(CiRuntime.callerSp());
		var ret = Execute.fireProbesAt(func, pc, frame);
		if (Trap.?(ret)) return prependFrames(Trap.!(ret), null, CiRuntime.callerSp());
		return ret;
	}
	def runtime_TRAP(func: WasmFunction, pc: int, reason: TrapReason) -> AbruptReturn {
		if (DEBUG) Trace.OUT.put3("runtime_TRAP(%q, @+%d, %s)", func.render, pc, reason.name).outln();
		var trap = Trap.new(reason, null, null);
		return prependFrames(trap, null, CiRuntime.callerSp());
	}
	def prependFrames(trap: Trap, hf: HostFunction, sp: Pointer) -> Trap {
		var frame = TargetFrame(sp);
		var trace = Vector<(WasmFunction, int)>.new();
		var accessor = frame.getFrameAccessor();
		while (accessor != null) {
			trace.put(accessor.func(), accessor.pc());
			var c = accessor.caller();
			if (c.func == null) break;
			accessor = c.frame.getFrameAccessor(); // XXX: get frame without inflating accessor
		}
		var result = Array<(WasmFunction, int)>.new(trace.length); // reverse frames
		for (i < result.length) {
			result[i] = trace[result.length - i - 1];
		}
		trap.prepend(result, hf);
		return trap;
	}
	def doTrap(reason: TrapReason, sp: Pointer) -> AbruptReturn {
		var trap = Trap.new(reason, null, null);
		return prependFrames(trap, null, sp);
	}
}
def computePCFromFrame(sp: Pointer) -> int {
	if (DEBUG) dumpFrame(sp);
	if (tuning.recordCurIpForTraps) return (sp + IVarConfig.frame.CURPC.disp).load<int>();
	var ip   = (sp + IVarConfig.frame.IP.disp).load<Pointer>();
	var code = Pointer.atContents((sp + IVarConfig.frame.CODE.disp).load<Array<byte>>());
	return int.!(ip - code - 1);
}
def dumpFrame(sp: Pointer) {
	Trace.OUT.put1("WASM_FUNC = %x\n", (sp + IVarConfig.frame.WASM_FUNC.disp).load<u64>());
	Trace.OUT.put1("MEM0_BASE = %x\n", (sp + IVarConfig.frame.MEM0_BASE.disp).load<u64>());
	Trace.OUT.put1("VFP       = %x\n", (sp + IVarConfig.frame.VFP.disp).load<u64>());
	Trace.OUT.put1("VSP       = %x\n", (sp + IVarConfig.frame.VSP.disp).load<u64>());
	Trace.OUT.put1("SIDETABLE = %x\n", (sp + IVarConfig.frame.SIDETABLE.disp).load<u64>());
	Trace.OUT.put1("STP       = %x\n", (sp + IVarConfig.frame.STP.disp).load<u64>());
	Trace.OUT.put1("CODE      = %x\n", (sp + IVarConfig.frame.CODE.disp).load<u64>());
	Trace.OUT.put1("IP        = %x\n", (sp + IVarConfig.frame.IP.disp).load<u64>());
	Trace.OUT.put1("EIP       = %x\n", (sp + IVarConfig.frame.EIP.disp).load<u64>());
	Trace.OUT.put1("FUNC_DECL = %x\n", (sp + IVarConfig.frame.FUNC_DECL.disp).load<u64>());
	Trace.OUT.put1("INSTANCE  = %x\n", (sp + IVarConfig.frame.INSTANCE.disp).load<u64>());
	Trace.OUT.put1("CURPC     = %x\n", (sp + IVarConfig.frame.CURPC.disp).load<u64>());
	Trace.OUT.put1("ACCESSOR  = %x\n", (sp + IVarConfig.frame.ACCESSOR.disp).load<u64>());
	Trace.OUT.outln();
}

// FrameAccessor for probing and debugging.
class X86_64InterpreterFrameAccessor(sp: Pointer) extends FrameAccessor {
	var cached_depth = -1;
	var decl: FuncDecl;

	new() {
		decl = (sp + IVarConfig.frame.FUNC_DECL.disp).load<FuncDecl>();
	}
	// Returns {true} if this frame has been unwound, either due to returning, a trap, or exception.
	def isUnwound() -> bool {
		if (!tuning.cacheFrameAccessor) return false; // TODO: proper unwound check
		return this != (sp + IVarConfig.frame.ACCESSOR.disp).load<X86_64InterpreterFrameAccessor>();
	}
	// Returns the Wasm function in this frame.
	def func() -> WasmFunction {
		checkNotUnwound();
		return (sp + IVarConfig.frame.WASM_FUNC.disp).load<WasmFunction>();
	}
	// Returns the current program counter.
	def pc() -> int {
		checkNotUnwound();
		return computePCFromFrame(sp);
	}
	// Returns {true} if this frame is currently the top executing frame, {false} if the
	// frame has called another function or been unwound.
	def isTop() -> bool {
		return true; // TODO?
	}
	// Returns the call depth of this frame within its segment, with the bottom frame being #0.
	def depth() -> int {
		checkNotUnwound();
		if (cached_depth < 0) cached_depth = computeDepth();
		return cached_depth;
	}
	private def computeDepth() -> int {
		var depth = 0;
		var next_sp = sp;
		while (true) {
			next_sp = next_sp + IVarConfig.frame.size + Pointer.SIZE;
			var retip = (next_sp + -Pointer.SIZE).load<Pointer>();
			if (!X86_64Interpreter.inCode(retip)) return depth;
			depth++;
		}
		return 0;
	}
	// Get the caller frame. If none, then {Frame.func} will be {null}.
	def caller() -> ProbeFrame {
		checkNotUnwound();
		var callerSp = sp + IVarConfig.frame.size + Pointer.SIZE;
		var frame = TargetFrame(callerSp);
		var accessor = frame.getFrameAccessor();
		if (accessor == null) return ProbeFrame(null, -1, frame);
		return ProbeFrame(accessor.func(), accessor.pc(), frame);
	}
	// Get the number of local variables in this frame.
	def numLocals() -> int {
		checkNotUnwound();
		return decl.num_locals;
	}
	// Get the value of local variable {i}.
	def getLocal(i: int) -> Value {
		checkNotUnwound();
		if (u32.view(i) >= decl.num_locals) System.error("FrameAccessorError", "local index out-of-bounds");
		var vfp = (sp + IVarConfig.frame.VFP.disp).load<Pointer>();
		return X86_64Interpreter.valueStack.readValue(vfp, i);
	}
	// Set the value of a local variable. (dynamically typechecked).
	def setLocal(i: int, v: Value);
	// Get the number of operand stack elements.
	def numOperands() -> int {
		checkNotUnwound();
		if (!isTop()) System.error("FrameAccessorError", "numOperands only supported for top frame"); // TODO
		var vfp = (sp + IVarConfig.frame.VFP.disp).load<Pointer>();
		var vsp = (sp + IVarConfig.frame.VSP.disp).load<Pointer>();
		var diff = int.!((vsp - vfp) / Target.tagging.slot_size);
		return diff - decl.num_locals;
	}
	// Get operand at depth {i}, with 0 being the top of the stack, -1 being one lower, etc.
	def getOperand(i: int) -> Value {
		checkNotUnwound();
		if (!isTop()) System.error("FrameAccessorError", "numOperands only supported for top frame"); // TODO
		var vsp = (sp + IVarConfig.frame.VSP.disp).load<Pointer>();
		return X86_64Interpreter.valueStack.readValue(vsp, i - 1);
	}
	// Set operand at depth {i}, with 0 being the top of the stack, -1 being one lower, etc. (dynamically typechecked).
	def setOperand(i: int, v: Value);

	private def checkNotUnwound() {
		if (isUnwound()) System.error("FrameAccessorError", "frame has been unwound");
	}
}

// Signal-handling for traps
def ucontext_rip_offset = 168;
def ucontext_rsp_offset = 160;
def SIGFPE  = 8;
def SIGBUS  = 10;
def SIGSEGV = 11;

// Implements the RiUserCode interface in order to add generated machine code to the V3 runtime.
// Also stores several important offsets needed in handling signals.
class X86_64InterpreterCode extends RiUserCode {
	def frameSize = IVarConfig.frameSize;
	var fastDispatchTableOffset: int;	// dispatch table when probes disabled
	var probedDispatchTableOffset: int;	// dispatch table when probes enabled
	var codeStart: int;			// start of all executable code
	var v3EntryOffset: int;			// entry from V3 calling code
	var oobMemoryHandlerOffset: int;	// handler for signals caused by OOB memory access
	var divZeroHandlerOffset: int;		// handler for signals caused by divide by zero
	var stackOverflowHandlerOffset: int;	// handler for signals caused by (value- or call-) stack overflow
	var codeEnd: int;			// end of all executable code
	var buf = StringBuilder.new().grow(128);  // avoid allocations when describing frames

	new(start: Pointer, end: Pointer) super(start, end) { }

	// Called from V3 runtime upon fatal errors to describe a frame for a stacktrace.
	def describeFrame(ip: Pointer, sp: Pointer, out: (Array<byte>, int, int) -> ()) {
		var msg = "\tin [fast-int] ";
		out(msg, 0, msg.length);
		var instance = (sp + IVarConfig.frame.INSTANCE.disp).load<Instance>();
		var func = (sp + IVarConfig.frame.FUNC_DECL.disp).load<FuncDecl>();
		// TODO: lazy parse of names section may allocate; must avoid this in OOM situation
		func.render(instance.module.names, buf);
		buf.ln().out(out);
		buf.reset();
	}

	// Called from V3 runtime for a frame where {ip} is in interpreter code.
	def nextFrame(ip: Pointer, sp: Pointer) -> (Pointer, Pointer) {
		sp += frameSize;	 // assume frame is allocated
		ip = sp.load<Pointer>(); // return address on stack
		return (ip + -1, sp + Pointer.SIZE); // XXX: V3 quirk with -1 (use RiOs?)
	}

	// Called from V3 runtime when the garbage collector needs to scan an interpreter stack frame.
	def scanFrame(ip: Pointer, sp: Pointer) {
		// Handle code and interior pointers
		var code_loc = (sp + IVarConfig.frame.CODE.disp);
		var code = code_loc.load<Pointer>();
		var ip_delta = (sp + IVarConfig.frame.IP.disp).load<Pointer>() - code;
		var eip_delta = (sp + IVarConfig.frame.EIP.disp).load<Pointer>() - code;
		RiGc.scanRoot(code_loc);
		var new_code = code_loc.load<Pointer>();
		if (new_code != code) {
			(sp + IVarConfig.frame.IP.disp).store<Pointer>(new_code + ip_delta);
			(sp + IVarConfig.frame.EIP.disp).store<Pointer>(new_code + eip_delta);
		}

		// Handle sidetable and interior pointer
		var sidetable_loc = (sp + IVarConfig.frame.SIDETABLE.disp);
		var sidetable = sidetable_loc.load<Pointer>();
		var xip_delta = (sp + IVarConfig.frame.STP.disp).load<Pointer>() - sidetable;
		RiGc.scanRoot(sidetable_loc);
		var new_sidetable = sidetable_loc.load<Pointer>();
		if (new_sidetable != sidetable) {
			(sp + IVarConfig.frame.STP.disp).store<Pointer>(new_sidetable + xip_delta);
		}

		// Handle other roots in the frame
		RiGc.scanRoot(sp + IVarConfig.frame.FUNC_DECL.disp);
		RiGc.scanRoot(sp + IVarConfig.frame.INSTANCE.disp);
	}

	// Called from V3 runtime to handle an OS-level signal that occurred while {ip} was in interpreter code.
	def handleSignal(signum: int, siginfo: Pointer, ucontext: Pointer, ip: Pointer, sp: Pointer) -> bool {
		var pip = ucontext + ucontext_rip_offset;
		var ip = pip.load<Pointer>();
		if (DEBUG) {
			Trace.OUT.put2("  !signal %d in interpreter @ 0x%x", signum, ip - Pointer.NULL).outln();
		}
		match (signum) {
			SIGFPE => {
				// presume divide/modulus by zero
				pip.store<Pointer>(start + divZeroHandlerOffset);
				return true;
			}
			SIGBUS, SIGSEGV => {
				var addr = RiOs.getAccessAddress(siginfo, ucontext);
				if (RedZones.isInRedZone(addr)) {
					pip.store<Pointer>(start + stackOverflowHandlerOffset);
					return true;
				}
				pip.store<Pointer>(start + oobMemoryHandlerOffset);
				return true;
			}
		}
		return true;
	}
	// Initializes the interpreter code object from serialized memory (e.g. global buffer).
	def deserialize(d: DataReader) -> bool {
		if (d.read_u32() != INTERPRETER_CODE_MARKER) return false;
		fastDispatchTableOffset = int.view(d.read_u32());
		probedDispatchTableOffset = int.view(d.read_u32());
		codeStart = int.view(d.read_u32());
		v3EntryOffset = int.view(d.read_u32());
		oobMemoryHandlerOffset = int.view(d.read_u32());
		divZeroHandlerOffset = int.view(d.read_u32());
		stackOverflowHandlerOffset = int.view(d.read_u32());
		codeEnd = int.view(d.read_u32());
		return true;
	}
	// Writes the interpreter code object fields to serialized memory (e.g. a file with global buffer).
	def serialize(w: DataWriter) {
		w.put_b32(INTERPRETER_CODE_MARKER);
		w.put_b32(fastDispatchTableOffset);
		w.put_b32(probedDispatchTableOffset);
		w.put_b32(codeStart);
		w.put_b32(v3EntryOffset);
		w.put_b32(oobMemoryHandlerOffset);
		w.put_b32(divZeroHandlerOffset);
		w.put_b32(stackOverflowHandlerOffset);
		w.put_b32(codeEnd);
	}
}

def fatal(msg: string) {
	System.error("X86_64InterpreterError", msg);
}


//------------------------------------------------------------------------------------------------
//-- Begin Interpreter Generator
//------------------------------------------------------------------------------------------------

// Internal register configuration for variables live in the interpreter execution context.
def R: X86_64Regs, G = X86_64Regs.GPRs, C: X86_64Conds;

// Space needed for the machine code of the interpreter
def PAGE_SIZE = 4 * 1024;
def INL_SIZE = 24 * 1024;
def OOL_SIZE = 4 * 1024;
def TOTAL_SIZE = INL_SIZE + OOL_SIZE;
// Statically-allocated buffer (in compiled binary).
def global_buffer = allocGlobalBufferWithMarker(TOTAL_SIZE + PAGE_SIZE);
def allocGlobalBufferWithMarker(size: int) -> Array<byte> {
	var result = Array<byte>.new(size);
	var w = DataWriter.new().reset(result, 0, result.length);
	w.put_b64(GLOBAL_BUFFER_MARKER);
	return result;
}
def deserializeOrGenerateInterpreterCode() -> X86_64InterpreterCode {
	def r = DataReader.new(global_buffer);

	var start = Pointer.atContents(global_buffer);
	var range = MemoryRange.new(start, start + global_buffer.length);

	var ic = X86_64InterpreterCode.new(range.start, range.end);

	// try deserializing the interpreter code object directly from the global buffer
	if (ic.deserialize(r.at(8))) {
		if (DEBUG) {
			Trace.OUT.put2("Deserialized asm interpreter into [0x%x ... 0x%x]",
				(range.start - Pointer.NULL),
				(range.end - Pointer.NULL));
			Trace.OUT.outln();
		}

	} else {
		//         global buffer v   [0 ...                                         TOTAL_SIZE  ]
		//      |xxxxxxxxxxxxxxxx|h|l|marker|global_header|...|dispatch|...|inline_code|ool_code|___
		//      ^----elem0_offset----^
		// page ^                                       1KiB  ^       page ^      page ^
		var mask = 4095L;
		var elem0_offset = (start - Pointer.NULL) & mask;
		var alloc_offset = elem0_offset + 8 + GLOBAL_BUFFER_HEADER_SIZE;
		var aligned_offset = (alloc_offset + mask) & ~mask;
		var skip = int.!(aligned_offset - elem0_offset);

		if (DEBUG) {
			Trace.OUT.put3("Generating asm interpreter into [0x%x ... 0x%x], skipping %d bytes",
				(range.start - Pointer.NULL),
				(range.end - Pointer.NULL),
				skip);
			Trace.OUT.outln();
		}


		var w_inl = DataWriter.new().reset(global_buffer, skip, skip);
		var w_ool = DataWriter.new().reset(global_buffer, skip + INL_SIZE,  skip + INL_SIZE);

		X86_64InterpreterGen.new(ic, w_inl).gen(range);
	}

	// Write-protect the executable code for security and debugging
	Mmap.protect(range.start + ic.codeStart, u64.!(ic.codeEnd - ic.codeStart), Mmap.PROT_READ | Mmap.PROT_EXEC);
	// Trace results to help in debugging
	if (DEBUG) {
		var s = range.start - Pointer.NULL;
		Trace.OUT
			.put1("\tcode start     = 0x%x\n", s + ic.codeStart)
			.put1("\tv3 entry       = 0x%x\n", s + ic.v3EntryOffset)
			.put1("\tfast dispatch  = 0x%x\n", s + ic.fastDispatchTableOffset)
			.put1("\tprobed dispatch= 0x%x\n", s + ic.probedDispatchTableOffset)
			.put1("\toob mem        = 0x%x\n", s + ic.oobMemoryHandlerOffset)
			.put1("\tdivzero        = 0x%x\n", s + ic.divZeroHandlerOffset)
			.put1("\tstack ovflw    = 0x%x\n", s + ic.stackOverflowHandlerOffset)
			.put1("\tcode end       = 0x%x\n", s + ic.codeEnd)
			.outln();
	}
	return ic;
}

type SlotAddrs(tag: X86_64Addr, value: X86_64Addr, upper: X86_64Addr) #unboxed { }
class VspHelper(vsp: X86_64Gpr, valuerep: Tagging, depth: int) {
	private def slots = Array<SlotAddrs>.new(depth + 1);
	new() {
		for (i < slots.length) {
			var offset = -1 * i * valuerep.slot_size;
			slots[i] = SlotAddrs(
				vsp.plus(offset),
				vsp.plus(offset + valuerep.tag_size),
				vsp.plus(offset + valuerep.tag_size + 4));
		}
	}
	def [i: int] -> SlotAddrs {
		return slots[0 - i]; // so caller can supply -1
	}
}
// Generates {X86_64InterpreterCode} for X86-64.
class X86_64InterpreterGen(ic: X86_64InterpreterCode, w: DataWriter) {
	def i: X86_64Interpreter;
	def asm = X86_64Assemblers.create64(w);
	def regs = IVarConfig.regs;
	def frame = IVarConfig.frame;

	def offsets = V3Offsets.new();
	def valuerep = Target.tagging;

	var oolULeb32Sites = Vector<OutOfLineLEB>.new();
	var firstDispatchOffset: int;
	var dispatchJmpOffset: int = -1;
	var callEntryOffset: int;
	var handlerEndOffset: int;
	var callReentryRef: IcCodeRef;
	var tailCallReentryRef: IcCodeRef;
	var abruptRetLabel = X86_64Label.new();
	var probedDispatchTableRef: IcCodeRef;
	var typeTagTableOffset: int;
	var useTypeTagTable = false;
	var trap_labels = Array<X86_64Label>.new(TrapReason.ERROR.tag + 1);

	def vsp = regs.VSP;
	def vsph = VspHelper.new(regs.VSP, valuerep, 3);

	def dispatchTables = Array<(byte, IcCodeRef, IcCodeRef)>.new(Opcodes.code_pages.length + 1);

	def ivar_MEM0_BASE	= (regs.MEM0_BASE, frame.MEM0_BASE);
	def ivar_VFP		= (regs.VFP, frame.VFP);
	def ivar_VSP		= (regs.VSP, frame.VSP);
	def ivar_STP		= (regs.STP, frame.STP);
	def ivar_IP		= (regs.IP, frame.IP);
	def ivar_EIP		= (regs.EIP, frame.EIP);
	def ivar_FUNC_DECL	= (regs.FUNC_DECL, frame.FUNC_DECL);
	def ivar_INSTANCE	= (regs.INSTANCE, frame.INSTANCE);
	def ivar_CURPC		= (regs.CURPC, frame.CURPC);

	def mutable_ivars = [
		ivar_VSP,
		ivar_STP,
		ivar_IP
	];
	def all_ivars = [
		ivar_MEM0_BASE,
		ivar_VFP,
		ivar_VSP,
		ivar_STP,
		ivar_IP,
		ivar_EIP,
		ivar_FUNC_DECL,
		ivar_INSTANCE,
		ivar_CURPC
	];

	new() {
		w.refill = reportOom;
		var p = Patcher.new(w);
		asm.patcher = asm.d.patcher = p;
	}

	def gen(range: MemoryRange) {
		if (tuning.dispatchEntrySize == 4 && (range.start - Pointer.NULL) > int.max) {
			fatal(Strings.format1("global buffer start address of 0x%x out of 31-bit range", (range.start - Pointer.NULL)));
		}
		// Allocate trap labels
		for (i < trap_labels.length) trap_labels[i] = X86_64Label.new();

		// Record code start
		ic.codeStart = w.atEnd().pos; // XXX: don't make dispatch tables executable

		if (tuning.useTypeTagTable) {
			// Reserve space for the type tag table and fill it out
			typeTagTableOffset = w.pos;
			w.skipN(256);
			for (t in BpTypeCode) {
				var offset = typeTagTableOffset + t.code;
				w.at(offset).putb(t.code);
				w.at(offset + 0x80).putb(t.code);
			}
			for (t in [BpTypeCode.REF_NULL, BpTypeCode.REF]) { // set upper bit
				var offset = typeTagTableOffset + t.code;
				w.at(offset).putb(t.code | 0x80);
				w.at(offset + 0x80).putb(t.code | 0x80);
			}
			for (t in [BpTypeCode.ABS]) { // TODO: load default value from instance
				var offset = typeTagTableOffset + t.code;
				w.at(offset).putb(BpTypeCode.REF_NULL.code | 0x80);
				w.at(offset + 0x80).putb(BpTypeCode.REF_NULL.code | 0x80);
			}
		}

		// Reserve space for the dispatch tables.
		reserveDispatchTables();
		// Begin code generation
		genInterpreterEntry();
		genOpcodeHandlers();
		handlerEndOffset = w.atEnd().pos;
		// Generate out-of-line code
		genOutOfLineLEBs();
		genTraps();
		ic.codeEnd = w.atEnd().pos;

		if (DEBUG) {
			var s = range.start - Pointer.NULL;
			Trace.OUT
				.put3("Finished asm interpreter @ (0x%x ... 0x%x), used %d bytes\n",
					s, (range.end - Pointer.NULL), w.pos)
				.put1("\tcall entry     = 0x%x\n", s + callEntryOffset);
			for (t in dispatchTables) Trace.OUT.put2("\tdispatch %x    = 0x%x\n", byte.view(t.0), s + t.1.offset);
			Trace.OUT
				.put1("\tfirst dispatch = 0x%x\n", s + firstDispatchOffset)
				.put1("\thandlers end   = 0x%x\n", s + handlerEndOffset)
				.put1("break *0x%x\n", s + dispatchJmpOffset)
				.outln();
		}
		if (w.pos > TOTAL_SIZE) fatal(Strings.format2("need %d bytes for interpreter code, only allocated %d", w.pos, TOTAL_SIZE));
	}
	def reserveDispatchTables() {
		{ // table #0
			w.align(tuning.dispatchEntrySize);
			var ref = IcCodeRef.new(-1);
			ref.offset = ic.fastDispatchTableOffset = w.pos;
			w.skipN(256 * tuning.dispatchEntrySize);
			dispatchTables[0] = (0, ref, null);
		}
		if (tuning.dispatchTableReg) {
			w.align(tuning.dispatchEntrySize);
			probedDispatchTableRef = IcCodeRef.new(-1);
			probedDispatchTableRef.offset = ic.probedDispatchTableOffset = w.pos;
			w.skipN(256 * tuning.dispatchEntrySize);
		}
		for (i < Opcodes.code_pages.length) {
			var page = Opcodes.code_pages[i];
			var ref = IcCodeRef.new(-1);
			w.align(tuning.dispatchEntrySize);
			ref.offset = w.pos;
			w.skipN(256 * tuning.dispatchEntrySize);
			var ref2: IcCodeRef;

			if (!page.oneByte) {
				ref2 = IcCodeRef.new(-1);
				ref2.offset = w.pos;
				w.skipN(256 * tuning.dispatchEntrySize);
			}

			dispatchTables[i + 1] = (page.prefix, ref, ref2);
		}
	}
	def genInterpreterEntry() {
		var shared_entry = X86_64Label.new();
		var tmp = regs.scratch;
		{ // Entrypoint for calls coming from V3
			ic.v3EntryOffset = w.pos;

			// Allocate and initialize interpreter stack frame from incoming V3 params.
			asm.q.sub_r_i(R.RSP, ic.frameSize);
			genInvalidateFrameAccessor();

			// Spill VSP (value stack pointer)
			asm.movq_m_r(frame.VSP, regs.v3_VSP);
			// load dispatch table into register
			if (tuning.dispatchTableReg) {
				asm.movq_r_m(regs.DISPATCH_TABLE, absPointer(offsets.Interpreter_dispatchTable));
			}
			// move WasmFunction into tmp
			asm.movq_r_r(tmp, regs.v3_WASM_FUNC);
			restoreReg(regs.VSP);
			asm.jmp_rel_near(shared_entry);
		}

		{ // Re-entry for calls within the interpreter itself
			callReentryRef = IcCodeRef.new(w.pos);
			// Allocate actual stack frame
			asm.q.sub_r_i(R.RSP, ic.frameSize);
			genInvalidateFrameAccessor();
			// Spill the (valid) stack pointer
			saveIVar(regs.VSP);
			// WasmFunction is in r1 for interpreter reentry
			asm.movq_r_r(tmp, regs.tmp1);
		}

		asm.bind(shared_entry);
		// Load wf.instance, wf.decl and spill
		asm.movq_m_r(frame.WASM_FUNC, tmp);
		asm.movq_r_m(regs.INSTANCE, tmp.plus(offsets.WasmFunction_instance));
		saveIVar(regs.INSTANCE);
		asm.movq_r_m(regs.FUNC_DECL, tmp.plus(offsets.WasmFunction_decl));
		saveIVar(regs.FUNC_DECL);

		// Compute VFP = VSP - func.sig.params.length * SLOT_SIZE
		asm.movq_r_m(tmp, regs.FUNC_DECL.plus(offsets.FuncDecl_sig));
		asm.movq_r_m(tmp, tmp.plus(offsets.SigDecl_params));
		asm.movd_r_m(tmp, tmp.plus(offsets.Array_length));
		asm.q.shl_r_i(tmp, valuerep.slot_size_log);
		asm.movq_r_r(regs.VFP, regs.VSP);
		asm.q.sub_r_r(regs.VFP, tmp);
		saveIVar(regs.VFP);

		tailCallReentryRef = IcCodeRef.new(w.pos);
		// Load &func.cur_bytecode[0] into IP
		asm.movq_r_m(tmp, regs.FUNC_DECL.plus(offsets.FuncDecl_cur_bytecode));
		asm.movq_m_r(frame.CODE, tmp); // save CODE
		asm.lea(regs.IP, tmp.plus(offsets.Array_contents));
		saveIVar(regs.IP);
		// Load IP + code.length into EIP
		asm.movd_r_m(regs.EIP, tmp.plus(offsets.Array_length));
		asm.q.add_r_r(regs.EIP, regs.IP);
		saveIVar(regs.EIP);
		// Load &func.sidetable[0] into STP
		asm.movq_r_m(regs.STP, regs.FUNC_DECL.plus(offsets.FuncDecl_sidetable));
		asm.movq_m_r(frame.SIDETABLE, regs.STP); // save SIDETABLE
		asm.q.add_r_i(regs.STP, offsets.Array_contents);
		saveIVar(regs.STP);

		// Load instance.memories[0].start into MEM0_BASE
		var mem0 = regs.MEM0_BASE;
		asm.movq_r_m(mem0, regs.INSTANCE.plus(offsets.Instance_memories));
		var no_mem = X86_64Label.new();
		asm.movd_r_m(regs.tmp0, mem0.plus(offsets.Array_length)); // XXX: always have a memories[0].start to avoid a branch?
		asm.d.cmp_r_i(regs.tmp0, 0);
		asm.jc_rel_near(C.Z, no_mem);
		asm.movq_r_m(mem0, mem0.plus(offsets.Array_contents));
		asm.movq_r_m(mem0, mem0.plus(offsets.X86_64Memory_start));
		asm.bind(no_mem);
		saveIVar(regs.MEM0_BASE);

		callEntryOffset = w.pos;
		// Decode locals and initialize them. (XXX: special-case 0 locals)
		var countGpr = regs.tmp0;
		genReadUleb32(countGpr);
		var start = X86_64Label.new(), done = X86_64Label.new();
		// gen: if (count != 0) do
		asm.d.cmp_r_i(countGpr, 0);
		asm.jc_rel_near(C.Z, done);
		asm.bind(start);
		// gen: var num = read_uleb32()
		var numGpr = regs.tmp1;
		genReadUleb32(numGpr);
		// gen: var type = read_type();
		var type_leb_done = X86_64Label.new(), type_done = X86_64Label.new();
		var typeGpr = regs.tmp2;
		asm.d.movbzx_r_m(typeGpr, regs.IP_ptr);	// load first byte
		asm.q.inc_r(regs.IP);			// increment pointer
		asm.d.test_r_i(typeGpr, 0x80);		// test most-significant bit
		asm.jc_rel_near(C.Z, type_leb_done);	// more type LEB bytes?
		genSkipLeb();				// only first byte matters
		asm.bind(type_leb_done);
		// gen: if((type = typeTags[type]) < 0) skip_leb()
		if (tuning.useTypeTagTable) {
			// TODO: AbstractT requires a load of the default value from the instance
			asm.q.lea(regs.scratch, IcCodeRef.new(typeTagTableOffset));   // materialize type tag table address
			asm.d.movbsx_r_m(typeGpr, typeGpr.plusR(regs.scratch, 1, 0)); // load from type tag table
			asm.jc_rel_near(C.NC, type_done);	// check upper bit of entry
			genSkipLeb();				// skip LEB if upper bit of entry set
		}
		asm.bind(type_done);

		// gen: if(num != 0) do
		var start2 = X86_64Label.new(), done2 = X86_64Label.new();
		asm.d.cmp_r_i(numGpr, 0);
		asm.jc_rel_near(C.Z, done2);
		asm.bind(start2);
		genTagPushR(typeGpr);			// *(sp) = type
		asm.movq_m_i(vsph[0].value, 0);		// *(sp + 8) = 0
		asm.add_r_i(vsp, valuerep.slot_size);	// sp += 16
		// gen: while (--num != 0)
		asm.d.dec_r(numGpr);
		asm.jc_rel_near(C.NZ, start2);

		// gen: while (--count != 0)
		asm.d.dec_r(countGpr);
		asm.jc_rel_near(C.NZ, start);
		asm.bind(done);

		// execute first instruction
		genDispatchOrJumpToDispatch();
	}

	// Generate all the opcode handlers.
	def genOpcodeHandlers() {
		// Generate the default handler and initialize dispatch tables
		var pos = w.atEnd().pos;
		computeCurIpForTrap(-1);
		asm.jmp_rel_far(trap_labels[TrapReason.INVALID_OPCODE.tag]);
		for (t in dispatchTables) {
			var ref = t.1;
			for (i < 256) writeDispatchEntry(ref, i, pos);
		}

		// Generate the secondary dispatch tables and point main table at them
		var ref0 = dispatchTables[0].1;
		for (t in dispatchTables) {
			if (t.0 == 0) continue; // main dispatch table
			var pos = w.atEnd().pos;
			computeCurIpForTrap(-1);
			genDispatch(regs.IP_ptr, t.1, true);
			writeDispatchEntry(ref0, t.0, pos);
		}

		// Generate extended LEB landing pads for secondary dispatch tables
		for (i = 1; i < dispatchTables.length; i++) {
			var t = dispatchTables[i];
			var pos = w.pos;
			if (t.2 != null) {
				// TODO: some code in this page are in the upper 128; read extended LEB
				genDispatch(null, t.2, false);
			} else {
				// all codes in this page are in the lower 128; just skip extended LEB
				genSkipLeb();
				asm.d.and_r_i(regs.tmp0, 0x7F);
				genDispatch(null, t.1, false);
			}
			for (i = 128; i < 256; i++) {
				writeDispatchEntry(t.1, i, pos);
			}
		}

		genConsts();
		genControlFlow();
		genLocals();
		genCallsAndRet();
		genLoadsAndStores();
		genCompares();
		genI32Arith();
		genI64Arith();
		genExtensions();
		genF32Arith();
		genF64Arith();
		genFloatCmps();
		genFloatMinAndMax();
		genFloatTruncs();
		genFloatConversions();
		genRuntimeCallOps();
		genMisc();
	}
	def writeDispatchEntry(ref: IcCodeRef, opcode: int, offset: int) {
		match (tuning.dispatchEntrySize) {
			2 => w.at(ref.offset + 2 * opcode).put_b16(offset - ref.offset);
			4 => w.at(ref.offset + 4 * opcode).put_b32(int.!((ic.start + offset) - Pointer.NULL));
			8 => w.at(ref.offset + 8 * opcode).put_b64((ic.start + offset) - Pointer.NULL);
		}
		w.atEnd();
	}
	def genConsts() {
		bindHandler(Opcode.I32_CONST); {
			genReadSleb32_inline(regs.tmp1);
			genTagPush(BpTypeCode.I32.code);
			asm.movq_m_r(vsph[0].value, regs.tmp1);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.I64_CONST); {
			genReadSleb64_inline(regs.tmp1);
			genTagPush(BpTypeCode.I64.code);
			asm.movq_m_r(vsph[0].value, regs.tmp1);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F32_CONST); {
			asm.movd_r_m(regs.tmp0, regs.IP_ptr);
			asm.add_r_i(regs.IP, 4);
			genTagPush(BpTypeCode.F32.code);
			asm.movq_m_r(vsph[0].value, regs.tmp0);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F64_CONST); {
			asm.movq_r_m(regs.tmp0, regs.IP_ptr);
			asm.add_r_i(regs.IP, 8);
			genTagPush(BpTypeCode.F64.code);
			asm.movq_m_r(vsph[0].value, regs.tmp0);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
	}
	def genControlFlow() {
		// NOP: just goes directly back to the dispatch loop
		patchDispatchTable(Opcode.NOP, firstDispatchOffset);

		// UNREACHABLE: abrupt return
		bindHandler(Opcode.UNREACHABLE);
		computeCurIpForTrap(-1);
		asm.jmp_rel_far(trap_labels[TrapReason.UNREACHABLE.tag]);

		// BLOCK, LOOP, and TRY are nops except skipping the LEB
		bindHandler(Opcode.BLOCK);
		bindHandler(Opcode.LOOP);
		bindHandler(Opcode.TRY);
		genSkipLeb();
		endHandler();

		var ctl_fallthru = X86_64Label.new();
		var ctl_xfer = X86_64Label.new();
		var ctl_xfer_nostack = X86_64Label.new();

		// IF: check condition and either fall thru to next bytecode or ctl xfer (without stack copying)
		bindHandler(Opcode.IF);
		asm.sub_r_i(vsp, valuerep.slot_size);
		asm.d.cmp_m_i(vsph[0].value, 0);
		asm.jc_rel_near(C.Z, ctl_xfer_nostack);
		asm.bind(ctl_fallthru);
		genSkipLeb();
		asm.add_r_i(regs.STP, offsets.STP_entry_size);
		endHandler();

		// BR_IF: check condition and either fall thru to next bytecode or ctl xfer (with stack copying)
		bindHandler(Opcode.BR_IF);
		asm.sub_r_i(vsp, valuerep.slot_size);
		asm.d.cmp_m_i(vsph[0].value, 0);
		asm.jc_rel_near(C.Z, ctl_fallthru);
		// fallthru to BR

		// BR: unconditional ctl xfer with stack copying
		bindHandlerNoAlign(Opcode.BR);
		asm.bind(ctl_xfer);
		var popcount = regs.tmp0;
		var valcount = regs.tmp1;
		// if popcount > 0
		asm.movd_r_m(popcount, regs.STP.plus(offsets.STP_popcount));
		asm.d.cmp_r_i(popcount, 0);
		asm.jc_rel_near(C.Z, ctl_xfer_nostack);
		// load valcount
		asm.movd_r_m(valcount, regs.STP.plus(offsets.STP_valcount));
		// popcount = popcount * SLOT_SIZE
		asm.d.shl_r_i(popcount, valuerep.slot_size_log);
		// vsp -= valcount + popcount (XXX: save an instruction here?)
		asm.q.sub_r_r(vsp, popcount);
		asm.movd_r_r(regs.scratch, valcount);
		asm.d.shl_r_i(regs.scratch, valuerep.slot_size_log);
		asm.q.sub_r_r(vsp, regs.scratch);
		// do { [vsp] = [vsp + popcount]; vsp++; valcount--; } while (valcount != 0)
		var loop = X86_64Label.new();
		asm.bind(loop);
		genCopySlot(vsp.plus(0), vsp.plusR(popcount, 1, 0));
		asm.q.add_r_i(vsp, valuerep.slot_size);
		asm.d.dec_r(valcount);
		asm.jc_rel_near(C.G, loop);

		// ELSE: unconditional ctl xfer without stack copying
		bindHandlerNoAlign(Opcode.ELSE);
		asm.bind(ctl_xfer_nostack);
		asm.movwsx_r_m(regs.tmp0, regs.STP.plus(offsets.STP_pc_delta)); // TODO: 4 bytes
		asm.q.lea(regs.IP, regs.IP.plusR(regs.tmp0, 1, -1)); // adjust ip
		asm.movwsx_r_m(regs.tmp1, regs.STP.plus(offsets.STP_xip_delta)); // TODO: 4 bytes
		asm.q.lea(regs.STP, regs.STP.plusR(regs.tmp1, 4, 0)); // adjust xip XXX: preshift?
		endHandler();

		// BR_TABLE: adjust STP based on input value and then ctl xfer with stack copying
		bindHandler(Opcode.BR_TABLE);
		var max = regs.tmp0, key = regs.tmp1;
		asm.movd_r_m(max, regs.STP.plus(offsets.STP_pc_delta));
		asm.sub_r_i(vsp, valuerep.slot_size);
		asm.movd_r_m(key, vsph[0].value);
		asm.d.cmp_r_r(key, max);
		var ok = X86_64Label.new();
		asm.jc_rel_near(C.NC, ok);
		asm.d.inc_r(key);
		asm.movd_r_r(max, key);
		asm.bind(ok);
		asm.q.add_r_r(regs.IP, max);
		asm.shl_r_i(max, offsets.STP_entry_size_log);
		asm.q.add_r_r(regs.STP, max);
		asm.jmp_rel_near(ctl_xfer);

		// BR_ON_NULL: check condition and either fall thru to next bytecode or ctl xfer (with stack copying)
		bindHandler(Opcode.BR_ON_NULL);
		asm.sub_r_i(vsp, valuerep.slot_size);
		asm.q.cmp_m_i(vsph[0].value, 0);
		asm.jc_rel_near(C.NZ, ctl_fallthru);
		asm.jmp_rel_near(ctl_xfer);

		// BR_ON_NON_NULL: check condition and either fall thru to next bytecode or ctl xfer (with stack copying)
		bindHandler(Opcode.BR_ON_NON_NULL);
		asm.sub_r_i(vsp, valuerep.slot_size);
		asm.q.cmp_m_i(vsph[0].value, 0);
		asm.jc_rel_near(C.Z, ctl_fallthru);
		asm.jmp_rel_near(ctl_xfer);

		bindHandler(Opcode.SELECT); {
			var label = X86_64Label.new();
			asm.d.cmp_m_i(vsph[-1].value, 0);
			asm.jc_rel_near(C.NZ, label);
			// false case; copy false value down
			asm.movq_r_m(regs.tmp0, vsph[-2].value);
			asm.movq_m_r(vsph[-3].value, regs.tmp0);
			// true case, nothing to do
			asm.bind(label);
			asm.sub_r_i(vsp, 2 * valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.SELECT_T); {
			genReadUleb32(regs.tmp0); // load # values
			var skip = X86_64Label.new();
			asm.movd_r_r(regs.tmp1, regs.tmp0);
			asm.bind(skip);  // skip value types
			genSkipLeb();
			asm.dec_r(regs.tmp1);
			asm.jc_rel_near(C.NZ, skip);

			asm.d.shl_r_i(regs.tmp0, valuerep.slot_size_log);
			asm.movd_r_m(regs.tmp1, vsph[-1].value);
			asm.sub_r_r(vsp, regs.tmp0);
			asm.sub_r_i(vsp, valuerep.slot_size); // XXX: combine with above using lea
			asm.d.cmp_r_i(regs.tmp1, 0);
			var label = X86_64Label.new();
			asm.jc_rel_near(C.NZ, label);
			// false case; copy false values down
			asm.movq_r_r(regs.tmp1, vsp);
			asm.q.sub_r_r(regs.tmp1, regs.tmp0);
			var copy = X86_64Label.new();
			asm.bind(copy);
			asm.movq_r_m(regs.tmp2, vsp.plusR(regs.tmp0, 1, - Pointer.SIZE));
			asm.movq_m_r(regs.tmp1.plusR(regs.tmp0, 1, - Pointer.SIZE), regs.tmp2);
			asm.d.sub_r_i(regs.tmp0, valuerep.slot_size);
			asm.jc_rel_near(C.NZ, copy);
			// true case, nothing to do
			asm.bind(label);
			endHandler();
		}
	}
	def genLocals() {
		bindHandler(Opcode.DROP);
		asm.sub_r_i(vsp, valuerep.slot_size);
		endHandler();

		bindHandler(Opcode.LOCAL_GET);
		genReadUleb32(regs.tmp0);
		asm.d.shl_r_i(regs.tmp0, valuerep.slot_size_log);
		genCopySlot(vsp.indirect(), regs.VFP.plusR(regs.tmp0, 1, 0));
		asm.add_r_i(vsp, valuerep.slot_size);
		endHandler();

		bindHandler(Opcode.LOCAL_SET);
		genReadUleb32(regs.tmp0);
		asm.d.shl_r_i(regs.tmp0, valuerep.slot_size_log);
		asm.sub_r_i(vsp, valuerep.slot_size);
		asm.movq_r_m(regs.tmp1, vsph[0].value);
		asm.movq_m_r(regs.VFP.plusR(regs.tmp0, 1, valuerep.tag_size), regs.tmp1);
		endHandler();

		bindHandler(Opcode.LOCAL_TEE);
		genReadUleb32(regs.tmp0);
		asm.d.shl_r_i(regs.tmp0, valuerep.slot_size_log);
		asm.movq_r_m(regs.tmp1, vsph[-1].value);
		asm.movq_m_r(regs.VFP.plusR(regs.tmp0, 1, valuerep.tag_size), regs.tmp1);
		endHandler();
	}
	def genCallsAndRet() {
		bindHandler(Opcode.END);
		asm.q.cmp_r_r(regs.IP, regs.EIP);
		// XXX: END: jump over inlined dispatch?
		asm.jc_rel(C.L, firstDispatchOffset - w.pos); // jump to dispatch (loop)
		// end falls through to return bytecode

		var callFunction = X86_64Label.new();
		var targetFunc = regs.tmp1;
		bindHandlerNoAlign(Opcode.RETURN); {
			// Copy return values from stack to overwrite locals
			var cnt = regs.tmp0;
			asm.movq_r_m(cnt, regs.FUNC_DECL.plus(offsets.FuncDecl_sig));
			asm.movq_r_m(cnt, cnt.plus(offsets.SigDecl_results));
			asm.movd_r_m(cnt, cnt.plus(offsets.Array_length));
			genCopyStackValsToVfp(cnt, regs.tmp1);
			// Deallocate interpreter frame and return to calling code.
			asm.movd_r_i(Target.V3_RET_GPRS[0], 0);
			genPopFrameAndRet();

			bindHandler(Opcode.CALL);
			computeCurIpForTrap(-1);
			genReadUleb32(regs.tmp1);

			asm.movq_r_m(regs.tmp0, regs.INSTANCE.plus(offsets.Instance_functions));
			asm.movq_r_m(targetFunc, regs.tmp0.plusR(regs.tmp1, offsets.REF_SIZE, offsets.Array_contents));

			// call_indirect jumps here
			asm.bind(callFunction);
			computePcFromCurIp();
			saveCallerIVars();
			var call_host = X86_64Label.new();
			asm.d.cmp_m_i(targetFunc.plus(0), offsets.WasmFunction_typeId);
			asm.jc_rel_near(C.NZ, call_host);

			// WasmFunction: call into interpreter reentry
			asm.callr_addr(callReentryRef);
			genExecStateCheck();
			restoreCallerIVars();
			genDispatchOrJumpToDispatch();

			// HostFunction: call into interpreter runtimeCall to enter into V3 code
			asm.bind(call_host);
			callRuntime(refRuntimeCall(this.i.runtime_callHost), [targetFunc], true);
			restoreCallerIVars();
			genDispatchOrJumpToDispatch();
		}

		var trap_func_invalid = trap_labels[TrapReason.FUNC_INVALID.tag];
		var check_sig_mismatch = X86_64Label.new();
		bindHandler(Opcode.CALL_INDIRECT); {
			computeCurIpForTrap(-1);
			var sig_index = regs.tmp1, table_index = regs.tmp2, func_index = regs.tmp0;
			genReadUleb32(sig_index);
			genReadUleb32(table_index);

			asm.sub_r_i(vsp, valuerep.slot_size);
			asm.movd_r_m(func_index, vsph[0].value);

			var tmp = regs.tmp3;
			// load instance.sig_ids[sig_index] into sig_index
			asm.movq_r_m(tmp, regs.INSTANCE.plus(offsets.Instance_sig_ids));
			asm.movd_r_m(sig_index, tmp.plusR(sig_index, offsets.INT_SIZE, offsets.Array_contents));
			// Bounds-check table.ids[func_index]
			asm.movq_r_m(tmp, regs.INSTANCE.plus(offsets.Instance_tables));
			var table = table_index;
			asm.movq_r_m(table, tmp.plusR(table_index, offsets.REF_SIZE, offsets.Array_contents));
			asm.movq_r_m(tmp, table.plus(offsets.Table_ids));
			asm.d.cmp_r_m(func_index, tmp.plus(offsets.Array_length));
			asm.jc_rel_far(C.NC, trap_func_invalid);
			// Check table.ids[func_index] == sig_index
			asm.d.cmp_r_m(sig_index, tmp.plusR(func_index, offsets.INT_SIZE, offsets.Array_contents));
			asm.jc_rel_near(C.NZ, check_sig_mismatch);
			// Load table.funcs[func_index] into {targetFunc} and jump to calling sequence
			asm.movq_r_m(tmp, table.plus(offsets.Table_funcs));
			asm.movq_r_m(targetFunc, tmp.plusR(func_index, offsets.REF_SIZE, offsets.Array_contents));
			asm.jmp_rel_near(callFunction);  // XXX: duplicate call sequence here?
			// Signature check failed. Mismatch or invalid function?
			asm.bind(check_sig_mismatch);
			asm.d.cmp_m_i(tmp.plusR(func_index, offsets.INT_SIZE, offsets.Array_contents), 0);
			asm.jc_rel_far(C.S, trap_func_invalid); // < 0 implies invalid function, not function sig mismatch
			asm.jmp_rel_far(trap_labels[TrapReason.FUNC_SIG_MISMATCH.tag]);
		}

		bindHandler(Opcode.CALL_REF); {
			computeCurIpForTrap(-1);
			genSkipLeb(); // skip signature index
			asm.sub_r_i(vsp, valuerep.slot_size);
			asm.movq_r_m(targetFunc, vsph[0].value);
			asm.q.cmp_r_i(targetFunc, 0);
			asm.jc_rel_near(X86_64Conds.NZ, callFunction);
			asm.jmp_rel_far(trap_labels[TrapReason.NULL_DEREF.tag]);
		}

		var tailCallFunction = X86_64Label.new();
		bindHandler(Opcode.RETURN_CALL); {
			genReadUleb32(regs.tmp1);

			asm.movq_r_m(regs.tmp0, regs.INSTANCE.plus(offsets.Instance_functions));
			asm.movq_r_m(targetFunc, regs.tmp0.plusR(regs.tmp1, offsets.REF_SIZE, offsets.Array_contents));

			// return_tail_call jumps here
			asm.bind(tailCallFunction);
			// Overwrite current locals with outgoing arguments
			var cnt = regs.tmp0;
			asm.movq_r_m(cnt, targetFunc.plus(offsets.Function_sig));
			asm.movq_r_m(cnt, cnt.plus(offsets.SigDecl_params));
			asm.movd_r_m(cnt, cnt.plus(offsets.Array_length));
			genCopyStackValsToVfp(cnt, regs.tmp2);

			// Check if the target function is a WasmFunction or HostFunction
			var tail_call_host = X86_64Label.new();
			asm.d.cmp_m_i(targetFunc.plus(0), offsets.WasmFunction_typeId);
			asm.jc_rel_near(C.NZ, tail_call_host);

			// WasmFunction: jump into interpreter reentry
			asm.q.lea(regs.VSP, regs.VFP.plusR(cnt, 1, 0)); // set VSP properly
			asm.movq_r_m(regs.INSTANCE, targetFunc.plus(offsets.WasmFunction_instance));
			saveIVar(regs.INSTANCE);
			asm.movq_r_m(regs.FUNC_DECL, targetFunc.plus(offsets.WasmFunction_decl));
			saveIVar(regs.FUNC_DECL);
			asm.jmp_rel_addr(tailCallReentryRef);

			// HostFunction: jump into interpreter runtimeCall to enter into V3 code
			asm.bind(tail_call_host);

			// Custom code to tail-call the runtime
			asm.movq_r_r(regs.tmp3, regs.VSP); // save VSP from being overwritten
			var abs = refRuntimeCall(this.i.runtime_callHost);
			var args = [targetFunc];
			// Generate parallel moves from args into param gprs; assume each src register used only once
			var dst = Array<X86_64Gpr>.new(G.length);
			for (i < args.length) {
				var sreg = args[i];
				var dreg = Target.V3_PARAM_GPRS[i + 1];
				if (sreg != dreg) dst[sreg.regnum] = dreg;
			}
			var stk = Array<i8>.new(G.length);
			for (i < dst.length) orderMoves(dst, stk, i);
			// load interpreter into first arg register
			var interp = Target.V3_PARAM_GPRS[0];
			// save a copy of VSP into interpreter.valueStack.sp
			asm.movq_r_m(regs.scratch, absPointer(offsets.Interpreter_valueStack));
			asm.movq_m_r(regs.scratch.plus(offsets.ValueStack_sp), regs.tmp3);
			genInvalidateFrameAccessor();
			asm.q.add_r_i(R.RSP, ic.frameSize); // deallocate interpreter frame
			asm.jmp_rel(int.!(abs - (ic.start + w.pos))); // tail-call into runtime
		}

		bindHandler(Opcode.RETURN_CALL_INDIRECT); {
			computeCurIpForTrap(-1);
			var sig_index = regs.tmp1, table_index = regs.tmp2, func_index = regs.tmp0;
			genReadUleb32(sig_index);
			genReadUleb32(table_index);

			asm.sub_r_i(vsp, valuerep.slot_size);
			asm.movd_r_m(func_index, vsph[0].value);

			var tmp = regs.tmp3;
			// load instance.sig_ids[sig_index] into sig_index
			asm.movq_r_m(tmp, regs.INSTANCE.plus(offsets.Instance_sig_ids));
			asm.movd_r_m(sig_index, tmp.plusR(sig_index, offsets.INT_SIZE, offsets.Array_contents));
			// Bounds-check table.ids[func_index]
			asm.movq_r_m(tmp, regs.INSTANCE.plus(offsets.Instance_tables));
			var table = table_index;
			asm.movq_r_m(table, tmp.plusR(table_index, offsets.REF_SIZE, offsets.Array_contents));
			asm.movq_r_m(tmp, table.plus(offsets.Table_ids));
			asm.d.cmp_r_m(func_index, tmp.plus(offsets.Array_length));
			asm.jc_rel_far(C.NC, trap_func_invalid);
			// Check table.ids[func_index] == sig_index
			asm.d.cmp_r_m(sig_index, tmp.plusR(func_index, offsets.INT_SIZE, offsets.Array_contents));
			asm.jc_rel_near(C.NZ, check_sig_mismatch);
			// Load table.funcs[func_index] into r1 and jump to calling sequence
			asm.movq_r_m(tmp, table.plus(offsets.Table_funcs));
			asm.movq_r_m(targetFunc, tmp.plusR(func_index, offsets.REF_SIZE, offsets.Array_contents));
			asm.jmp_rel_near(tailCallFunction); // XXX: duplicate tail call sequence here?
		}

		bindHandler(Opcode.RETURN_CALL_REF); {
			computeCurIpForTrap(-1);
			genSkipLeb(); // skip signature index
			asm.sub_r_i(vsp, valuerep.slot_size);
			asm.movq_r_m(targetFunc, vsph[0].value);
			asm.q.cmp_r_i(targetFunc, 0);
			asm.jc_rel_near(X86_64Conds.NZ, tailCallFunction);
			asm.jmp_rel_far(trap_labels[TrapReason.NULL_DEREF.tag]);
		}
	}
	def genCopyStackValsToVfp(cnt: X86_64Gpr, i: X86_64Gpr) {
		var done = X86_64Label.new();
		// Copy argument(s) from VSP to VFP.
		asm.cmp_r_i(cnt, 0);
		asm.jc_rel_near(C.Z, done);
		asm.movd_r_i(i, 0);
		asm.d.shl_r_i(cnt, valuerep.slot_size_log);
		asm.q.sub_r_r(regs.VSP, cnt);
		var loop = X86_64Label.new();
		// while (i < cnt)
		asm.bind(loop);
		genCopySlot(regs.VFP.plusR(i, 1, 0), regs.VSP.plusR(i, 1, 0));
		asm.q.add_r_i(i, valuerep.slot_size);
		asm.q.cmp_r_r(i, cnt);
		asm.jc_rel_near(C.L, loop);
		asm.bind(done);
		// set VSP properly
		asm.q.lea(regs.VSP, regs.VFP.plusR(cnt, 1, 0));
	}
	def genLoadsAndStores() {
		genLoad(Opcode.I32_LOAD, BpTypeCode.I32.code, asm.movd_r_m);
		genLoad(Opcode.I64_LOAD, BpTypeCode.I64.code, asm.movq_r_m);
		genLoad(Opcode.F32_LOAD, BpTypeCode.F32.code, asm.movd_r_m);
		genLoad(Opcode.F64_LOAD, BpTypeCode.F64.code, asm.movq_r_m);
		genLoad(Opcode.I32_LOAD8_S, BpTypeCode.I32.code, asm.movbsx_r_m);
		genLoad(Opcode.I32_LOAD8_U, BpTypeCode.I32.code, asm.movbzx_r_m);
		genLoad(Opcode.I32_LOAD16_S, BpTypeCode.I32.code, asm.movwsx_r_m);
		genLoad(Opcode.I32_LOAD16_U, BpTypeCode.I32.code, asm.movwzx_r_m);
		genLoad(Opcode.I64_LOAD8_S, BpTypeCode.I64.code, asm.movbsx_r_m);
		genLoad(Opcode.I64_LOAD8_U, BpTypeCode.I64.code, asm.movbzx_r_m);
		genLoad(Opcode.I64_LOAD16_S, BpTypeCode.I64.code, asm.movwsx_r_m);
		genLoad(Opcode.I64_LOAD16_U, BpTypeCode.I64.code, asm.movwzx_r_m);
		bindHandler(Opcode.I64_LOAD32_S); {
			asm.q.inc_r(regs.IP); 				// skip flags byte
			genReadUleb32(regs.tmp0);				// decode offset
			asm.movd_r_m(regs.tmp1, vsph[-1].value);			// read index
			asm.q.add_r_r(regs.tmp0, regs.tmp1);			// add index + offset
			asm.movd_r_m(regs.tmp1, regs.MEM0_BASE.plusR(regs.tmp0, 1, 0));
			asm.q.shl_r_i(regs.tmp1, 32); 			// special sign-extension necessary
			asm.q.sar_r_i(regs.tmp1, 32);
			genTagUpdate(BpTypeCode.I64.code);
			asm.movq_m_r(vsph[-1].value, regs.tmp1);
			endHandler();
		}
		genLoad(Opcode.I64_LOAD32_U, BpTypeCode.I64.code, asm.movd_r_m);

		bindHandler(Opcode.I32_STORE);
		bindHandler(Opcode.F32_STORE);
		bindHandler(Opcode.I64_STORE32);
		genStore(asm.movd_m_r);

		bindHandler(Opcode.I64_STORE);
		bindHandler(Opcode.F64_STORE);
		genStore(asm.movq_m_r);

		bindHandler(Opcode.I32_STORE8);
		bindHandler(Opcode.I64_STORE8);
		genStore(asm.movb_m_r);

		bindHandler(Opcode.I32_STORE16);
		bindHandler(Opcode.I64_STORE16);
		genStore(asm.movw_m_r);
	}
	def genCompares() {
		// 32-bit integer compares
		for (t in [
			(Opcode.I32_EQ, C.Z),
			(Opcode.I32_NE, C.NZ),
			(Opcode.I32_LT_S, C.L),
			(Opcode.I32_LT_U, C.C),
			(Opcode.I32_GT_S, C.G),
			(Opcode.I32_GT_U, C.A),
			(Opcode.I32_LE_S, C.LE),
			(Opcode.I32_LE_U, C.NA),
			(Opcode.I32_GE_S, C.GE),
			(Opcode.I32_GE_U, C.NC)
		]) {
			bindHandler(t.0);
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			asm.d.cmp_m_r(vsph[-2].value, regs.tmp0);
			asm.set_r(t.1, regs.tmp0);
			asm.movbzx_r_r(regs.tmp0, regs.tmp0);
			asm.movd_m_r(vsph[-2].value, regs.tmp0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		// 64-bit integer compares
		bindHandler(Opcode.REF_EQ); // share handler with I64_EQ
		for (t in [
			(Opcode.I64_EQ, C.Z),
			(Opcode.I64_NE, C.NZ),
			(Opcode.I64_LT_S, C.L),
			(Opcode.I64_LT_U, C.C),
			(Opcode.I64_GT_S, C.G),
			(Opcode.I64_GT_U, C.A),
			(Opcode.I64_LE_S, C.LE),
			(Opcode.I64_LE_U, C.NA),
			(Opcode.I64_GE_S, C.GE),
			(Opcode.I64_GE_U, C.NC)
		]) {
			bindHandler(t.0);
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			asm.q.cmp_m_r(vsph[-2].value, regs.tmp0);
			asm.set_r(t.1, regs.tmp0);
			asm.movbzx_r_r(regs.tmp0, regs.tmp0);
			asm.movq_m_r(vsph[-2].value, regs.tmp0);
			if (valuerep.tagged) asm.movq_m_i(vsph[-2].tag, BpTypeCode.I32.code);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}

	}
	def genI32Arith() {
		bindHandler(Opcode.I32_EQZ); {
			asm.d.test_m_i(vsph[-1].value, -1);
			asm.set_r(C.Z, regs.tmp0);
			asm.movbzx_r_r(regs.tmp0, regs.tmp0);
			asm.movd_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I32_CLZ); {
			asm.movd_r_i(regs.tmp1, -1);
			asm.d.bsr_r_m(regs.tmp0, vsph[-1].value);
			asm.d.cmov_r(C.Z, regs.tmp0, regs.tmp1);
			asm.movd_r_i(regs.tmp1, 31);
			asm.d.sub_r_r(regs.tmp1, regs.tmp0);
			asm.movd_m_r(vsph[-1].value, regs.tmp1);
			endHandler();
		}
		bindHandler(Opcode.I32_CTZ); {
			asm.d.bsf_r_m(regs.tmp0, vsph[-1].value);
			asm.movd_r_i(regs.tmp1, 32);
			asm.d.cmov_r(C.Z, regs.tmp0, regs.tmp1);
			asm.movd_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I32_POPCNT); {
			asm.d.popcnt_r_m(regs.tmp0, vsph[-1].value);
			asm.movd_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I32_MUL); {
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			asm.d.imul_r_m(regs.tmp0, vsph[-2].value);
			asm.movd_m_r(vsph[-2].value, regs.tmp0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.I32_DIV_S); {
			computeCurIpForTrap(-1);
			var div = X86_64Label.new(), neg = X86_64Label.new(), done = X86_64Label.new();
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			asm.d.cmp_r_i(regs.tmp0, -1);
			asm.jc_rel_near(C.NZ, div);
			asm.movd_r_m(regs.tmp0, vsph[-2].value);
			asm.d.cmp_r_i(regs.tmp0, 0x80000000);
			asm.jc_rel_near(C.NZ, neg);
			asm.jmp_rel_far(trap_labels[TrapReason.DIV_UNREPRESENTABLE.tag]); // XXX: invert
			asm.bind(neg);
			asm.d.neg_m(vsph[-2].value);
			asm.jmp_rel_near(done);
			asm.bind(div);
			spillReg(R.RAX);
			spillReg(R.RDX);
			asm.movd_r_m(R.RAX, vsph[-2].value);
			asm.d.cdq();
			asm.d.idiv_r(regs.tmp0);
			asm.movd_m_r(vsph[-2].value, R.RAX);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			asm.bind(done);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.I32_DIV_U); {
			computeCurIpForTrap(-1);
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			spillReg(R.RAX);
			spillReg(R.RDX);
			asm.movd_r_m(R.RAX, vsph[-2].value);
			asm.movd_r_i(R.RDX, 0);
			asm.d.div_r(regs.tmp0);
			asm.movd_m_r(vsph[-2].value, R.RAX);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.I32_REM_S); {
			computeCurIpForTrap(-1);
			var div = X86_64Label.new(), done = X86_64Label.new();
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			asm.d.cmp_r_i(regs.tmp0, -1);
			asm.jc_rel_near(C.NZ, div);
			asm.movd_m_i(vsph[-2].value, 0);
			asm.jmp_rel_near(done);
			asm.bind(div);
			spillReg(R.RAX);
			spillReg(R.RDX);
			asm.movd_r_m(R.RAX, vsph[-2].value);
			asm.d.cdq();
			asm.d.idiv_r(regs.tmp0);
			asm.movd_m_r(vsph[-2].value, R.RDX);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			asm.bind(done);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.I32_REM_U); {
			computeCurIpForTrap(-1);
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			spillReg(R.RAX);
			spillReg(R.RDX);
			asm.movd_r_m(R.RAX, vsph[-2].value);
			asm.movd_r_i(R.RDX, 0);
			asm.d.div_r(regs.tmp0);
			asm.movd_m_r(vsph[-2].value, R.RDX);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.I32_ADD, asm.d.add_m_r),
			(Opcode.I32_SUB, asm.d.sub_m_r),
			(Opcode.I32_AND, asm.d.and_m_r),
			(Opcode.I32_OR, asm.d.or_m_r),
			(Opcode.I32_XOR, asm.d.xor_m_r)
		]) {
			bindHandler(t.0);
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			t.1(vsph[-2].value, regs.tmp0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.I32_SHL, asm.d.shl_m_cl),
			(Opcode.I32_SHR_S, asm.d.sar_m_cl),
			(Opcode.I32_SHR_U, asm.d.shr_m_cl),
			(Opcode.I32_ROTL, asm.d.rol_m_cl),
			(Opcode.I32_ROTR, asm.d.ror_m_cl)
		]) {
			bindHandler(t.0);
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			t.1(vsph[-2].value);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
	}
	def genI64Arith() {
		bindHandler(Opcode.I64_EQZ); {
			asm.q.test_m_i(vsph[-1].value, -1);
			asm.set_r(C.Z, regs.tmp0);
			asm.movbzx_r_r(regs.tmp0, regs.tmp0);
			asm.movd_m_r(vsph[-1].value, regs.tmp0);
			if (valuerep.tagged) asm.movd_m_i(vsph[-1].tag, BpTypeCode.I32.code);
			endHandler();
		}
		bindHandler(Opcode.I64_CLZ); {
			asm.movq_r_i(regs.tmp1, -1);
			asm.q.bsr_r_m(regs.tmp0, vsph[-1].value);
			asm.q.cmov_r(C.Z, regs.tmp0, regs.tmp1);
			asm.movd_r_i(regs.tmp1, 63);
			asm.q.sub_r_r(regs.tmp1, regs.tmp0);
			asm.movq_m_r(vsph[-1].value, regs.tmp1);
			endHandler();
		}
		bindHandler(Opcode.I64_CTZ); {
			asm.q.bsf_r_m(regs.tmp0, vsph[-1].value);
			asm.movd_r_i(regs.tmp1, 64);
			asm.q.cmov_r(C.Z, regs.tmp0, regs.tmp1);
			asm.movq_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I64_POPCNT); {
			asm.q.popcnt_r_m(regs.tmp0, vsph[-1].value);
			asm.movq_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I64_MUL); {
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			asm.q.imul_r_m(regs.tmp0, vsph[-2].value);
			asm.movq_m_r(vsph[-2].value, regs.tmp0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.I64_DIV_S); {
			computeCurIpForTrap(-1);
			var div = X86_64Label.new(), neg = X86_64Label.new(), done = X86_64Label.new();
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			asm.cmp_r_i(regs.tmp0, -1);
			asm.jc_rel_near(C.NZ, div);
			asm.movq_r_m(regs.tmp0, vsph[-2].value);
			asm.movq_r_i(regs.tmp1, 0x80);
			asm.shl_r_i(regs.tmp1, 56);
			asm.cmp_r_r(regs.tmp0, regs.tmp1);
			asm.jc_rel_near(C.NZ, neg);
			asm.jmp_rel_far(trap_labels[TrapReason.DIV_UNREPRESENTABLE.tag]); // XXX: invert
			asm.bind(neg);
			asm.neg_m(vsph[-2].value);
			asm.jmp_rel_near(done);
			asm.bind(div);
			spillReg(R.RAX);
			spillReg(R.RDX);
			asm.movq_r_m(R.RAX, vsph[-2].value);
			asm.cqo();
			asm.idiv_r(regs.tmp0);
			asm.movq_m_r(vsph[-2].value, R.RAX);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			asm.bind(done);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.I64_DIV_U); {
			computeCurIpForTrap(-1);
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			spillReg(R.RAX);
			spillReg(R.RDX);
			asm.movq_r_m(R.RAX, vsph[-2].value);
			asm.movd_r_i(R.RDX, 0);
			asm.div_r(regs.tmp0);
			asm.movq_m_r(vsph[-2].value, R.RAX);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.I64_REM_S); {
			computeCurIpForTrap(-1);
			var div = X86_64Label.new(), done = X86_64Label.new();
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			asm.cmp_r_i(regs.tmp0, -1);
			asm.jc_rel_near(C.NZ, div);
			asm.movq_m_i(vsph[-2].value, 0);
			asm.jmp_rel_near(done);
			asm.bind(div);
			spillReg(R.RAX);
			spillReg(R.RDX);
			asm.movq_r_m(R.RAX, vsph[-2].value);
			asm.cqo();
			asm.idiv_r(regs.tmp0);
			asm.movq_m_r(vsph[-2].value, R.RDX);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			asm.bind(done);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.I64_REM_U); {
			computeCurIpForTrap(-1);
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			spillReg(R.RAX);
			spillReg(R.RDX);
			asm.movq_r_m(R.RAX, vsph[-2].value);
			asm.movd_r_i(R.RDX, 0);
			asm.div_r(regs.tmp0);
			asm.movq_m_r(vsph[-2].value, R.RDX);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.I64_ADD, asm.q.add_m_r),
			(Opcode.I64_SUB, asm.q.sub_m_r),
			(Opcode.I64_AND, asm.q.and_m_r),
			(Opcode.I64_OR, asm.q.or_m_r),
			(Opcode.I64_XOR, asm.q.xor_m_r)
		]) {
			bindHandler(t.0);
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			t.1(vsph[-2].value, regs.tmp0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.I64_SHL, asm.q.shl_m_cl),
			(Opcode.I64_SHR_S, asm.q.sar_m_cl),
			(Opcode.I64_SHR_U, asm.q.shr_m_cl),
			(Opcode.I64_ROTL, asm.q.rol_m_cl),
			(Opcode.I64_ROTR, asm.q.ror_m_cl)
		]) {
			bindHandler(t.0);
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			t.1(vsph[-2].value);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
	}
	def genExtensions() {
		bindHandler(Opcode.I32_WRAP_I64); {
			genTagUpdate(BpTypeCode.I32.code);
			endHandler();
		}
		bindHandler(Opcode.I32_REINTERPRET_F32); {
			genTagUpdate(BpTypeCode.I32.code);
			endHandler();
		}
		bindHandler(Opcode.I64_REINTERPRET_F64); {
			genTagUpdate(BpTypeCode.I64.code);
			endHandler();
		}
		bindHandler(Opcode.F32_REINTERPRET_I32); {
			genTagUpdate(BpTypeCode.F32.code);
			endHandler();
		}
		bindHandler(Opcode.F64_REINTERPRET_I64); {
			genTagUpdate(BpTypeCode.F64.code);
			endHandler();
		}
		bindHandler(Opcode.I32_EXTEND8_S); {
			asm.d.movbsx_r_m(regs.tmp0, vsph[-1].value);
			asm.movd_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I32_EXTEND16_S); {
			asm.d.movwsx_r_m(regs.tmp0, vsph[-1].value);
			asm.movd_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I64_EXTEND8_S); {
			asm.q.movbsx_r_m(regs.tmp0, vsph[-1].value);
			asm.movq_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I64_EXTEND16_S); {
			asm.q.movwsx_r_m(regs.tmp0, vsph[-1].value);
			asm.movq_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I64_EXTEND_I32_S);
		bindHandler(Opcode.I64_EXTEND32_S); {
			genTagUpdate(BpTypeCode.I64.code);
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			asm.q.shl_r_i(regs.tmp0, 32);
			asm.q.sar_r_i(regs.tmp0, 32);
			asm.movq_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I64_EXTEND_I32_U); {
			genTagUpdate(BpTypeCode.I64.code);
			asm.movd_m_i(regs.VSP.plus(-4), 0); // zero upper portion
			endHandler();
		}
	}
	def genF32Arith() {
		bindHandler(Opcode.F32_ABS); {
			asm.d.and_m_i(vsph[-1].value, 0x7FFFFFFF); // explicit update of upper word
			endHandler();
		}
		bindHandler(Opcode.F32_NEG); {
			asm.d.xor_m_i(vsph[-1].value, 0x80000000); // explicit update of upper word
			endHandler();
		}
		bindHandler(Opcode.F32_ADD); {
			asm.movss_s_m(regs.xmm0, vsph[-2].value);
			asm.addss_s_m(regs.xmm0, vsph[-1].value);
			asm.movss_m_s(vsph[-2].value, regs.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F32_SUB); {
			asm.movss_s_m(regs.xmm0, vsph[-2].value);
			asm.subss_s_m(regs.xmm0, vsph[-1].value);
			asm.movss_m_s(vsph[-2].value, regs.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F32_MUL); {
			asm.movss_s_m(regs.xmm0, vsph[-2].value);
			asm.mulss_s_m(regs.xmm0, vsph[-1].value);
			asm.movss_m_s(vsph[-2].value, regs.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F32_DIV); {
			asm.movss_s_m(regs.xmm0, vsph[-2].value);
			asm.divss_s_m(regs.xmm0, vsph[-1].value);
			asm.movss_m_s(vsph[-2].value, regs.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F32_SQRT); {
			asm.sqrtss_s_m(regs.xmm0, vsph[-1].value);
			asm.movss_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_COPYSIGN); {
			asm.movd_r_m(regs.tmp0, vsph[-2].value); // XXX: tradeoff between memory operands and extra regs?
			asm.d.and_r_i(regs.tmp0, 0x7FFFFFFF);
			asm.movd_r_m(regs.tmp1, vsph[-1].value);
			asm.d.and_r_i(regs.tmp1, 0x80000000);
			asm.d.or_r_r(regs.tmp0, regs.tmp1);
			asm.movd_m_r(vsph[-2].value, regs.tmp0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.F32_CEIL, X86_64Rounding.TO_POS_INF),
			(Opcode.F32_FLOOR, X86_64Rounding.TO_NEG_INF),
			(Opcode.F32_TRUNC, X86_64Rounding.TO_ZERO),
			(Opcode.F32_NEAREST, X86_64Rounding.TO_NEAREST)
		]) {
			bindHandler(t.0);
			asm.movss_s_m(regs.xmm0, vsph[-1].value);
			asm.roundss_s_s(regs.xmm0, regs.xmm0, t.1);
			asm.movss_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
	}
	def genF64Arith() {
		bindHandler(Opcode.F64_ABS); {
			asm.d.and_m_i(vsph[-1].upper, 0x7FFFFFFF);
			endHandler();
		}
		bindHandler(Opcode.F64_NEG); {
			asm.d.xor_m_i(vsph[-1].upper, 0x80000000);
			endHandler();
		}
		bindHandler(Opcode.F64_ADD); {
			asm.movsd_s_m(regs.xmm0, vsph[-2].value);
			asm.addsd_s_m(regs.xmm0, vsph[-1].value);
			asm.movsd_m_s(vsph[-2].value, regs.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F64_SUB); {
			asm.movsd_s_m(regs.xmm0, vsph[-2].value);
			asm.subsd_s_m(regs.xmm0, vsph[-1].value);
			asm.movsd_m_s(vsph[-2].value, regs.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F64_MUL); {
				asm.movsd_s_m(regs.xmm0, vsph[-2].value);
				asm.mulsd_s_m(regs.xmm0, vsph[-1].value);
				asm.movsd_m_s(vsph[-2].value, regs.xmm0);
				asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F64_DIV); {
			asm.movsd_s_m(regs.xmm0, vsph[-2].value);
			asm.divsd_s_m(regs.xmm0, vsph[-1].value);
			asm.movsd_m_s(vsph[-2].value, regs.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F64_SQRT); {
			asm.sqrtsd_s_m(regs.xmm0, vsph[-1].value);
			asm.movsd_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_COPYSIGN); {
			asm.movd_r_m(regs.tmp0, vsph[-2].upper); // XXX: tradeoff between memory operands and extra regs?
			asm.d.and_r_i(regs.tmp0, 0x7FFFFFFF);
			asm.movd_r_m(regs.tmp1, vsph[-1].upper);
			asm.d.and_r_i(regs.tmp1, 0x80000000);
			asm.d.or_r_r(regs.tmp0, regs.tmp1);
			asm.movd_m_r(vsph[-2].upper, regs.tmp0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.F64_CEIL, X86_64Rounding.TO_POS_INF),
			(Opcode.F64_FLOOR, X86_64Rounding.TO_NEG_INF),
			(Opcode.F64_TRUNC, X86_64Rounding.TO_ZERO),
			(Opcode.F64_NEAREST, X86_64Rounding.TO_NEAREST)
		]) {
			bindHandler(t.0);
			asm.movsd_s_m(regs.xmm0, vsph[-1].value);
			asm.roundsd_s_s(regs.xmm0, regs.xmm0, t.1);
			asm.movsd_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
	}
	def genFloatCmps() {
		var ret_zero = X86_64Label.new(), ret_one = X86_64Label.new();
		for (t in [
			(Opcode.F32_EQ, C.NZ),
			(Opcode.F32_NE, C.Z),
			(Opcode.F32_LT, C.NC),
			(Opcode.F32_GT, C.NA),
			(Opcode.F32_LE, C.A),
			(Opcode.F32_GE, C.C)]) {
			bindHandler(t.0);
			asm.movss_s_m(regs.xmm0, vsph[-2].value);
			asm.ucomiss_s_m(regs.xmm0, vsph[-1].value);
			asm.jc_rel_near(C.P, if(t.0 == Opcode.F32_NE, ret_one, ret_zero));
			asm.jc_rel_near(t.1, ret_zero);
			asm.jmp_rel_near(ret_one);
		}

		asm.bind(ret_zero);
		asm.q.sub_r_i(vsp, valuerep.slot_size);
		genTagUpdate(BpTypeCode.I32.code);
		asm.movd_m_i(vsph[-1].value, 0);
		endHandler();

		asm.bind(ret_one);
		asm.q.sub_r_i(vsp, valuerep.slot_size);
		genTagUpdate(BpTypeCode.I32.code);
		asm.movd_m_i(vsph[-1].value, 1);
		endHandler();

		// XXX: too far of a near jump to share these between f32 and f64
		ret_zero = X86_64Label.new();
		ret_one = X86_64Label.new();
		for (t in [
			(Opcode.F64_EQ, C.NZ),
			(Opcode.F64_NE, C.Z),
			(Opcode.F64_LT, C.NC),
			(Opcode.F64_GT, C.NA),
			(Opcode.F64_LE, C.A),
			(Opcode.F64_GE, C.C)]) {
			bindHandler(t.0);
			asm.movsd_s_m(regs.xmm0, vsph[-2].value);
			asm.ucomisd_s_m(regs.xmm0, vsph[-1].value);
			asm.jc_rel_near(C.P, if(t.0 == Opcode.F64_NE, ret_one, ret_zero));
			asm.jc_rel_near(t.1, ret_zero);
			asm.jmp_rel_near(ret_one);
		}

		asm.bind(ret_zero);
		asm.q.sub_r_i(vsp, valuerep.slot_size);
		genTagUpdate(BpTypeCode.I32.code);
		asm.movd_m_i(vsph[-1].value, 0);
		genDispatchOrJumpToDispatch();

		asm.bind(ret_one);
		asm.q.sub_r_i(vsp, valuerep.slot_size);
		genTagUpdate(BpTypeCode.I32.code);
		asm.movd_m_i(vsph[-1].value, 1);
		genDispatchOrJumpToDispatch();
	}
	def genMisc() {
		bindHandler(Opcode.MEMORY_SIZE); {
			genReadUleb32(regs.tmp1);
			asm.movq_r_m(regs.tmp0, regs.INSTANCE.plus(offsets.Instance_memories));
			asm.movq_r_m(regs.tmp0, regs.tmp0.plusR(regs.tmp1, offsets.REF_SIZE, offsets.Array_contents));
			asm.movq_r_m(regs.tmp1, regs.tmp0.plus(offsets.X86_64Memory_limit));
			asm.movq_r_m(regs.tmp0, regs.tmp0.plus(offsets.X86_64Memory_start));
			asm.q.sub_r_r(regs.tmp1, regs.tmp0);
			asm.q.shr_r_i(regs.tmp1, 16);
			genTagPush(BpTypeCode.I32.code);
			asm.movq_m_r(vsph[0].value, regs.tmp1);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.REF_NULL); {
			asm.movbzx_r_m(regs.tmp0, regs.IP_ptr);
			genSkipLeb(); // skip type
			genTagPushR(regs.tmp0);
			asm.movq_m_i(vsph[0].value, 0);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.REF_IS_NULL); {
			asm.d.test_m_i(vsph[-1].value, -1);
			asm.set_r(C.Z, regs.tmp0);
			asm.movbzx_r_r(regs.tmp0, regs.tmp0);
			if (valuerep.tagged) asm.movd_m_i(vsph[-1].tag, i7.view(BpTypeCode.I32.code));
			asm.movd_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.REF_FUNC); {
			genReadUleb32(regs.tmp1);
			asm.movq_r_m(regs.tmp0, regs.INSTANCE.plus(offsets.Instance_functions));
			asm.movq_r_m(regs.tmp0, regs.tmp0.plusR(regs.tmp1, offsets.REF_SIZE, offsets.Array_contents));
			genTagPush(BpTypeCode.FUNCREF.code);
			asm.movq_m_r(vsph[0].value, regs.tmp0);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.DATA_DROP); {
			genReadUleb32(regs.tmp1);
			asm.movq_r_m(regs.tmp0, regs.INSTANCE.plus(offsets.Instance_dropped_data));
			asm.movb_m_i(regs.tmp0.plusR(regs.tmp1, 1, offsets.Array_contents), 1);
			endHandler();
		}
		bindHandler(Opcode.ELEM_DROP); {
			genReadUleb32(regs.tmp1);
			asm.movq_r_m(regs.tmp0, regs.INSTANCE.plus(offsets.Instance_dropped_elems));
			asm.movb_m_i(regs.tmp0.plusR(regs.tmp1, 1, offsets.Array_contents), 1);
			endHandler();
		}
		bindHandler(Opcode.TABLE_SIZE); {
			genReadUleb32(regs.tmp1);
			asm.movq_r_m(regs.tmp0, regs.INSTANCE.plus(offsets.Instance_tables));
			asm.movq_r_m(regs.tmp0, regs.tmp0.plusR(regs.tmp1, offsets.REF_SIZE, offsets.Array_contents));
			asm.movq_r_m(regs.tmp0, regs.tmp0.plus(offsets.Table_elems));
			asm.movq_r_m(regs.tmp0, regs.tmp0.plus(offsets.Array_length));
			genTagPush(BpTypeCode.I32.code);
			asm.movq_m_r(vsph[0].value, regs.tmp0);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		writeDispatchEntry(dispatchTables[0].1, InternalOpcode.PROBE.code, w.atEnd().pos); {
			computeCurIpFromIp(-1);
			computePcFromCurIp();
			saveCallerIVars();
			asm.movq_r_m(regs.tmp0, frame.WASM_FUNC); // XXX: compute func and pc directly in the right regs
			callRuntime(refRuntimeCall(this.i.runtime_PROBE_instr), [regs.tmp0, regs.CURPC], true);
			restoreCallerIVars();
			// Compute a pointer to the original code at this pc offset
			var pc = regs.tmp1; // = IP - CODE
			asm.movq_r_r(pc, regs.IP);
			asm.sub_r_m(pc, frame.CODE);
			var origIp = regs.tmp0; // FUNC_DECL.orig_bytecode + pc - 1
			asm.movq_r_m(origIp, regs.FUNC_DECL.plus(offsets.FuncDecl_orig_bytecode));
			asm.add_r_r(origIp, pc);
			asm.sub_r_i(origIp, 1);
			genDispatch(origIp.indirect(), dispatchTables[0].1, false);
		}
		if (tuning.dispatchTableReg) {
			var offset = w.atEnd().pos;
			for (i < 256) {
				writeDispatchEntry(probedDispatchTableRef, i, offset);
			}
			computeCurIpForTrap(-1);
			computePcFromCurIp();
			saveCallerIVars();
			asm.movq_r_m(regs.tmp0, frame.WASM_FUNC); // XXX: compute func and pc directly in the right regs
			callRuntime(refRuntimeCall(this.i.runtime_PROBE_loop), [regs.tmp0, regs.CURPC], true);
			restoreCallerIVars();
			// TODO: reload code from function, as local probes may have been inserted or removed
			asm.sub_r_i(regs.IP, 1);
			genDispatch(regs.IP.indirect(), dispatchTables[0].1, true);
		}
	}
	def genFloatMinAndMax() {
		var ret_b = X86_64Label.new(), ret_a = X86_64Label.new(), is_nan32 = X86_64Label.new(), is_nan64 = X86_64Label.new();
		bindHandler(Opcode.F32_MIN);
		asm.movss_s_m(regs.xmm0, vsph[-2].value);
		asm.movss_s_m(regs.xmm1, vsph[-1].value);
		asm.ucomiss_s_s(regs.xmm0, regs.xmm1);
		asm.jc_rel_far(C.P, is_nan32);
		asm.jc_rel_near(C.C, ret_a);
		asm.jc_rel_near(C.A, ret_b);
		asm.d.cmp_m_i(vsph[-1].value, 0);
		asm.jc_rel_near(C.S, ret_b); // handle min(-0, 0) == -0
		asm.jmp_rel_near(ret_a);

		bindHandler(Opcode.F32_MAX);
		asm.movss_s_m(regs.xmm0, vsph[-2].value);
		asm.movss_s_m(regs.xmm1, vsph[-1].value);
		asm.ucomiss_s_s(regs.xmm0, regs.xmm1);
		asm.jc_rel_far(C.P, is_nan32);
		asm.jc_rel_near(C.C, ret_b);
		asm.jc_rel_near(C.A, ret_a);
		asm.d.cmp_m_i(vsph[-1].value, 0);
		asm.jc_rel_near(C.NS, ret_b); // handle max(-0, 0) == 0
		asm.jmp_rel_near(ret_a);

		bindHandler(Opcode.F64_MIN);
		asm.movsd_s_m(regs.xmm0, vsph[-2].value);
		asm.movsd_s_m(regs.xmm1, vsph[-1].value);
		asm.ucomisd_s_s(regs.xmm0, regs.xmm1);
		asm.jc_rel_near(C.P, is_nan64);
		asm.jc_rel_near(C.C, ret_a);
		asm.jc_rel_near(C.A, ret_b);
		asm.d.cmp_m_i(vsph[-1].upper, 0);
		asm.jc_rel_near(C.S, ret_b); // handle min(-0, 0) == -0
		// fall through to ret_a
		asm.bind(ret_a);
		asm.q.sub_r_i(vsp, valuerep.slot_size);
		endHandler();

		bindHandler(Opcode.F64_MAX);
		asm.movsd_s_m(regs.xmm0, vsph[-2].value);
		asm.movsd_s_m(regs.xmm1, vsph[-1].value);
		asm.ucomisd_s_s(regs.xmm0, regs.xmm1);
		asm.jc_rel_near(C.P, is_nan64);
		asm.jc_rel_near(C.C, ret_b);
		asm.jc_rel_near(C.A, ret_a);
		asm.d.cmp_m_i(vsph[-1].upper, 0);
		asm.jc_rel_near(C.S, ret_a); // handle max(-0, 0) == 0
		// fall through to ret_b
		asm.bind(ret_b);
		asm.movsd_m_s(vsph[-2].value, regs.xmm1);
		asm.q.sub_r_i(vsp, valuerep.slot_size);
		endHandler();

		asm.bind(is_nan32);
		asm.movd_m_i(vsph[-2].value, int.view(Floats.f_nan));
		asm.jmp_rel_near(ret_a);

		asm.bind(is_nan64);
		asm.movd_m_i(vsph[-2].upper, int.view(Floats.d_nan >> 32));
		asm.movd_m_i(vsph[-2].value, 0);
		asm.jmp_rel_near(ret_a);
	}
	def genFloatTruncs() {
		genFloatTrunc(Opcode.I32_TRUNC_F32_S, TRUNC_i32_f32_s, false);
		genFloatTrunc(Opcode.I32_TRUNC_F32_U, TRUNC_i32_f32_u, false);
		genFloatTrunc(Opcode.I32_TRUNC_F64_S, TRUNC_i32_f64_s, false);
		genFloatTrunc(Opcode.I32_TRUNC_F64_U, TRUNC_i32_f64_u, false);
		genFloatTrunc(Opcode.I64_TRUNC_F32_S, TRUNC_i64_f32_s, false);
		genFloatTrunc(Opcode.I64_TRUNC_F32_U, TRUNC_i64_f32_u, false);
		genFloatTrunc(Opcode.I64_TRUNC_F64_S, TRUNC_i64_f64_s, false);
		genFloatTrunc(Opcode.I64_TRUNC_F64_U, TRUNC_i64_f64_u, false);
		genFloatTrunc(Opcode.I32_TRUNC_SAT_F32_S, TRUNC_i32_f32_s, true);
		genFloatTrunc(Opcode.I32_TRUNC_SAT_F32_U, TRUNC_i32_f32_u, true);
		genFloatTrunc(Opcode.I32_TRUNC_SAT_F64_S, TRUNC_i32_f64_s, true);
		genFloatTrunc(Opcode.I32_TRUNC_SAT_F64_U, TRUNC_i32_f64_u, true);
		genFloatTrunc(Opcode.I64_TRUNC_SAT_F32_U, TRUNC_i64_f32_u, true);
		genFloatTrunc(Opcode.I64_TRUNC_SAT_F64_U, TRUNC_i64_f64_u, true);

		bindHandler(Opcode.I64_TRUNC_SAT_F32_S); {
			genTagUpdate(BpTypeCode.I64.code);
			asm.movss_s_m(regs.xmm0, vsph[-1].value);
			asm.movd_r_i(regs.tmp0, int.view(Floats.f_1p63));
			asm.movd_s_r(regs.xmm1, regs.tmp0);
			asm.ucomiss_s_s(regs.xmm0, regs.xmm1);
			var is_nan = X86_64Label.new(), ovf_pos = X86_64Label.new(), done = X86_64Label.new();
			asm.jc_rel_near(C.P, is_nan);
			asm.jc_rel_near(C.NC, ovf_pos);
			asm.roundss_s_s(regs.xmm0, regs.xmm0, X86_64Rounding.TO_ZERO);
			asm.q.cvtss2si_r_s(regs.tmp0, regs.xmm0);
			asm.bind(done);
			asm.movq_m_r(vsph[-1].value, regs.tmp0);
			genDispatchOrJumpToDispatch();
			asm.bind(is_nan);
			asm.movd_r_i(regs.tmp0, 0);
			asm.jmp_rel_near(done);
			asm.bind(ovf_pos);
			asm.movq_r_i(regs.tmp0, 0xFFFFFFFE);  // TODO: tricky constant
			asm.q.ror_r_i(regs.tmp0, 1); // result = 0x7FFFFFFF_FFFFFFFF
			asm.jmp_rel_near(done);
		}
		bindHandler(Opcode.I64_TRUNC_SAT_F64_S); {
			genTagUpdate(BpTypeCode.I64.code);
			asm.movsd_s_m(regs.xmm0, vsph[-1].value);
			asm.movd_r_i(regs.tmp0, int.view(Floats.d_1p63 >> 32));
			asm.q.shl_r_i(regs.tmp0, 32);
			asm.movq_s_r(regs.xmm1, regs.tmp0);
			asm.ucomisd_s_s(regs.xmm0, regs.xmm1);
			var is_nan = X86_64Label.new(), ovf_pos = X86_64Label.new(), done = X86_64Label.new();
			asm.jc_rel_near(C.P, is_nan);
			asm.jc_rel_near(C.NC, ovf_pos);
			asm.roundsd_s_s(regs.xmm0, regs.xmm0, X86_64Rounding.TO_ZERO);
			asm.q.cvtsd2si_r_s(regs.tmp0, regs.xmm0);
			asm.bind(done);
			asm.movq_m_r(vsph[-1].value, regs.tmp0);
			genDispatchOrJumpToDispatch();
			asm.bind(is_nan);
			asm.movd_r_i(regs.tmp0, 0);
			asm.jmp_rel_near(done);
			asm.bind(ovf_pos);
			asm.movq_r_i(regs.tmp0, 0xFFFFFFFE); // TODO: tricky constant
			asm.q.ror_r_i(regs.tmp0, 1); // result = 0x7FFFFFFF_FFFFFFFF
			asm.jmp_rel_near(done);
			endHandler();
		}
	}
	def genFloatConversions() {
		bindHandler(Opcode.F32_CONVERT_I32_S); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			asm.q.shl_r_i(regs.tmp0, 32);
			asm.q.sar_r_i(regs.tmp0, 32); // sign-extend
			asm.cvtsi2ss_s_r(regs.xmm0, regs.tmp0);
			asm.movss_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_CONVERT_I32_U); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			asm.cvtsi2ss_s_r(regs.xmm0, regs.tmp0);
			asm.movss_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_CONVERT_I64_S); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			asm.cvtsi2ss_s_r(regs.xmm0, regs.tmp0);
			asm.movss_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_CONVERT_I64_U); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			asm.q.cvtsi2ss_s_r(regs.xmm0, regs.tmp0);
			asm.q.cmp_r_i(regs.tmp0, 0);
			var done = X86_64Label.new();
			asm.jc_rel_near(C.NS, done);
			// input < 0, compute 2.0d * cvt((x >> 1) | (x&1))
			asm.movq_r_r(regs.tmp1, regs.tmp0);
			asm.q.and_r_i(regs.tmp1, 1);
			asm.q.shr_r_i(regs.tmp0, 1);
			asm.q.or_r_r(regs.tmp0, regs.tmp1);
			asm.q.cvtsi2ss_s_r(regs.xmm0, regs.tmp0);
			asm.movd_r_i(regs.tmp1, int.view(Floats.f_1p1));
			asm.movd_s_r(regs.xmm1, regs.tmp1);
			asm.mulss_s_s(regs.xmm0, regs.xmm1);
			// done
			asm.bind(done);
			asm.movss_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_DEMOTE_F64); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.movsd_s_m(regs.xmm0, vsph[-1].value);
			asm.cvtsd2ss_s_s(regs.xmm0, regs.xmm0);
			asm.movss_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_CONVERT_I32_S); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			asm.q.shl_r_i(regs.tmp0, 32);
			asm.q.sar_r_i(regs.tmp0, 32); // sign-extend
			asm.cvtsi2sd_s_r(regs.xmm0, regs.tmp0);
			asm.movsd_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_CONVERT_I32_U); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			asm.cvtsi2sd_s_r(regs.xmm0, regs.tmp0);
			asm.movsd_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_CONVERT_I64_S); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			asm.cvtsi2sd_s_r(regs.xmm0, regs.tmp0);
			asm.movsd_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_CONVERT_I64_U); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			asm.q.cvtsi2sd_s_r(regs.xmm0, regs.tmp0);
			asm.q.cmp_r_i(regs.tmp0, 0);
			var done = X86_64Label.new();
			asm.jc_rel_near(C.NS, done);
			// input < 0, compute 2.0d * cvt((x >> 1) | (x&1))
			asm.movq_r_r(regs.tmp1, regs.tmp0);
			asm.q.and_r_i(regs.tmp1, 1);
			asm.q.shr_r_i(regs.tmp0, 1);
			asm.q.or_r_r(regs.tmp0, regs.tmp1);
			asm.q.cvtsi2sd_s_r(regs.xmm0, regs.tmp0);
			asm.movd_r_i(regs.tmp1, int.view(Floats.d_1p1 >> 32));
			asm.q.shl_r_i(regs.tmp1, 32);
			asm.movq_s_r(regs.xmm1, regs.tmp1);
			asm.mulsd_s_s(regs.xmm0, regs.xmm1);
			// done
			asm.bind(done);
			asm.movsd_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_PROMOTE_F32); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.movss_s_m(regs.xmm0, vsph[-1].value);
			asm.cvtss2sd_s_s(regs.xmm0, regs.xmm0);
			asm.movsd_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
	}
	def genRuntimeCallOps() {
		// generate code for runtime calls with 1 LEB that cannot trap.
		var call_irt = asm.newLabel();
		for (t in [
			(Opcode.GLOBAL_GET, refRuntimeCall(i.runtime_GLOBAL_GET)),
			(Opcode.GLOBAL_SET, refRuntimeCall(i.runtime_GLOBAL_SET)),
			(Opcode.MEMORY_GROW, refRuntimeCall(i.runtime_MEMORY_GROW)),
			(Opcode.TABLE_GROW, refRuntimeCall(i.runtime_TABLE_GROW))
		]) {
			bindHandler(t.0);
			genReadUleb32(regs.tmp0);
			saveCallerIVars();
			callRuntime(t.1, [regs.INSTANCE, regs.tmp0], false);
			restoreCallerIVars();
			endHandler();
		}
		asm.bind(call_irt);

		// generate code for runtime calls with 1 LEB that can trap.
		call_irt = asm.newLabel();
		for (t in [
			(Opcode.TABLE_GET, refRuntimeCall(i.runtime_TABLE_GET)),
			(Opcode.TABLE_SET, refRuntimeCall(i.runtime_TABLE_SET)),
			(Opcode.MEMORY_FILL, refRuntimeCall(i.runtime_MEMORY_FILL)),
			(Opcode.TABLE_FILL, refRuntimeCall(i.runtime_TABLE_FILL))
		]) {
			bindHandler(t.0);
			genReadUleb32(regs.tmp0);
			saveCallerIVars();
			callRuntime(t.1, [regs.INSTANCE, regs.tmp0], true);
			restoreCallerIVars();
			endHandler();
		}
		asm.bind(call_irt);

		// generate code for runtime calls with 2 LEBS that can trap.
		call_irt = asm.newLabel();
		for (t in [
			(Opcode.TABLE_INIT, refRuntimeCall(i.runtime_TABLE_INIT)),
			(Opcode.MEMORY_INIT, refRuntimeCall(i.runtime_MEMORY_INIT)),
			(Opcode.MEMORY_COPY, refRuntimeCall(i.runtime_MEMORY_COPY)),
			(Opcode.TABLE_COPY, refRuntimeCall(i.runtime_TABLE_COPY))
		]) {
			bindHandler(t.0);
			genReadUleb32(regs.tmp0);
			genReadUleb32(regs.tmp1);
			saveCallerIVars();
			callRuntime(t.1, [regs.INSTANCE, regs.tmp0, regs.tmp1], true);
			genExecStateCheck();
			restoreCallerIVars();
			endHandler();
		}
		asm.bind(call_irt);
	}
	def bindHandler(opcode: Opcode) {
		if (tuning.handlerAlignment > 1) w.align(tuning.handlerAlignment);
		patchDispatchTable(opcode, w.atEnd().pos);
	}
	def bindHandlerNoAlign(opcode: Opcode) {
		patchDispatchTable(opcode, w.atEnd().pos);
	}
	def genFloatTrunc(opcode: Opcode, config: FloatTrunc, saturate: bool) {
		bindHandler(opcode);
		if (!saturate) computeCurIpForTrap(-1);
		config.mov_s_m(asm, regs.xmm0, vsph[-1].value);
		config.mov_s_i(asm, regs.xmm1, config.maxv, regs.scratch);
		config.ucomi_s_s(asm, regs.xmm0, regs.xmm1);
		var above = X86_64Label.new(), is_nan = X86_64Label.new(), below = X86_64Label.new();
		var ret = X86_64Label.new();
		asm.jc_rel_near(C.P, is_nan);
		asm.jc_rel_near(C.NC, above);
		var not_big = X86_64Label.new();

		if (config.isI64 && !config.isSigned) {
			// handle u64 convert of 1p63 < v <= 1p64
			config.mov_s_i(asm, regs.xmm1, if(config.isF64, Floats.d_1p63, Floats.f_1p63), regs.scratch);
			config.ucomi_s_s(asm, regs.xmm0, regs.xmm1);
			asm.jc_rel_near(C.C, not_big);
			config.sub_s_s(asm, regs.xmm0, regs.xmm1);
			config.round_s_s(asm, regs.xmm0, regs.xmm0, X86_64Rounding.TO_ZERO);
			config.cvt2si_r_s(asm.q, regs.tmp0, regs.xmm0);
			asm.movd_r_i(regs.tmp1, 1);
			asm.ror_r_i(regs.tmp1, 1);
			asm.q.add_r_r(regs.tmp0, regs.tmp1);
			asm.jmp_rel_near(ret);
		}
		asm.bind(not_big);

		if (!saturate || config.isI64 || !config.isSigned) {
			config.mov_s_i(asm, regs.xmm1, config.minv, regs.scratch);
			config.ucomi_s_s(asm, regs.xmm0, regs.xmm1);
			asm.jc_rel_near(C.NA, below); // v <= min
		}

		config.round_s_s(asm, regs.xmm0, regs.xmm0, X86_64Rounding.TO_ZERO);
		if (!config.isI64 && config.isSigned) {
			config.cvt2si_r_s(asm.d, regs.tmp0, regs.xmm0);
		} else {
			config.cvt2si_r_s(asm.q, regs.tmp0, regs.xmm0);
		}
		asm.bind(ret);
		config.mov_m_r(asm, vsph[-1].value, regs.tmp0);
		genTagUpdate(config.tag);
		genDispatchOrJumpToDispatch();
		if (saturate) {
			asm.bind(above);
			config.mov_r_i(asm, regs.tmp0, config.ceilv);
			asm.jmp_rel_near(ret);
			asm.bind(is_nan);
			asm.bind(below);
			asm.movd_r_i(regs.tmp0, 0);
			asm.jmp_rel_near(ret);
		} else {  // XXX: share trap code among all float truncs
			asm.bind(above);
			asm.bind(below);
			asm.bind(is_nan);
			asm.jmp_rel_far(trap_labels[TrapReason.FLOAT_UNREPRESENTABLE.tag]);
		}
	}
	def genExecStateCheck() {
		asm.q.cmp_r_i(Target.V3_RET_GPRS[0], 0);
		asm.jc_rel_far(C.NZ, abruptRetLabel);
	}
	def saveCallerIVars() {
		saveIVar(regs.IP);
		saveIVar(regs.STP);
		if (tuning.recordCurIpForTraps) saveIVar(regs.CURPC);
	}
	def restoreCallerIVars() {
		restoreReg(regs.IP);
		restoreReg(regs.STP);
		restoreReg(regs.EIP);
		restoreReg(regs.INSTANCE);
		restoreReg(regs.FUNC_DECL);
		restoreReg(regs.MEM0_BASE);
		restoreReg(regs.VFP);
	}
	def callRuntime(abs: Pointer, args: Array<X86_64Gpr>, canTrap: bool) {
		saveIVar(regs.VSP);
		// save a copy of VSP into valueStack.sp
		asm.movq_r_m(regs.scratch, absPointer(offsets.Interpreter_valueStack));
		asm.movq_m_r(regs.scratch.plus(offsets.ValueStack_sp), regs.VSP);
		// Generate parallel moves from args into param gprs; assume each src register used only once
		var dst = Array<X86_64Gpr>.new(G.length);
		for (i < args.length) {
			var sreg = args[i];
			var dreg = Target.V3_PARAM_GPRS[i + 1];
			if (sreg != dreg) dst[sreg.regnum] = dreg;
		}
		var stk = Array<i8>.new(G.length);
		for (i < dst.length) orderMoves(dst, stk, i);
		// emit actual call
		asm.callr(int.!(abs - (ic.start + w.pos + 5)));
		// check for trap
		if (canTrap) genExecStateCheck();
		if (tuning.dispatchTableReg) {
			// restore dispatch table from interpreter.dispatchTable
			asm.movq_r_m(regs.DISPATCH_TABLE, absPointer(offsets.Interpreter_dispatchTable));
		}
		// restore VSP from valueStack.sp
		asm.movq_r_m(regs.VSP, absPointer(offsets.Interpreter_valueStack));
		asm.movq_r_m(regs.VSP, regs.VSP.plus(offsets.ValueStack_sp));
	}
	def absPointer(ptr: Pointer) -> X86_64Addr {
		return X86_64Addr.new(null, null, 1, int.!(ptr - Pointer.NULL));
	}
	def orderMoves(dst: Array<X86_64Gpr>, stk: Array<i8>, i: int) {
		var dreg = dst[i];
		if (dreg == null) return;		// no moves here
		if (stk[i] > 0) return;			// this node already done
		stk[i] = -1;				// mark as on stack
		if (stk[dreg.regnum] < 0) {		// destination on stack => cycle
			asm.movq_r_r(regs.scratch, dreg);	// save destination first
			stk[dreg.regnum] = -2;		// mark as cycle
		} else {
			orderMoves(dst, stk, dreg.regnum);	// recurse on destination
		}
		asm.movq_r_r(dreg, if(stk[i] == -2, regs.scratch, G[i]));	// emit post-order move
		stk[i] = 1;				// mark as done
	}
	def genTagUpdate(tag: byte) {
		if (valuerep.tagged) asm.movq_m_i(vsph[-1].tag, tag);
	}
	def genTagPush(tag: byte) {
		if (valuerep.tagged) asm.movq_m_i(vsph[0].tag, i7.view(tag));
	}
	def genTagPushR(r: X86_64Gpr) {
		if (valuerep.tagged) asm.movq_m_r(vsph[0].tag, r);
	}
	def genCopySlot(dst: X86_64Addr, src: X86_64Addr) {
		if (valuerep.slot_size == 16) {
			asm.movdqu_s_m(regs.xmm0, src);
			asm.movdqu_m_s(dst, regs.xmm0);
		} else {
			asm.movq_r_m(regs.scratch, src);
			asm.movq_m_r(dst, regs.scratch);
		}
	}
	def saveIVar(r: X86_64Gpr) {
		for (t in all_ivars) {
			if (t.0 == r) asm.movq_m_r(t.1, r);
		}
	}
	def spillReg(r: X86_64Gpr) {
		for (t in mutable_ivars) {
			if (t.0 == r) asm.movq_m_r(t.1, r);
		}
	}
	def restoreReg(r: X86_64Gpr) {
		for (t in all_ivars) {
			if (t.0 == r) asm.movq_r_m(r, t.1);
		}
	}
	def genLoad(opcode: Opcode, tag: byte, gen: (X86_64Gpr, X86_64Addr) -> X86_64Assembler) {
		bindHandler(opcode);
		computeCurIpForTrap(-1);
		asm.q.inc_r(regs.IP);				// skip flags byte
		genReadUleb32(regs.tmp0);			// decode offset
		asm.movd_r_m(regs.tmp1, vsph[-1].value);	// read index
		asm.q.add_r_r(regs.tmp0, regs.tmp1);		// add index + offset
		gen(regs.tmp1, regs.MEM0_BASE.plusR(regs.tmp0, 1, 0));
		if (valuerep.tagged && tag != BpTypeCode.I32.code) genTagUpdate(tag); // update tag if necessary
		asm.movq_m_r(vsph[-1].value, regs.tmp1);
		endHandler();
	}
	def genStore(gen: (X86_64Addr, X86_64Gpr) -> X86_64Assembler) {
		computeCurIpForTrap(-1);
		asm.q.inc_r(regs.IP);				// skip flags byte
		genReadUleb32(regs.tmp0);			// decode offset
		asm.movd_r_m(regs.tmp1, vsph[-2].value);	// read index
		asm.q.add_r_r(regs.tmp0, regs.tmp1);		// add index + offset
		asm.movq_r_m(regs.tmp1, vsph[-1].value);	// read value
		gen(regs.MEM0_BASE.plusR(regs.tmp0, 1, 0), regs.tmp1);
		asm.q.sub_r_i(vsp, 2 * valuerep.slot_size);
		endHandler();
	}
	def genPopFrameAndRet() {
		genInvalidateFrameAccessor();
		asm.q.add_r_i(R.RSP, ic.frameSize);
		asm.ret();
	}
	def genInvalidateFrameAccessor() {
		if (tuning.cacheFrameAccessor) asm.movq_m_i(frame.ACCESSOR, 0);
	}

	// Generate a read of a 32-bit unsigned LEB.
	def genReadUleb32(dest: X86_64Gpr) {
		var ool_leb: OutOfLineLEB;
		if (!tuning.inlineAllLEBs) {
			ool_leb = OutOfLineLEB.new(dest);
			oolULeb32Sites.put(ool_leb);
		}
		var asm = this.asm.d;
		asm.movbzx_r_m(dest, regs.IP_ptr);	// load first byte
		asm.q.inc_r(regs.IP);			// increment pointer
		asm.test_r_i(dest, 0x80);		// test most-significant bit
		if (tuning.inlineAllLEBs) {
			var leb_done = X86_64Label.new();
			asm.jc_rel_near(C.Z, leb_done);
			genReadLEBext(dest);
			asm.bind(leb_done);
		} else {
			asm.jc_rel_addr(C.NZ, ool_leb);
			ool_leb.retOffset = asm.pos();
		}
	}
	// Generate a read of a 32-bit signed LEB.
	def genReadSleb32_inline(dest: X86_64Gpr) {
		var done = X86_64Label.new(), sext = X86_64Label.new(), loop = X86_64Label.new();
		asm.movd_r_i(R.RCX, 0);
		asm.movd_r_i(dest, 0);

		asm.bind(loop);
		asm.movbzx_r_m(regs.scratch, regs.IP_ptr);	// load byte
		asm.q.inc_r(regs.IP);			// increment pointer
		asm.d.test_r_i(regs.scratch, 0x80);	// test most-significant bit
		asm.jc_rel_near(C.Z, sext);		// break if not set
		asm.d.and_r_i(regs.scratch, 0x7F);	// mask off upper bit
		asm.d.shl_r_cl(regs.scratch);		// shift byte into correct bit pos
		asm.d.or_r_r(dest, regs.scratch);	// merge byte into val
		asm.d.add_r_i(R.RCX, 7);		// compute next bit pos
		asm.jmp_rel_near(loop);			// loop

		asm.bind(sext);
		asm.d.shl_r_cl(regs.scratch);		// shift byte into correct bit pos
		asm.d.or_r_r(dest, regs.scratch);	// merge byte into val
		asm.d.sub_r_i(R.RCX, 25);		// compute 25 - shift
		asm.d.neg_r(R.RCX);
		asm.jc_rel_near(C.S, done);		// if shift > 25, done
		asm.d.shl_r_cl(dest);			// sign extension
		asm.d.sar_r_cl(dest);
		asm.bind(done);
	}
	// Generate a read of a 64-bit signed LEB.
	def genReadSleb64_inline(dest: X86_64Gpr) {
		var done = X86_64Label.new(), sext = X86_64Label.new(), loop = X86_64Label.new();
		asm.movd_r_i(R.RCX, 0);
		asm.movd_r_i(dest, 0);

		asm.bind(loop);
		asm.movbzx_r_m(regs.scratch, regs.IP_ptr);	// load byte
		asm.q.inc_r(regs.IP);			// increment pointer
		asm.d.test_r_i(regs.scratch, 0x80);	// test most-significant bit
		asm.jc_rel_near(C.Z, sext);		// break if not set
		asm.d.and_r_i(regs.scratch, 0x7F);	// mask off upper bit
		asm.q.shl_r_cl(regs.scratch);		// shift byte into correct bit pos
		asm.q.or_r_r(dest, regs.scratch);	// merge byte into val
		asm.d.add_r_i(R.RCX, 7);		// compute next bit pos
		asm.jmp_rel_near(loop);			// loop

		asm.bind(sext);
		asm.q.shl_r_cl(regs.scratch);		// shift byte into correct bit pos
		asm.q.or_r_r(dest, regs.scratch);	// merge byte into val
		asm.d.sub_r_i(R.RCX, 57);		// compute 57 - shift
		asm.d.neg_r(R.RCX);
		asm.jc_rel_near(C.S, done);		// if shift > 57, done
		asm.q.shl_r_cl(dest);			// sign extension
		asm.q.sar_r_cl(dest);
		asm.bind(done);
	}
	// Generate code which skips over an LEB.
	def genSkipLeb() {
		var more = X86_64Label.new();
		asm.bind(more);
		asm.movbzx_r_m(regs.scratch, regs.IP_ptr);	// load first byte
		asm.q.inc_r(regs.IP);			// increment pointer
		asm.test_r_i(regs.scratch, 0x80);	// test most-significant bit
		asm.jc_rel_near(C.NZ, more);
	}
	// End the handler for the current bytecode
	def endHandler() {
		genDispatchOrJumpToDispatch();
	}
	// Generate an inline dispatch or a jump to the dispatch loop, depending on config.
	def genDispatchOrJumpToDispatch() {
		var gen = tuning.threadedDispatch;
		if (firstDispatchOffset == 0) {
			firstDispatchOffset = w.pos;
			gen = true;
		}
		if (gen) {
			genDispatch(regs.IP_ptr, if (!tuning.dispatchTableReg, dispatchTables[0].1), true);
		} else {
			asm.jmp_rel(firstDispatchOffset - w.atEnd().pos);
		}
	}
	// Generate a load of the next bytecode and a dispatch through the dispatch table.
	def genDispatch(ptr: X86_64Addr, table: IcCodeRef, increment: bool) {
		var opcode = regs.tmp0;
		var base = regs.tmp1;
		if (ptr != null) asm.movbzx_r_m(opcode, ptr);
		if (increment) asm.inc_r(regs.IP);
		match (tuning.dispatchEntrySize) {
			2 => {
				if (table == null) asm.movq_r_r(base, regs.DISPATCH_TABLE);
				else asm.lea(base, table); // RIP-relative LEA
				asm.movwsx_r_m(opcode, base.plusR(opcode, 2, 0)); // load 16-bit offset
				asm.add_r_r(base, opcode);
				if (dispatchJmpOffset < 0) dispatchJmpOffset = w.pos;
				asm.ijmp_r(base);
			}
			4 => {
				if (table == null) {
					asm.movd_r_m(base, regs.DISPATCH_TABLE.plusR(opcode, 4, 0));
				} else {
					var addr = ic.start + table.offset;
					asm.movd_r_m(base, X86_64Addr.new(null, opcode, 4, int.!(addr - Pointer.NULL)));
				}
				if (dispatchJmpOffset < 0) dispatchJmpOffset = w.pos;
				asm.ijmp_r(base);
			}
			8 => {
				if (table == null) {
					if (dispatchJmpOffset < 0) dispatchJmpOffset = w.pos;
					asm.ijmp_m(regs.DISPATCH_TABLE.plusR(opcode, 8, 0));
				} else {
					var addr = ic.start + table.offset;
					if (dispatchJmpOffset < 0) dispatchJmpOffset = w.pos;
					asm.ijmp_m(X86_64Addr.new(null, opcode, 8, int.!(addr - Pointer.NULL)));
				}
			}
		}
	}
	// Patch the dispatch table for the given opcode to go to the given position.
	def patchDispatchTable(opcode: Opcode, pos: int) {
		for (t in dispatchTables) {
			if (t.0 != opcode.prefix) continue;
			var ref1 = t.1;
			if (opcode.prefix == 0 || opcode.code < 128) writeDispatchEntry(ref1, opcode.code, pos);
			var ref2 = t.2;
			if (ref2 != null) writeDispatchEntry(ref2, opcode.code, pos);
			w.atEnd();
			return;
		}
		fatal("no dispatch table found for prefix");
	}
	// Generate the out-of-line LEB decoding code.
	def genOutOfLineLEBs() { // XXX: use a separate out-of-line assembler on the end of the buffer
		for (i < oolULeb32Sites.length) {
			var o = oolULeb32Sites[i];
			var pos = w.atEnd().pos;
			w.at(o.pos).put_b32(pos - (o.pos + o.delta));
			w.atEnd();
			// XXX: share code between out-of-line LEB cases
			genReadLEBext(o.dest);
			asm.jmp_rel(o.retOffset - w.atEnd().pos);
		}
		oolULeb32Sites = null;
	}
	// Generate code for > 1 byte LEB cases
	def genReadLEBext(dest: X86_64Gpr) {
		var destRcx = dest == R.RCX;
		asm.d.and_r_i(dest, 0x7F);		// mask off upper bit of first byte
		if (destRcx) {
			asm.movd_r_r(regs.tmp3, dest);
			dest = regs.tmp3;
		} else {
			asm.movd_r_r(regs.tmp3, R.RCX);	// save RCX
		}
		asm.movd_r_i(R.RCX, 7);
		var loop = X86_64Label.new(), nomore = X86_64Label.new();
		asm.bind(loop);
		asm.movbzx_r_m(regs.scratch, regs.IP_ptr);	// load byte
		asm.q.inc_r(regs.IP);				// increment pointer
		asm.d.test_r_i(regs.scratch, 0x80);		// test most-significant bit
		asm.jc_rel_near(C.Z, nomore);			// break if not set
		asm.d.and_r_i(regs.scratch, 0x7F);		// mask off upper bit
		asm.d.shl_r_cl(regs.scratch);			// shift byte into correct bit pos
		asm.d.or_r_r(dest, regs.scratch);		// merge byte into val
		asm.d.add_r_i(R.RCX, 7);			// compute next bit pos
		asm.jmp_rel_near(loop);				// loop

		asm.bind(nomore);
		asm.d.shl_r_cl(regs.scratch);		// shift byte into correct bit pos
		asm.d.or_r_r(dest, regs.scratch);	// merge byte into val
		if (destRcx) asm.movd_r_r(R.RCX, dest);
		else asm.movd_r_r(R.RCX, regs.tmp3);	// restore RCX
	}
	// Runtime calls and traps need CURPC register to be valid.
	def computeCurIpForTrap(delta: int) {
		if (tuning.recordCurIpForTraps) computeCurIpFromIp(delta);
	}
	def computeCurIpFromIp(delta: int) {
		asm.q.lea(regs.CURPC, X86_64Addr.new(regs.IP, null, 1, delta - offsets.Array_contents));
	}
	def computePcFromCurIp() {
		if (tuning.recordCurIpForTraps) asm.q.sub_r_m(regs.CURPC, frame.CODE);
	}
	// All traps are generated out-of-line and call into the runtime.
	def genTraps() {
		w.atEnd();

		var call_runtime_TRAP = X86_64Label.new();
		asm.bind(call_runtime_TRAP);
		computePcFromCurIp();
		saveCallerIVars();
		asm.movq_r_m(regs.tmp1, frame.WASM_FUNC);
		// XXX: load runtime arg registers directly
		callRuntime(refRuntimeCall(this.i.runtime_TRAP), [regs.tmp1, regs.CURPC, regs.tmp4], true);
		asm.bind(abruptRetLabel);
		genPopFrameAndRet();

		for (reason in TrapReason) {
			if (reason == TrapReason.STACK_OVERFLOW) continue; // must be special
			if (reason == TrapReason.DIV_BY_ZERO) continue; // must be special
			asm.bind(trap_labels[reason.tag]);
			asm.movd_r_i(regs.tmp4, reason.tag);
			asm.jmp_rel_near(call_runtime_TRAP);
		}
		// divide by zero happens when RAX and RDX are clobbered
		var divzero_label = trap_labels[TrapReason.DIV_BY_ZERO.tag];
		asm.bind(divzero_label);
		computePcFromCurIp();
		saveIVar(regs.STP);
		if (tuning.recordCurIpForTraps) saveIVar(regs.CURPC);
		// XXX: load runtime arg registers directly
		asm.movq_r_m(regs.tmp1, frame.WASM_FUNC);
		asm.movd_r_i(regs.tmp4, TrapReason.DIV_BY_ZERO.tag);
		callRuntime(refRuntimeCall(this.i.runtime_TRAP), [regs.tmp1, regs.CURPC, regs.tmp4], true);
		asm.jmp_rel_near(abruptRetLabel);

		// stack overflow cannot call into runtime, because it might be out of stack (!)
		var stackoverflow_label = trap_labels[TrapReason.STACK_OVERFLOW.tag];
		asm.bind(stackoverflow_label);
		var addr = Pointer.atObject(Execute.trapObjects[TrapReason.STACK_OVERFLOW.tag]) - Pointer.NULL;
		asm.movd_r_i(Target.V3_RET_GPRS[0], int.view(u32.!(addr)));
		asm.jmp_rel_near(abruptRetLabel);
		ic.stackOverflowHandlerOffset = stackoverflow_label.pos;
		ic.oobMemoryHandlerOffset = trap_labels[TrapReason.MEM_OUT_OF_BOUNDS.tag].pos;
		ic.divZeroHandlerOffset = trap_labels[TrapReason.DIV_BY_ZERO.tag].pos;

	}
	def refRuntimeCall<P, R>(f: P -> R) -> Pointer {
		var ptr = CiRuntime.unpackClosure<X86_64Interpreter, P, R>(f).0;
		var abs = ptr - Pointer.NULL;
		if (abs > u32.max) fatal("runtime call address not in 4GB");
		return ptr;
	}
	def reportOom(w: DataWriter, nlength: int) -> DataWriter {
		fatal("ran out of buffer space");
		return w;
	}
}

// Assembler patching support for out-of-line LEBs and other code refs.
def ABS_MARKER = 0x55443322;
def REL_MARKER = 0x44332211;
class OutOfLineLEB(dest: X86_64Gpr) extends X86_64Addr {
	var retOffset: int; // where OOB code should "return"
	var pos: int = -1;
	var delta: int;

	new() super(null, null, 1, REL_MARKER) { }
}
class IcCodeRef(var offset: int) extends X86_64Addr {
	new() super(null, null, 1, REL_MARKER) { }
}
class Patcher(w: DataWriter) extends X86_64AddrPatcher {
	new() super(ABS_MARKER, REL_MARKER) { }
	def recordRel32(pos: int, delta: int, addr: X86_64Addr) {
		match (addr) {
			x: OutOfLineLEB => {
				x.pos = pos;
				x.delta = delta;
			}
			x: IcCodeRef => {
				if (x.offset < 0) System.error("InterpreterGen", "unbound forward code ref");
				w.at(pos).put_b32(x.offset - (pos + delta));
				w.atEnd();
			}
		}
	}
}
// A utility that generates constants and picks appropriate instructions for rounding, data movement,
// and conversion in dealing with floating point truncations.
class FloatTrunc(isI64: bool, isF64: bool, isSigned: bool) {
	def round_s_s = if(isF64, X86_64Assembler.roundsd_s_s, X86_64Assembler.roundss_s_s);
	def sub_s_s = if(isF64, X86_64Assembler.subsd_s_s, X86_64Assembler.subss_s_s);
	def ucomi_s_s = if(isF64, X86_64Assembler.ucomisd_s_s, X86_64Assembler.ucomiss_s_s);
	def mov_s_r = if(isF64, X86_64Assembler.movq_s_r, X86_64Assembler.movd_s_r);
	def mov_s_m = if(isF64, X86_64Assembler.movsd_s_m, X86_64Assembler.movss_s_m);
	def mov_m_s = if(isF64, X86_64Assembler.movsd_m_s, X86_64Assembler.movss_m_s);
	def mov_m_r = if(isI64, X86_64Assembler.movq_m_r, X86_64Assembler.movd_m_r);
	def maxv: u64 = if(isI64,
				if(isSigned,
					if(isF64, Floats.d_1p63, Floats.f_1p63),
					if(isF64, Floats.d_1p64, Floats.f_1p64)),
				if(isSigned,
					if(isF64, Floats.d_1p31, Floats.f_1p31),
					if(isF64, Floats.d_1p32, Floats.f_1p32)));
	def minv: u64 = if(isI64, // XXX: share these constants with V3 interpreter
				if(isSigned,
					if(isF64, u64.view(-9.223372036854778E18d), u32.view(-9.223373e18f)),
					if(isF64, u64.view(-1d), u32.view(-1f))),
				if(isSigned,
					if(isF64, u64.view(-2147483649d), u32.view(-2.1474839E9f)),
					if(isF64, u64.view(-1d), u32.view(-1f))));

	def minus1: u64 = if(isF64, Floats.d_minus1, Floats.f_minus1);

	def ceilv: u64 = if(isI64,
				if(isSigned, u63.max, u64.max),
				if(isSigned, u31.max, u32.max));
	def floorv: u64 = if(isSigned,
				if(isI64, u64.view(i63.min), u64.view(i31.min)));

	def tag = if(isI64, BpTypeCode.I64, BpTypeCode.I32).code;

	def mov_s_i(asm: X86_64Assembler, s: X86_64Xmmr, v: u64, scratch: X86_64Gpr) {
		if (isF64) {
			if ((v & u32.max) == 0) {
				asm.movd_r_i(scratch, int.view(v >> 32));
				asm.q.shl_r_i(scratch, 32);
				asm.movq_s_r(s, scratch);
			} else if (int.view(v) > 0) { // no sign extension
				// XXX: load float constants from memory
				asm.movd_r_i(scratch, int.view(v >> 32));
				asm.q.shl_r_i(scratch, 32);
				asm.q.or_r_i(scratch, int.view(v));
				asm.movq_s_r(s, scratch);
			} else {
				System.error("FloatTrunc", "tricky 64-bit constant unimplemented");
			}
		} else {
			asm.movd_r_i(scratch, int.view(v));
			asm.movd_s_r(s, scratch);
		}
	}
	def mov_r_i(asm: X86_64Assembler, r: X86_64Gpr, v: u64) {
		if (isI64) {
			if (i32.view(v) == i64.view(v)) {
				asm.movq_r_i(r, int.view(v));
			} else if (u32.view(v) == u64.view(v)) {
				asm.movd_r_i(r, int.view(v));
			} else {
				System.error("FloatTrunc", "tricky 64-bit constant unimplemented");
			}
		} else {
			asm.movd_r_i(r, int.view(v));
		}
	}
	def cvt2si_r_s = if(isF64, X86_64Assembler.cvtsd2si_r_s, X86_64Assembler.cvtss2si_r_s);
}
def TRUNC_i32_f32_s = FloatTrunc.new(false, false, true);
def TRUNC_i32_f32_u = FloatTrunc.new(false, false, false);
def TRUNC_i32_f64_s = FloatTrunc.new(false, true, true);
def TRUNC_i32_f64_u = FloatTrunc.new(false, true, false);
def TRUNC_i64_f32_s = FloatTrunc.new(true, false, true);
def TRUNC_i64_f32_u = FloatTrunc.new(true, false, false);
def TRUNC_i64_f64_s = FloatTrunc.new(true, true, true);
def TRUNC_i64_f64_u = FloatTrunc.new(true, true, false);
