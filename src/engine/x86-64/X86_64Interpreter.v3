// Copyright 2021 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

// Dynamically-generated interpreter code.
var interpreterCode: InterpreterCode;
// Implements a Wasm interpreter by running handwritten x86-64 interpreter loop.
class X86_64Interpreter extends Interpreter {
	def valueStack = ValueStack.new();
	var asmEntry: (/*wf: */ WasmFunction, /*sp: */ Pointer) -> (ExecState, TrapReason);

	new() {
		if (interpreterCode == null) {
			var start = System.ticksUs();
			interpreterCode = X86_64InterpreterGen.new(this).gen();
			RiRuntime.userSignalHandler = handleSignal;
			var diff = System.ticksUs() - start;
			if (trace != null) trace.put1("Generated interpreter in %d \xCE\xBCs.\n", diff).outln();
		}
		asmEntry = CiRuntime.forgeClosure<
			X86_64Interpreter,				// closure type
			(/*wf: */ WasmFunction, /*sp: */ Pointer),	// parameter types
			(ExecState, TrapReason)>(			// return types
				interpreterCode.mapping.range.start + interpreterCode.v3EntryOffset, this);
	}

	def run(count: int, f: Function, args: Array<Value>) -> Result {
		// Unpack arguments into value stack format.
		var fp = valueStack.sp;
		if (args != null) for (v in args) valueStack.push(v);

		// Call the main loop with handles tail calls.
		var state = runWithTailCalls(f);

		// Unpack state into interpreter result.
		match (state.0) {
			STOPPED,
			RUNNING => return Result.Trap(TrapReason.UNIMPLEMENTED);
			TRAPPED => return Result.Trap(state.1);
			BREAKPOINT => return Result.Break;
			FINISHED => return popResult(f.sig);
			TIMEOUT => return Result.Break;
		}
	}
	private def runWithTailCalls(f: Function) -> (ExecState, TrapReason) {
		var state = (ExecState.FINISHED, TrapReason.NONE);
		while (true) { // handle repeated tail calls
			var result: HostResult;
			match (f) {
				wf: WasmFunction => {
					var sp = valueStack.sp;
					state = invoke(wf, sp);
					if (state.0 == ExecState.FINISHED) {
						valueStack.sp = sp + ((wf.sig.results.length - wf.sig.params.length) * SLOT_SIZE);
					}
					break;
				}
				hf: HostFunction0 => {
					if (trace != null) traceCallHostFunction(hf);
					result = hf.invoke0();
				}
				hf: HostFunction1 => {
					if (trace != null) traceCallHostFunction(hf);
					var a0 = valueStack.pop(hf.sig.params[0]);
					result = hf.invoke1(a0);
				}
				hf: HostFunction2 => {
					if (trace != null) traceCallHostFunction(hf);
					var a1 = valueStack.pop(hf.sig.params[1]);
					var a0 = valueStack.pop(hf.sig.params[0]);
					result = hf.invoke2(a0, a1);
				}
				hf: HostFunction3 => {
					if (trace != null) traceCallHostFunction(hf);
					var a2 = valueStack.pop(hf.sig.params[2]);
					var a1 = valueStack.pop(hf.sig.params[1]);
					var a0 = valueStack.pop(hf.sig.params[0]);
					result = hf.invoke3(a0, a1, a2);
				}
				hf: HostFunctionN => {
					if (trace != null) traceCallHostFunction(hf);
					var aN = valueStack.popN(hf.sig.params);
					result = hf.invokeN(aN);
				}
			}
			match (result) {
				Trap(reason) => {
					state = (ExecState.TRAPPED, reason);
					break;
				}
				Error(msg) => {
					state = (ExecState.TRAPPED, TrapReason.ERROR);
					error_msg = msg;
					break;
				}
				Value0 => {
					break;
				}
				Value1(val) => {
					valueStack.push(val);
					break;
				}
				ValueN(vals) => {
					for (a in vals) valueStack.push(a);
					break;
				}
				TailCall(target, args) => {
					for (a in args) valueStack.push(a);
					f = target;
					continue; // continue with next tail call
				}
			}
		}
		return state;
	}
	private def invoke(wf: WasmFunction, sp: Pointer) -> (ExecState, TrapReason) {
		if (trace != null) {
			trace.put2("X86_64Interpreter.asmEntry(wf=0x%x, sp=0x%x)",
				Pointer.atObject(wf) - Pointer.NULL, sp - Pointer.NULL);
			trace.outln();
		}
		return asmEntry(wf, sp);
	}
	private def popResult(sig: SigDecl) -> Result {
		var rt = sig.results;
		var r = Array<Value>.new(rt.length);
		for (i = r.length - 1; i >= 0; i--) r[i] = valueStack.pop(rt[i]);
		return Result.Value(r);
	}
	// callback to call into host functions
	def callHost(f: Function, vspp: Pointer) -> (ExecState, TrapReason) {
		valueStack.sp = vspp.load<Pointer>();
		var state = runWithTailCalls(f);
		vspp.store<Pointer>(valueStack.sp);
		return state;
	}
	def callback_MEMORY_GROW(instance: Instance, index: u32, vspp: Pointer) {
		var memory = instance.memories[index];
		valueStack.sp = vspp.load<Pointer>(); // XXX: set valueStack.sp in asm caller code
		var pages = Values.v_u(valueStack.pop(ValueType.I32));
		var result = memory.grow(pages);
		valueStack.push(Values.i_v(result));
		vspp.store<Pointer>(valueStack.sp);
	}
	def callback_MEMORY_INIT(instance: Instance, dindex: u32, mindex: u32, vspp: Pointer) -> (ExecState, TrapReason) {
		var memory = instance.memories[mindex];
		var data = if(!instance.dropped_data[dindex], instance.module.data[int.!(dindex)]);
		valueStack.sp = vspp.load<Pointer>();
		var size = valueStack.popu();
		var src_offset = valueStack.popu();
		var dst_offset = valueStack.popu();
		vspp.store<Pointer>(valueStack.sp);
		var t = memory.copyD(dst_offset, data, src_offset, size);
		if (t != TrapReason.NONE) return (ExecState.TRAPPED, t);
		return (ExecState.FINISHED, TrapReason.NONE);
	}
	def callback_MEMORY_COPY(instance: Instance, mindex1: u32, mindex2: u32, vspp: Pointer) -> (ExecState, TrapReason) {
		var dst = instance.memories[mindex1];
		var src = instance.memories[mindex2];
		valueStack.sp = vspp.load<Pointer>();
		var size = valueStack.popu();
		var src_offset = valueStack.popu();
		var dst_offset = valueStack.popu();
		vspp.store<Pointer>(valueStack.sp);
		var t = dst.copyM(dst_offset, src, src_offset, size);
		if (t != TrapReason.NONE) return (ExecState.TRAPPED, t);
		return (ExecState.FINISHED, TrapReason.NONE);
	}
	def callback_MEMORY_FILL(instance: Instance, mindex: u32, vspp: Pointer) -> (ExecState, TrapReason) {
		var memory = instance.memories[mindex];
		valueStack.sp = vspp.load<Pointer>();
		var size = valueStack.popu();
		var val = valueStack.popu();
		var dest = valueStack.popu();
		vspp.store<Pointer>(valueStack.sp);
		var t = memory.fill(dest, u8.view(val), size);
		if (t != TrapReason.NONE) return (ExecState.TRAPPED, t);
		return (ExecState.FINISHED, TrapReason.NONE);
	}
	def callback_GLOBAL_GET(instance: Instance, index: u32, vspp: Pointer) {
		var val = instance.globals[index].value;
		if (trace != null) trace.put2("GLOBAL_GET[%d] = %q", index, val.render).outln();
		valueStack.sp = vspp.load<Pointer>();
		valueStack.push(val);
		vspp.store<Pointer>(valueStack.sp);
	}
	def callback_GLOBAL_SET(instance: Instance, index: u32, vspp: Pointer) {
		valueStack.sp = vspp.load<Pointer>();
		var g = instance.globals[index];
		var val = valueStack.pop(g.valtype);
		g.value = val;
		vspp.store<Pointer>(valueStack.sp);
	}
	def callback_TABLE_GET(instance: Instance, index: u32, vspp: Pointer) -> (ExecState, TrapReason) {
		valueStack.sp = vspp.load<Pointer>();
		var table = instance.tables[index];
		var elem = Values.v_u(valueStack.pop(ValueType.I32));
		if (elem >= table.elems.length) return (ExecState.TRAPPED, TrapReason.TABLE_INDEX_OUT_OF_BOUNDS);
		var val = table.elems[elem];
		valueStack.push(val);
		vspp.store<Pointer>(valueStack.sp);
		return (ExecState.FINISHED, TrapReason.NONE);
	}
	def callback_TABLE_SET(instance: Instance, index: u32, vspp: Pointer) -> (ExecState, TrapReason) {
		valueStack.sp = vspp.load<Pointer>();
		var table = instance.tables[index];
		var val = valueStack.pop(table.elemtype);
		var elem = Values.v_u(valueStack.pop(ValueType.I32));
		if (elem >= table.elems.length) return (ExecState.TRAPPED, TrapReason.TABLE_INDEX_OUT_OF_BOUNDS);
		table[int.view(elem)] = val;
		vspp.store<Pointer>(valueStack.sp);
		return (ExecState.FINISHED, TrapReason.NONE);
	}
	def callback_TABLE_INIT(instance: Instance, eindex: u32, tindex: u32, vspp: Pointer) -> (ExecState, TrapReason) {
		valueStack.sp = vspp.load<Pointer>();
		var elem = if (!instance.dropped_elems[eindex], instance.module.elems[int.!(eindex)]);
		var table = instance.tables[tindex];
		var size = valueStack.popu();
		var src_offset = valueStack.popu();
		var dst_offset = valueStack.popu();
		var t = table.copyE(instance, dst_offset, elem, src_offset, size);
		vspp.store<Pointer>(valueStack.sp);
		if (t != TrapReason.NONE) return (ExecState.TRAPPED, t);
		return (ExecState.FINISHED, TrapReason.NONE);
	}
	def callback_TABLE_COPY(instance: Instance, t1: u32, t2: u32, vspp: Pointer) -> (ExecState, TrapReason) {
		valueStack.sp = vspp.load<Pointer>();
		var dst = instance.tables[t1];
		var src = instance.tables[t2];
		var size = valueStack.popu(), src_offset = valueStack.popu(), dst_offset = valueStack.popu();
		var t = dst.copyT(dst_offset, src, src_offset, size);
		vspp.store<Pointer>(valueStack.sp);
		if (t != TrapReason.NONE) return (ExecState.TRAPPED, t);
		return (ExecState.FINISHED, TrapReason.NONE);
	}
	def callback_TABLE_GROW(instance: Instance, tindex: u32, vspp: Pointer) {
		valueStack.sp = vspp.load<Pointer>();
		var table = instance.tables[tindex];
		var size = valueStack.popu();
		var val = valueStack.pop(table.elemtype);
		var r = table.grow(size, val);
		valueStack.push(Values.i_v(r));
		vspp.store<Pointer>(valueStack.sp);
	}
	def callback_TABLE_FILL(instance: Instance, tindex: u32, vspp: Pointer) -> (ExecState, TrapReason) {
		valueStack.sp = vspp.load<Pointer>();
		var table = instance.tables[tindex];
		var size = valueStack.popu();
		var val = valueStack.pop(table.elemtype);
		var dest = valueStack.popu();
		var t = table.fill(dest, val, size);
		vspp.store<Pointer>(valueStack.sp);
		if (t != TrapReason.NONE) return (ExecState.TRAPPED, t);
		return (ExecState.FINISHED, TrapReason.NONE);
	}
}

// Size constants
def SLOT_SIZE = 2 * Pointer.SIZE;
def PTR_SIZE = Pointer.SIZE;
def SLOT_SIZE_LOG = u6.view(4);
def DEFAULT_STACK_SIZE = 128u * 1024u;

// Signal-handling for traps
def ucontext_rip_offset = 168;
def ucontext_rsp_offset = 160;
def SIGFPE  = 8;
def SIGBUS  = 10;
def SIGSEGV = 11;
def handleSignal(signum: int, siginfo: Pointer, ucontext: Pointer) -> bool {
	var pip = ucontext + ucontext_rip_offset;
	var ip = pip.load<Pointer>();
	var r = interpreterCode.mapping.range;
	if (!r.contains(ip)) return false;
	if (Trace.interpreter) {
		TraceBuilder.new().put2("  !signal %d in interpreter @ 0x%x", signum, ip - Pointer.NULL).outln();
	}
	match (signum) {
		SIGFPE => {
			// presume divide/modulus by zero
			pip.store<Pointer>(r.start + interpreterCode.divZeroHandlerOffset);
			return true;
		}
		SIGBUS, SIGSEGV => {
			var addr = RiOs.getAccessAddress(siginfo, ucontext);
			if (RedZones.isInRedZone(addr)) {
				pip.store<Pointer>(r.start + interpreterCode.stackOverflowHandlerOffset);
				return true;
			}
			pip.store<Pointer>(r.start + interpreterCode.oobMemoryHandlerOffset);
			return true;
		}
	}
	return true;
}

// Implements a value stack using raw (Pointer) memory, with explicitly tagged values.
// Maps a value stack red zone at the end to catch and report stack overflow.
// TODO: need custom scan extension for V3 garbage collector for reference values.
class ValueStack {
	def mapping = Mmap.reserve(DEFAULT_STACK_SIZE, Mmap.PROT_READ | Mmap.PROT_WRITE);
	var sp: Pointer;

	new() {
		if (mapping == null) fatal("out of memory allocating value stack");
		sp = mapping.range.start;
		def PAGE_SIZE = 4096u;
		var ok = RedZones.addRedZone(mapping, DEFAULT_STACK_SIZE - PAGE_SIZE, PAGE_SIZE);
		if (!ok) fatal("could not protect value stack red zone");
	}
	def push(v: Value) {
		match (v) {
			Ref(obj) => pushPair(BpTypecon.RefNullT.code, obj);
			I31(val) => pushPair(BpTypecon.I31REF.code, u64.view(val));
			I32(val) => pushPair(BpTypecon.I32.code, u64.view(val));
			I64(val) => pushPair(BpTypecon.I64.code, u64.view(val));
			F32(bits) => pushPair(BpTypecon.F32.code, u64.view(bits));
			F64(bits) => pushPair(BpTypecon.F64.code, u64.view(bits));
		}
	}
	def popN(t: Array<ValueType>) -> Array<Value> {
		var r = Array<Value>.new(t.length);
		for (i < t.length) {
			var j = r.length - i - 1;
			r[j] = pop(t[j]);
		}
		return r;
	}
	def pop(t: ValueType) -> Value {
		match (t) {
			I32 => return Value.I32(popb32(BpTypecon.I32.code));
			I64 => return Value.I64(popb64(BpTypecon.I64.code));
			F32 => return Value.F32(popb32(BpTypecon.F32.code));
			F64 => return Value.F64(popb64(BpTypecon.F64.code));
			RefStruct,
			RefArray,
			Host,
			EXTERNREF,
			EXTERNREF_NULL,
			ANYREF => return Value.Ref(popObject());
			I31REF => {
				if (peekTag() == BpTypecon.I31REF.code) return Value.I31(u31.view(popb32(BpTypecon.I31REF.code)));
				else return Value.Ref(popObject());
			}
			RefFunc,
			FUNCREF => return Value.Ref(popFunction());
			Rtt => return Value.Ref(popRtt());
			_ => fatal(Strings.format1("unexpected type: %s", t.name));
		}
		return Value.Ref(null);
	}
	def popu() -> u32 {
		return popb32(BpTypecon.I32.code);
	}
	def popb32(tag: byte) -> u32 {
		checkTag(tag);
		sp += -(SLOT_SIZE);
		return (sp + Pointer.SIZE).load<u32>();
	}
	def popb64(tag: byte) -> u64 {
		checkTag(tag);
		sp += -(SLOT_SIZE);
		return (sp + Pointer.SIZE).load<u64>();
	}
	def popObject() -> Object {
		var got = peekTag();
		match (got) {
			BpTypecon.ANYREF.code,
			BpTypecon.FUNCREF.code,
			BpTypecon.EXTERNREF.code,
			BpTypecon.RefNullT.code,
			BpTypecon.RefT.code,
			BpTypecon.I31REF.code => ;
			_ => fatal(Strings.format1("value stack tag mismatch, expected ref, got %x", got));
		}
		sp += -(SLOT_SIZE);
		return (sp + Pointer.SIZE).load<Object>();
	}
	def popFunction() -> Function {
		var obj = popObject();
		return Function.!(obj);
	}
	def popRtt() -> RttObject {
		var obj = popObject();
		return RttObject.!(obj);
	}
	def checkTag(tag: byte) -> byte {
		var got = peekTag();
		if (got == tag) return tag;
		fatal(Strings.format2("value stack tag mismatch, expected: %x, got %x", tag, got));
		return tag;
	}
	def peekTag() -> byte {
		return (sp + -(SLOT_SIZE)).load<u8>() & '\x7F';
	}
	def pushPair<T>(tag: byte, bits: T) {
		sp.store<u8>(tag);
		(sp + Pointer.SIZE).store(bits);
		sp += SLOT_SIZE;
	}
}

def fatal(msg: string) {
	System.error("X86_64Interpreter", msg);
}

//------------------------------------------------------------------------------------------------
//-- Begin Interpreter Generator
//------------------------------------------------------------------------------------------------

// The result of generating an interpreter is machine code with some known offsets into it.
class InterpreterCode(mapping: Mapping) {
	var v3EntryOffset: int;			// entry from V3 calling code
	var oobMemoryHandlerOffset: int;	// handler for signals caused by OOB memory access
	var divZeroHandlerOffset: int;		// handler for signals caused by divide by zero
	var stackOverflowHandlerOffset: int;	// handler for signals caused by (value- or call-) stack overflow
}

// Internal register configuration for variables live in the interpreter execution context.
def R: X86_64Regs, G = X86_64Regs.GPRs, C: X86_64Conds;
enum IVar(entryParam: int, gpr: X86_64Gpr, frameOffset: int, baseline: bool, mutable: bool) {
	INTERPRETER	(0, 	null,	0,	false,	false),	// Interpreter object
	WASM_FUNC	(1, 	null,	0,	false,	false),	// WasmFunction object
	MEM0_BASE	(-1,	R.R10,	8,	true,	false),	// base of memory #0
	VFP		(-1,	R.R11,	16,	false,	false),	// value stack frame pointer
	VSP		(2,	R.RSI,	24,	false,	true),	// value stack stack pointer
	XIP		(-1,	R.RBX,	32,	false,	true),	// extended instruction pointer
	IP		(-1,	R.RAX,	40,	false,	true),	// current instruction pointer
	EIP		(-1,	R.R13,	48,	false,	false),	// end instruction pointer
	FUNC_DECL	(-1,	R.R12,	56,	true,	false),	// FuncDecl
	INSTANCE	(-1,	R.RDI,	64,	true,	false)	// Instance
}
// Shorthand for codegen.
component I {
	def V3_PARAM_GPRS = [R.RDI, R.RSI, R.RDX, R.RCX, R.R8, R.R9]; 		// System-V
	def V3_RET_GPRS = [R.RAX, R.RDX, R.RCX, R.RSI]; 			// System-V + 2

	def INTERPRETER_GPRS = filterGprs(isTrue);	// allocatable interpreter registers
	def BASELINE_GPRS = filterGprs(IVar.baseline);	// allocatable baseline registers

	def filterGprs(cond: IVar -> bool) -> Array<X86_64Gpr> {
		var gprs = G;
		var used = Array<bool>.new(gprs.length);
		used[R.RSP.regnum] = true;
		for (v in IVar) {
			if (v.gpr != null && cond(v)) used[v.gpr.regnum] = true;
		}
		var v = Vector<X86_64Gpr>.new().grow(gprs.length);
		for (i < gprs.length) {
			if (!used[i]) v.put(gprs[i]);
		}
		return v.extract();
	}
	def isTrue(v: IVar) -> bool {
		return true;
	}

	// Shorthand for individual registers and addresses.
	def ip = R.RAX;
	def eip = R.R13;
	def ip_ptr = ip.indirect();
	def vsp = R.RSI;
	def vsp_ptr = vsp.indirect();
	def vsp_ptr_p1 = vsp.plus(PTR_SIZE);
	def vsp_ptr_m1 = vsp.plus(-PTR_SIZE);
	def vsp_ptr_m1u = vsp.plus(-PTR_SIZE + 4);
	def vsp_ptr_m2 = vsp.plus(-SLOT_SIZE);
	def vsp_ptr_m3 = vsp.plus(-PTR_SIZE - SLOT_SIZE);
	def vsp_ptr_m3u = vsp.plus(-PTR_SIZE - SLOT_SIZE + 4);
	def vsp_ptr_m4 = vsp.plus(-2 * SLOT_SIZE);
	def vsp_ptr_m5 = vsp.plus(-PTR_SIZE - (2 * SLOT_SIZE));
	def r0 = I.INTERPRETER_GPRS[0];
	def r1 = I.INTERPRETER_GPRS[1];
	def r2 = I.INTERPRETER_GPRS[2];
	def r3 = I.INTERPRETER_GPRS[3];
	def xmm0 = X86_64Regs.XMM0;
	def xmm1 = X86_64Regs.XMM1;
	def scratch = R.RBP;
}

// Options that only impact interpreter performance.
class X86_64InterpreterTuning {
	var threadedDispatch = true;
	var handlerAlignment = 1;
	var shortDispatchEntries = true;
	var twoByteEntries = false;
}

// Space needed for the machine code of the interpreter
def INL_SIZE = 32 * 1024;
def OOL_SIZE = 4 * 1024;
def TOTAL_SIZE = INL_SIZE + OOL_SIZE;
// Encapsulates temporary state needed for generating the {InterpreterCode} for X86-64.
class X86_64InterpreterGen(i: X86_64Interpreter) {
	def buf = Array<byte>.new(TOTAL_SIZE);
	def w = DataWriter.new().reset(buf, 0, 0);
	def w_ool = DataWriter.new().reset(buf, INL_SIZE, INL_SIZE);
	def asm = X86_64Assemblers.create64(w);
	def ool = X86_64Assemblers.create64(w_ool);
	
	def frameSize = IVar.INSTANCE.frameOffset + PTR_SIZE;
	def offsets = V3Offsets.new();
	def tuning = X86_64InterpreterTuning.new();

	var ic: InterpreterCode;
	var oolULeb32Sites = Vector<OutOfLineLEB>.new();
	var handler_RETURN: int;
	var handler_END: int;
	var handler_BLOCK: int;
	var handler_UNIMPLEMENTED: int;
	var handler_INVALID: int;
	var firstDispatchOffset: int;
	var dispatchJmpOffset: int = -1;
	var callEntryOffset: int;
	var callFunctionOffset: int;
	var codeEndOffset: int;
	var handlerEndOffset: int;
	var callReentryRef: IcCodeRef;
	var abruptRetRef: IcCodeRef;
	var callbackTableOffset: int;
	var callbackTableLimit: int;

	var dispatchTables = [
		(0x00, IcCodeRef.new(-1)),
		(0xFC, IcCodeRef.new(-1))
	];

	new() {
		w.refill = reportOom;
		w_ool.refill = reportOom;
		var p = Patcher.new(w);
		asm.patcher = asm.d.patcher = p;
		ool.patcher = ool.d.patcher = p;
	}

	def gen() -> InterpreterCode {
		// Reserve a memory mapping for the executable code
		var mapping = Mmap.reserve(u32.!(TOTAL_SIZE), Mmap.PROT_READ | Mmap.PROT_WRITE);
		if (mapping == null) return null;
		ic = InterpreterCode.new(mapping);
		// Reserve space for pointers to interpreter callbacks
		w.align(8);
		callbackTableOffset = w.pos;
		w.skipN(16 * 8);
		callbackTableLimit = w.pos;
		// Reserve space for each of the dispatch tables
		for (t in dispatchTables) {
			asm.w.align(2);
			t.1.offset = w.pos;
			asm.w.skipN(256 * 2);
		}
		// Begin code generation
		genInterpreterEntry();
		genOpcodeHandlers();
		handlerEndOffset = w.pos;
		// Generate out-of-line code
		genOutOfLineLEBs();
		codeEndOffset = w.atEnd().pos;
		genTrapHandlers();
		// Finished. Copy code from Datawriter into the memory mapping
		var p = mapping.range.start;
		w.atEnd();
		if (w.pos > TOTAL_SIZE) System.error("X86_64InterpreterGen", Strings.format2("need %d bytes for interpreter code, only allocated %d", w.pos, TOTAL_SIZE));
		for (i < w.pos) {
			p.store<u8>(w.data[i]);
			p++;
		}
		// Write-protect the executable code for security and debugging
		Mmap.protect(mapping.range.start, mapping.range.size(), Mmap.PROT_READ | Mmap.PROT_EXEC);
		// Trace results to help in debugging
		if (Trace.interpreter) {
			var buf = TraceBuilder.new();
			var start = ic.mapping.range.start - Pointer.NULL;
			buf.put3("Generated asm interpreter @ (0x%x ... 0x%x), %d bytes\n",
				start,
				(ic.mapping.range.end - Pointer.NULL),
				w.pos);
			buf.put1("\tv3 entry     = 0x%x\n", start + ic.v3EntryOffset);
			buf.put1("\tcall entry   = 0x%x\n", start + callEntryOffset);
			buf.put1("\tdispatch     = 0x%x\n", start + firstDispatchOffset);
			buf.put1("\thandlers end = 0x%x\n", start + handlerEndOffset);
			buf.put1("\tcode end     = 0x%x\n", start + codeEndOffset);
			buf.put1("\toob mem      = 0x%x\n", start + ic.oobMemoryHandlerOffset);
			buf.put1("\tdivzero      = 0x%x\n", start + ic.divZeroHandlerOffset);
			buf.put1("\tstack ovflw  = 0x%x\n", start + ic.stackOverflowHandlerOffset);
			buf.put1("break *0x%x\n", start + dispatchJmpOffset);
			buf.outln();
		}
		return ic;
	}

	def genInterpreterEntry() {
		var shared_entry = X86_64Label.new();
		var tmp = I.scratch;
		{ // Entrypoint for calls coming from V3
			ic.v3EntryOffset = w.atEnd().pos;

			// Allocate and initialize interpreter stack frame from incoming V3 params.
			asm.q.sub_r_i(R.RSP, frameSize);

			var iv: IVar;
			// Spill interpreter object
			iv = IVar.INTERPRETER;
			asm.movq_m_r(R.RSP.plus(iv.frameOffset), I.V3_PARAM_GPRS[iv.entryParam]);
			// Spill VSP (value stack pointer)
			iv = IVar.VSP;
			asm.movq_m_r(R.RSP.plus(iv.frameOffset), I.V3_PARAM_GPRS[iv.entryParam]);
			// move WasmFunction into tmp
			asm.movq_r_r(tmp, I.V3_PARAM_GPRS[IVar.WASM_FUNC.entryParam]);
			restoreIVar(IVar.VSP);
			asm.jmp_rel_near(shared_entry);
		}

		{ // Re-entry for calls within the interpreter itself
			callReentryRef = IcCodeRef.new(w.pos);
			// Load and respill interpreter onto new stack frame
			asm.movq_r_m(tmp, R.RSP.plus(IVar.INTERPRETER.frameOffset + PTR_SIZE)); // account for additional return address
			// Allocate actual stack frame
			asm.q.sub_r_i(R.RSP, frameSize);
			// Re-spill interpreter object
			asm.movq_m_r(R.RSP.plus(IVar.INTERPRETER.frameOffset), tmp);
			// Spill the (valid) stack pointer
			saveIVar(IVar.VSP);
			// WasmFunction is in r1 for interpreter reentry
			asm.movq_r_r(tmp, I.r1);
		}

		asm.bind(shared_entry);
		// Load wf.instance, wf.decl and spill
		asm.movq_r_m(IVar.INSTANCE.gpr, X86_64Addr.new(tmp, null, 1, offsets.WasmFunction_instance));
		saveIVar(IVar.INSTANCE);
		asm.movq_r_m(IVar.FUNC_DECL.gpr, X86_64Addr.new(tmp, null, 1, offsets.WasmFunction_decl));
		saveIVar(IVar.FUNC_DECL);

		// Compute VFP = VSP - func.sig.params.length * SLOT_SIZE
		asm.movq_r_m(tmp, X86_64Addr.new(IVar.FUNC_DECL.gpr, null, 1, offsets.FuncDecl_sig));
		asm.movq_r_m(tmp, X86_64Addr.new(tmp, null, 1, offsets.SigDecl_params));
		asm.movd_r_m(tmp, X86_64Addr.new(tmp, null, 1, offsets.Array_length));
		asm.q.shl_r_i(tmp, SLOT_SIZE_LOG);
		asm.movq_r_r(IVar.VFP.gpr, IVar.VSP.gpr);
		asm.q.sub_r_r(IVar.VFP.gpr, tmp);
		saveIVar(IVar.VFP);

		// Load &func.code.code[0] into IP
		var codeReg = I.r0;
		asm.movq_r_m(codeReg, X86_64Addr.new(IVar.FUNC_DECL.gpr, null, 1, offsets.FuncDecl_code));
		asm.movq_r_m(tmp, X86_64Addr.new(codeReg, null, 1, offsets.Code_code));
		asm.lea(IVar.IP.gpr, X86_64Addr.new(tmp, null, 1, offsets.Array_contents));
		saveIVar(IVar.IP);
		// Load IP + code.length into EIP
		asm.movd_r_m(IVar.EIP.gpr, X86_64Addr.new(tmp, null, 1, offsets.Array_length));
		asm.q.add_r_r(IVar.EIP.gpr, IVar.IP.gpr);
		saveIVar(IVar.EIP);
		// Load &func.code.xcode[0] into XIP
		asm.movq_r_m(tmp, X86_64Addr.new(codeReg, null, 1, offsets.Code_xcode));
		asm.lea(IVar.XIP.gpr, X86_64Addr.new(tmp, null, 1, offsets.Array_contents));
		saveIVar(IVar.XIP);

		// Load instance.memories[0].start into MEM0_BASE
		var mem0 = IVar.MEM0_BASE.gpr;
		asm.movq_r_m(mem0, X86_64Addr.new(IVar.INSTANCE.gpr, null, 1, offsets.Instance_memories));
		var no_mem = X86_64Label.new();
		asm.movd_r_m(I.r0, X86_64Addr.new(mem0, null, 1, offsets.Array_length)); // XXX: always have a memories[0].start to avoid a branch?
		asm.d.cmp_r_i(I.r0, 0);
		asm.jc_rel_near(C.Z, no_mem);
		asm.movq_r_m(mem0, X86_64Addr.new(mem0, null, 1, offsets.Array_contents));
		asm.movq_r_m(mem0, X86_64Addr.new(mem0, null, 1, offsets.X86_64Memory_start));
		asm.bind(no_mem);
		saveIVar(IVar.MEM0_BASE);

		callEntryOffset = w.pos;
		// Decode locals and initialize them. (XXX: special-case 0 locals)
		var countGpr = I.r0;
		genReadUleb32(countGpr);
		var start = X86_64Label.new(), done = X86_64Label.new();
		// gen: if (count != 0) do
		asm.d.cmp_r_i(countGpr, 0);
		asm.jc_rel_near(C.Z, done);
		asm.bind(start);
		// gen: var num = read_uleb32()
		var numGpr = I.r1;
		genReadUleb32(numGpr);
		// gen: var type = read_type();
		var type_done = X86_64Label.new();
		var typeGpr = I.r2;
		asm.d.movbzx_r_m(typeGpr, I.ip_ptr);	// load first byte
		asm.q.inc_r(I.ip);			// increment pointer
		asm.d.test_r_i(typeGpr, 0x80);		// test most-significant bit
		asm.jc_rel_near(C.Z, type_done);	// more type LEB bytes?
		genSkipLeb();				// only first byte matters
		asm.bind(type_done);

		// TODO: for RefNullT, AbstractT, skip additional ULEB

		// gen: if(num != 0) do
		var start2 = X86_64Label.new(), done2 = X86_64Label.new();
		asm.d.cmp_r_i(numGpr, 0);
		asm.jc_rel_near(C.Z, done2);
		asm.bind(start2);
		asm.movq_m_r(I.vsp_ptr, typeGpr);	// *(sp) = type
		asm.movq_m_i(I.vsp_ptr_p1, 0);		// *(sp + 8) = 0
		asm.add_r_i(I.vsp, SLOT_SIZE);		// sp += 16
		// gen: while (--num != 0)
		asm.d.dec_r(numGpr);
		asm.jc_rel_near(C.NZ, start2);

		// gen: while (--count != 0)
		asm.d.dec_r(countGpr);
		asm.jc_rel_near(C.NZ, start);
		asm.bind(done);

		// execute first instruction
		firstDispatchOffset = w.atEnd().pos;
		genDispatch(dispatchTables[0]);
	}

	// Generate all the opcode handlers.
	def genOpcodeHandlers() {
		// Generate the default handler and initialize dispatch tables
		var pos = w.atEnd().pos;
		genAbruptReturn(ExecState.TRAPPED, TrapReason.INVALID_OPCODE);
		for (t in dispatchTables) {
			var ref = t.1;
			var offset = pos - ref.offset;
			for (i < 256) w.at(ref.offset + 2 * i).put_b16(offset);
		}

		// Generate the secondary dispatch tables and point main table at them
		var ref0 = dispatchTables[0].1;
		for (t in dispatchTables) {
			if (t.0 == 0) continue; // main dispatch table
			var pos = w.atEnd().pos;
			genDispatch(t);
			var offset = pos - ref0.offset;
			w.at(ref0.offset + 2 * t.0).put_b16(offset);
		}

		// Generate handlers for all opcodes
		for (opcode in Opcode) {  // XXX: order opcodes by frequency
			w.atEnd();
			if (tuning.handlerAlignment > 1) w.align(tuning.handlerAlignment);
			var pos = w.pos;
			// try to generate the handler for the opcode
			var result = genOpcodeHandler(opcode);
			match (result) {
				UNHANDLED => ; // already points to default handler
				HANDLED => {
					patchDispatchTable(opcode, pos);
					genDispatchOrJumpToDispatch();
				}
				Offset(off) => {
					patchDispatchTable(opcode, off);
				}
				END => {
					patchDispatchTable(opcode, pos);
				}
			}
		}
		genControlFlow();
		genLoadsAndStores();
		genFloatCmps();
		genFloatMinAndMax();
	}
	def genControlFlow() {
		// NOP: just goes directly back to the dispatch loop
		patchDispatchTable(Opcode.NOP, firstDispatchOffset);

		// UNREACHABLE: abrupt return
		bindOpcodeHandler(Opcode.UNREACHABLE);
		genAbruptReturn(ExecState.TRAPPED, TrapReason.UNREACHABLE);

		// BLOCK, LOOP, and TRY
		bindOpcodeHandler(Opcode.BLOCK);
		bindOpcodeHandler(Opcode.LOOP);
		bindOpcodeHandler(Opcode.TRY);
		genSkipLeb();
		genDispatchOrJumpToDispatch();

		var ctl_fallthru = X86_64Label.new();
		var ctl_xfer = X86_64Label.new();
		var ctl_xfer_nostack = X86_64Label.new();

		// IF: check condition and either fall thru to next bytecode or ctl xfer (without stack copying)
		bindOpcodeHandler(Opcode.IF);
		asm.sub_r_i(I.vsp, SLOT_SIZE);
		asm.d.cmp_m_i(I.vsp_ptr_p1, 0);
		asm.jc_rel_near(C.Z, ctl_xfer_nostack);
		asm.bind(ctl_fallthru);
		genSkipLeb();
		asm.add_r_i(IVar.XIP.gpr, offsets.XIP_entry_size);
		genDispatchOrJumpToDispatch();

		// BR_IF: check condition and either fall thru to next bytecode or ctl xfer (with stack copying)
		bindOpcodeHandler(Opcode.BR_IF);
		asm.sub_r_i(I.vsp, SLOT_SIZE);
		asm.d.cmp_m_i(I.vsp_ptr_p1, 0);
		asm.jc_rel_near(C.Z, ctl_fallthru);
		// fallthru to BR

		// BR: unconditional ctl xfer with stack copying
		bindOpcodeHandler(Opcode.BR);
		asm.bind(ctl_xfer);
		var popcount = I.r0;
		var valcount = I.r1;
		// if popcount > 0
		asm.movd_r_m(popcount, X86_64Addr.new(IVar.XIP.gpr, null, 1, offsets.XIP_popcount));
		asm.d.cmp_r_i(popcount, 0);
		asm.jc_rel_near(C.Z, ctl_xfer_nostack);
		// load valcount
		asm.movd_r_m(valcount, X86_64Addr.new(IVar.XIP.gpr, null, 1, offsets.XIP_valcount));
		// popcount = popcount * SLOT_SIZE
		asm.d.shl_r_i(popcount, SLOT_SIZE_LOG);
		// vsp -= valcount + popcount (XXX: save an instruction here?)
		asm.q.sub_r_r(I.vsp, popcount);
		asm.movd_r_r(I.scratch, valcount);
		asm.d.shl_r_i(I.scratch, SLOT_SIZE_LOG);
		asm.q.sub_r_r(I.vsp, I.scratch);
		// do { [vsp] = [vsp + popcount]; vsp++; valcount--; } while (valcount != 0)
		var loop = X86_64Label.new();
		asm.bind(loop);
		asm.movdqu_s_m(I.xmm0, X86_64Addr.new(I.vsp, popcount, 1, 0));
		asm.movdqu_m_s(X86_64Addr.new(I.vsp, null, 1, 0), I.xmm0);
		asm.q.add_r_i(I.vsp, SLOT_SIZE);
		asm.d.dec_r(valcount);
		asm.jc_rel_near(C.G, loop);

		// ELSE: unconditional ctl xfer without stack copying
		bindOpcodeHandler(Opcode.ELSE);
		asm.bind(ctl_xfer_nostack);
		asm.movwsx_r_m(I.r0, X86_64Addr.new(IVar.XIP.gpr, null, 1, offsets.XIP_pc_delta)); // TODO: 4 bytes
		asm.q.lea(IVar.IP.gpr, X86_64Addr.new(IVar.IP.gpr, I.r0, 1, -1)); // adjust ip
		asm.movwsx_r_m(I.r1, X86_64Addr.new(IVar.XIP.gpr, null, 1, offsets.XIP_xip_delta)); // TODO: 4 bytes
		asm.q.lea(IVar.XIP.gpr, X86_64Addr.new(IVar.XIP.gpr, I.r1, 4, 0)); // adjust xip XXX: preshift?
		genDispatchOrJumpToDispatch();

		// BR_TABLE: adjust XIP based on input value and then ctl xfer with stack copying
		bindOpcodeHandler(Opcode.BR_TABLE);
		var max = I.r0, key = I.r1;
		asm.movd_r_m(max, X86_64Addr.new(IVar.XIP.gpr, null, 1, offsets.XIP_pc_delta));
		asm.sub_r_i(I.vsp, SLOT_SIZE);
		asm.movd_r_m(key, I.vsp_ptr_p1);
		asm.d.cmp_r_r(key, max);
		var ok = X86_64Label.new();
		asm.jc_rel_near(C.NC, ok);
		asm.d.inc_r(key);
		asm.movd_r_r(max, key);
		asm.bind(ok);
		asm.q.add_r_r(IVar.IP.gpr, max);
		asm.shl_r_i(max, offsets.XIP_entry_size_log);
		asm.q.add_r_r(IVar.XIP.gpr, max);
		asm.jmp_rel_near(ctl_xfer);
	}
	def genLoadsAndStores() {
		genLoad(Opcode.I32_LOAD, BpTypecon.I32.code, asm.movd_r_m);
		genLoad(Opcode.I64_LOAD, BpTypecon.I64.code, asm.movq_r_m);
		genLoad(Opcode.F32_LOAD, BpTypecon.F32.code, asm.movd_r_m);
		genLoad(Opcode.F64_LOAD, BpTypecon.F64.code, asm.movq_r_m);
		genLoad(Opcode.I32_LOAD8_S, BpTypecon.I32.code, asm.movbsx_r_m);
		genLoad(Opcode.I32_LOAD8_U, BpTypecon.I32.code, asm.movbzx_r_m);
		genLoad(Opcode.I32_LOAD16_S, BpTypecon.I32.code, asm.movwsx_r_m);
		genLoad(Opcode.I32_LOAD16_U, BpTypecon.I32.code, asm.movwzx_r_m);
		genLoad(Opcode.I64_LOAD8_S, BpTypecon.I64.code, asm.movbsx_r_m);
		genLoad(Opcode.I64_LOAD8_U, BpTypecon.I64.code, asm.movbzx_r_m);
		genLoad(Opcode.I64_LOAD16_S, BpTypecon.I64.code, asm.movwsx_r_m);
		genLoad(Opcode.I64_LOAD16_U, BpTypecon.I64.code, asm.movwzx_r_m);
		bindOpcodeHandler(Opcode.I64_LOAD32_S); {
			asm.q.inc_r(I.ip); // skip flags byte
			genReadUleb32(I.r0);	// decode offset
			asm.movd_r_m(I.r1, I.vsp_ptr_m1);		// read index
			asm.q.add_r_r(I.r0, I.r1);			// add index + offset
			asm.movd_r_m(I.r1, X86_64Addr.new(IVar.MEM0_BASE.gpr, I.r0, 1, 0));
			asm.q.shl_r_i(I.r1, 32); // special sign-extension necessary
			asm.q.sar_r_i(I.r1, 32);
			genTagUpdate(BpTypecon.I64.code);
			asm.movq_m_r(I.vsp_ptr_m1, I.r1);
			genDispatchOrJumpToDispatch();
		}
		genLoad(Opcode.I64_LOAD32_U, BpTypecon.I64.code, asm.movd_r_m);

		bindOpcodeHandler(Opcode.I32_STORE);
		bindOpcodeHandler(Opcode.F32_STORE);
		bindOpcodeHandler(Opcode.I64_STORE32);
		genStore(asm.movd_m_r);

		bindOpcodeHandler(Opcode.I64_STORE);
		bindOpcodeHandler(Opcode.F64_STORE);
		genStore(asm.movq_m_r);

		bindOpcodeHandler(Opcode.I32_STORE8);
		bindOpcodeHandler(Opcode.I64_STORE8);
		genStore(asm.movb_m_r);

		bindOpcodeHandler(Opcode.I32_STORE16);
		bindOpcodeHandler(Opcode.I64_STORE16);
		genStore(asm.movw_m_r);
	}
	def genFloatCmps() {
		var ret_zero = X86_64Label.new(), ret_one = X86_64Label.new();
		for (t in [
			(Opcode.F32_EQ, C.NZ),
			(Opcode.F32_NE, C.Z),
			(Opcode.F32_LT, C.NC),
			(Opcode.F32_GT, C.NA),
			(Opcode.F32_LE, C.A),
			(Opcode.F32_GE, C.C)]) {
			bindOpcodeHandler(t.0);
			asm.movss_s_m(I.xmm0, I.vsp_ptr_m3);
			asm.ucomiss_s_m(I.xmm0, I.vsp_ptr_m1);
			asm.jc_rel_near(C.P, if(t.0 == Opcode.F32_NE, ret_one, ret_zero));
			asm.jc_rel_near(t.1, ret_zero);
			asm.jmp_rel_near(ret_one);
		}

		asm.bind(ret_zero);
		asm.q.sub_r_i(I.vsp, SLOT_SIZE);
		genTagUpdate(BpTypecon.I32.code);
		asm.movd_m_i(I.vsp_ptr_m1, 0);
		genDispatchOrJumpToDispatch();

		asm.bind(ret_one);
		asm.q.sub_r_i(I.vsp, SLOT_SIZE);
		genTagUpdate(BpTypecon.I32.code);
		asm.movd_m_i(I.vsp_ptr_m1, 1);
		genDispatchOrJumpToDispatch();

		// XXX: too far of a near jump to share these between f32 and f64
		ret_zero = X86_64Label.new();
		ret_one = X86_64Label.new();
		for (t in [
			(Opcode.F64_EQ, C.NZ),
			(Opcode.F64_NE, C.Z),
			(Opcode.F64_LT, C.NC),
			(Opcode.F64_GT, C.NA),
			(Opcode.F64_LE, C.A),
			(Opcode.F64_GE, C.C)]) {
			bindOpcodeHandler(t.0);
			asm.movsd_s_m(I.xmm0, I.vsp_ptr_m3);
			asm.ucomisd_s_m(I.xmm0, I.vsp_ptr_m1);
			asm.jc_rel_near(C.P, if(t.0 == Opcode.F64_NE, ret_one, ret_zero));
			asm.jc_rel_near(t.1, ret_zero);
			asm.jmp_rel_near(ret_one);
		}

		asm.bind(ret_zero);
		asm.q.sub_r_i(I.vsp, SLOT_SIZE);
		genTagUpdate(BpTypecon.I32.code);
		asm.movd_m_i(I.vsp_ptr_m1, 0);
		genDispatchOrJumpToDispatch();

		asm.bind(ret_one);
		asm.q.sub_r_i(I.vsp, SLOT_SIZE);
		genTagUpdate(BpTypecon.I32.code);
		asm.movd_m_i(I.vsp_ptr_m1, 1);
		genDispatchOrJumpToDispatch();
	}
	// Generate the code of a single opcode.
	def genOpcodeHandler(opcode: Opcode) -> HandlerGenResult {
		// XXX: does it matter to update TOS tags first or last?
		match (opcode) {
			END, RETURN => {
				if (handler_END != 0) {
					return HandlerGenResult.Offset(if(opcode == Opcode.END, handler_END, handler_RETURN));
				}
				handler_END = w.pos;
				asm.q.cmp_r_r(I.ip, I.eip);
				asm.jc_rel(C.L, firstDispatchOffset - w.pos); // jump to dispatch (loop)
				handler_RETURN = w.pos;

				// Load return count from func.sig.results.length
				var cnt = I.r0, i = I.r1;
				asm.movq_r_m(cnt, X86_64Addr.new(IVar.FUNC_DECL.gpr, null, 1, offsets.FuncDecl_sig));
				asm.movq_r_m(cnt, X86_64Addr.new(cnt, null, 1, offsets.SigDecl_results));
				asm.movd_r_m(cnt, X86_64Addr.new(cnt, null, 1, offsets.Array_length));
				var done = X86_64Label.new();
				// Copy return value(s) from VSP to VFP.
				asm.cmp_r_i(cnt, 0);
				asm.jc_rel_near(C.Z, done);
				asm.movd_r_i(i, 0);
				asm.d.shl_r_i(cnt, SLOT_SIZE_LOG);
				asm.q.sub_r_r(IVar.VSP.gpr, cnt);
				var loop = X86_64Label.new();
				// while (i < cnt)
				asm.bind(loop);
				asm.movdqu_s_m(I.xmm0, X86_64Addr.new(IVar.VSP.gpr, i, 1, 0));
				asm.movdqu_m_s(X86_64Addr.new(IVar.VFP.gpr, i, 1, 0), I.xmm0);
				asm.q.add_r_i(i, SLOT_SIZE);
				asm.q.cmp_r_r(i, cnt);
				asm.jc_rel_near(C.L, loop);
				asm.bind(done);
				asm.q.lea(IVar.VSP.gpr, X86_64Addr.new(IVar.VFP.gpr, cnt, 1, 0)); // set VSP properly
				// Deallocate interpreter frame and return to calling code.
				asm.q.add_r_i(R.RSP, frameSize);
				asm.movd_r_i(I.V3_RET_GPRS[0], ExecState.FINISHED.tag);
				asm.ret();

				return HandlerGenResult.Offset(if(opcode == Opcode.END, handler_END, handler_RETURN));
			}
			CALL => {
				// XXX: save ip prior to call for stack traces?
				genReadUleb32(I.r1); // TODO: r0 == ecx, which is overwritten

				asm.movq_r_m(I.r0, X86_64Addr.new(IVar.INSTANCE.gpr, null, 1, offsets.Instance_functions));
				asm.movq_r_m(I.r1, X86_64Addr.new(I.r0, I.r1, offsets.REF_SIZE, offsets.Array_contents));

				// call_indirect jumps here
				callFunctionOffset = w.pos;
				saveCallerIVars(false);
				var call_host = X86_64Label.new();
				asm.d.cmp_m_i(X86_64Addr.new(I.r1, null, 1, 0), offsets.WasmFunction_typeId);
				asm.jc_rel_near(C.NZ, call_host);

				// WasmFunction: call into interpreter reentry
				asm.callr_addr(callReentryRef);
				genExecStateCheck();
				restoreCallerIVars(false);
				genDispatchOrJumpToDispatch();

				// HostFunction: call into interpreter callback to enter into V3 code
				asm.bind(call_host);
				saveIVar(IVar.VSP);
				callV3WithVspp(refCallback(i.callHost), [I.r1]);
				genExecStateCheck();
				restoreCallerIVars(true);
			}
			CALL_INDIRECT => {
				// XXX: save ip prior to call for stack traces?
				var sig_index = I.r1, table_index = I.r2, func_index = I.r0;
				genReadUleb32(sig_index);
				genReadUleb32(table_index);

				asm.sub_r_i(I.vsp, SLOT_SIZE);
				asm.movd_r_m(func_index, I.vsp_ptr_p1);

				var trap_oob = X86_64Label.new(), trap_sig_mismatch = X86_64Label.new();

				var tmp = I.r3;
				// load instance.sig_ids[sig_index] into sig_index
				asm.movq_r_m(tmp, X86_64Addr.new(IVar.INSTANCE.gpr, null, 1, offsets.Instance_sig_ids));
				asm.movd_r_m(sig_index, X86_64Addr.new(tmp, sig_index, offsets.INT_SIZE, offsets.Array_contents));
				// Bounds-check table.ids[func_index]
				asm.movq_r_m(tmp, X86_64Addr.new(IVar.INSTANCE.gpr, null, 1, offsets.Instance_tables));
				var table = table_index;
				asm.movq_r_m(table, X86_64Addr.new(tmp, table_index, offsets.REF_SIZE, offsets.Array_contents));
				asm.movq_r_m(tmp, X86_64Addr.new(table, null, 1, offsets.Table_ids));
				asm.d.cmp_r_m(func_index, X86_64Addr.new(tmp, null, 1, offsets.Array_length));
				asm.jc_rel_near(C.NC, trap_oob);
				// Check table.ids[func_index] == sig_index
				asm.d.cmp_r_m(sig_index, X86_64Addr.new(tmp, func_index, offsets.INT_SIZE, offsets.Array_contents));
				asm.jc_rel_near(C.NZ, trap_sig_mismatch);
				// Load table.funcs[func_index] into r1 and jump to calling sequence
				asm.movq_r_m(tmp, X86_64Addr.new(table, null, 1, offsets.Table_funcs));
				asm.movq_r_m(I.r1, X86_64Addr.new(tmp, func_index, offsets.REF_SIZE, offsets.Array_contents));
				asm.jmp_rel(callFunctionOffset - w.pos);
				// XXX: move traps for call indirect out-of-line
				asm.bind(trap_oob);
				genAbruptReturn(ExecState.TRAPPED, TrapReason.FUNC_INVALID);
				asm.bind(trap_sig_mismatch);
				asm.d.cmp_m_i(X86_64Addr.new(tmp, func_index, offsets.INT_SIZE, offsets.Array_contents), 0);
				asm.jc_rel_near(C.S, trap_oob); // < 0 implies invalid function, not function sig mismatch
				genAbruptReturn(ExecState.TRAPPED, TrapReason.FUNC_SIG_MISMATCH);
				return HandlerGenResult.END;
			}
			DROP => {
				asm.sub_r_i(I.vsp, SLOT_SIZE);
			}
			SELECT => {
				var label = X86_64Label.new();
				asm.d.cmp_m_i(I.vsp_ptr_m1, 0);
				asm.jc_rel_near(C.NZ, label);
				// false case; copy false value down
				asm.movq_r_m(I.r0, I.vsp_ptr_m3);
				asm.movq_m_r(I.vsp_ptr_m5, I.r0);
				// true case, nothing to do
				asm.bind(label);
				asm.sub_r_i(I.vsp, 2 * SLOT_SIZE);
			}
			SELECT_T => {
				genReadUleb32(I.r0); // load # values
				var skip = X86_64Label.new();
				asm.movd_r_r(I.r1, I.r0);
				asm.bind(skip);  // skip value types
				genSkipLeb();
				asm.dec_r(I.r1);
				asm.jc_rel_near(C.NZ, skip);

				asm.d.shl_r_i(I.r0, SLOT_SIZE_LOG);
				asm.movd_r_m(I.r1, I.vsp_ptr_m1);
				asm.sub_r_r(I.vsp, I.r0);
				asm.sub_r_i(I.vsp, SLOT_SIZE); // XXX: combine with above using lea
				asm.d.cmp_r_i(I.r1, 0);
				var label = X86_64Label.new();
				asm.jc_rel_near(C.NZ, label);
				// false case; copy false values down
				asm.movq_r_r(I.r1, I.vsp);
				asm.q.sub_r_r(I.r1, I.r0);
				var copy = X86_64Label.new();
				asm.bind(copy);
				asm.movq_r_m(I.r2, X86_64Addr.new(I.vsp, I.r0, 1, - Pointer.SIZE));
				asm.movq_m_r(X86_64Addr.new(I.r1, I.r0, 1, - Pointer.SIZE), I.r2);
				asm.d.sub_r_i(I.r0, SLOT_SIZE);
				asm.jc_rel_near(C.NZ, copy);
				// true case, nothing to do
				asm.bind(label);
			}
			LOCAL_GET => {
				genReadUleb32(I.r0);
				asm.d.shl_r_i(I.r0, SLOT_SIZE_LOG);
				asm.movdqu_s_m(I.xmm0, X86_64Addr.new(IVar.VFP.gpr, I.r0, 1, 0));
				asm.movdqu_m_s(I.vsp_ptr, I.xmm0);
				asm.add_r_i(I.vsp, SLOT_SIZE);
			}
			LOCAL_SET => {
				genReadUleb32(I.r0);
				asm.d.shl_r_i(I.r0, SLOT_SIZE_LOG);
				asm.sub_r_i(I.vsp, SLOT_SIZE);
				asm.movq_r_m(I.r1, I.vsp_ptr_p1);
				asm.movq_m_r(X86_64Addr.new(IVar.VFP.gpr, I.r0, 1, 8), I.r1);
			}
			LOCAL_TEE => {
				genReadUleb32(I.r0);
				asm.d.shl_r_i(I.r0, SLOT_SIZE_LOG);
				asm.movq_r_m(I.r1, X86_64Addr.new(IVar.VSP.gpr, null, 1, 8 - SLOT_SIZE));
				asm.movq_m_r(X86_64Addr.new(IVar.VFP.gpr, I.r0, 1, 8), I.r1);
			}
			GLOBAL_GET => genCallbackWith1Leb(refCallback(i.callback_GLOBAL_GET), false);
			GLOBAL_SET => genCallbackWith1Leb(refCallback(i.callback_GLOBAL_SET), false);
			TABLE_GET => genCallbackWith1Leb(refCallback(i.callback_TABLE_GET), true);
			TABLE_SET => genCallbackWith1Leb(refCallback(i.callback_TABLE_SET), true);
			MEMORY_SIZE => {
				genReadUleb32(I.r1); // TODO: r0 == ecx, which is overwritten
				asm.movq_r_m(I.r0, X86_64Addr.new(IVar.INSTANCE.gpr, null, 1, offsets.Instance_memories));
				asm.movq_r_m(I.r0, X86_64Addr.new(I.r0, I.r1, offsets.REF_SIZE, offsets.Array_contents));
				asm.movq_r_m(I.r1, X86_64Addr.new(I.r0, null, 1, offsets.X86_64Memory_limit));
				asm.movq_r_m(I.r0, X86_64Addr.new(I.r0, null, 1, offsets.X86_64Memory_start));
				asm.q.sub_r_r(I.r1, I.r0);
				asm.q.shr_r_i(I.r1, 16);
				asm.movq_m_i(I.vsp_ptr, i7.view(BpTypecon.I32.code));
				asm.movq_m_r(I.vsp_ptr_p1, I.r1);
				asm.add_r_i(I.vsp, SLOT_SIZE);
			}
			MEMORY_GROW => genCallbackWith1Leb(refCallback(i.callback_MEMORY_GROW), false);
			I32_CONST => {
				genReadSleb32_inline(I.r1); // TODO: r0 == ecx, which is overwritten
				asm.movq_m_i(I.vsp_ptr, i7.view(BpTypecon.I32.code));
				asm.movq_m_r(I.vsp_ptr_p1, I.r1);
				asm.add_r_i(I.vsp, SLOT_SIZE);
			}
			I64_CONST => {
				genReadSleb64_inline(I.r1); // TODO: r0 == ecx, which is overwritten
				asm.movq_m_i(I.vsp_ptr, i7.view(BpTypecon.I64.code));
				asm.movq_m_r(I.vsp_ptr_p1, I.r1);
				asm.add_r_i(I.vsp, SLOT_SIZE);
			}
			F32_CONST => {
				asm.movd_r_m(I.r0, I.ip_ptr);
				asm.add_r_i(I.ip, 4);
				asm.movq_m_i(I.vsp_ptr, i7.view(BpTypecon.F32.code));
				asm.movq_m_r(I.vsp_ptr_p1, I.r0);
				asm.add_r_i(I.vsp, SLOT_SIZE);
			}
			F64_CONST => {
				asm.movq_r_m(I.r0, I.ip_ptr);
				asm.add_r_i(I.ip, 8);
				asm.movq_m_i(I.vsp_ptr, i7.view(BpTypecon.F64.code));
				asm.movq_m_r(I.vsp_ptr_p1, I.r0);
				asm.add_r_i(I.vsp, SLOT_SIZE);
			}
			I32_EQZ => {
				asm.d.test_m_i(I.vsp_ptr_m1, -1);
				asm.set_r(C.Z, I.r0);
				asm.movbzx_r_r(I.r0, I.r0);
				asm.movd_m_r(I.vsp_ptr_m1, I.r0);
			}
			I32_EQ => genCmpI32(C.Z);
			I32_NE => genCmpI32(C.NZ);
			I32_LT_S => genCmpI32(C.L);
			I32_LT_U => genCmpI32(C.C);
			I32_GT_S => genCmpI32(C.G);
			I32_GT_U => genCmpI32(C.A);
			I32_LE_S => genCmpI32(C.LE);
			I32_LE_U => genCmpI32(C.NA);
			I32_GE_S => genCmpI32(C.GE);
			I32_GE_U => genCmpI32(C.NC);
			I64_EQZ => {
				asm.q.test_m_i(I.vsp_ptr_m1, -1);
				asm.set_r(C.Z, I.r0);
				asm.movbzx_r_r(I.r0, I.r0);
				asm.movd_m_r(I.vsp_ptr_m1, I.r0);
				asm.movd_m_i(I.vsp_ptr_m2, BpTypecon.I32.code);
			}
			I64_EQ, REF_EQ => genCmpI64(C.Z); // XXX: share handler code
			I64_NE => genCmpI64(C.NZ);
			I64_LT_S => genCmpI64(C.L);
			I64_LT_U => genCmpI64(C.C);
			I64_GT_S => genCmpI64(C.G);
			I64_GT_U => genCmpI64(C.A);
			I64_LE_S => genCmpI64(C.LE);
			I64_LE_U => genCmpI64(C.NA);
			I64_GE_S => genCmpI64(C.GE);
			I64_GE_U => genCmpI64(C.NC);
			I32_CLZ => {
				asm.movd_r_i(I.r1, -1);
				asm.d.bsr_r_m(I.r0, I.vsp_ptr_m1);
				asm.d.cmov_r(C.Z, I.r0, I.r1);
				asm.movd_r_i(I.r1, 31);
				asm.d.sub_r_r(I.r1, I.r0);
				asm.movd_m_r(I.vsp_ptr_m1, I.r1);
			}
			I32_CTZ => {
				asm.d.bsf_r_m(I.r0, I.vsp_ptr_m1);
				asm.movd_r_i(I.r1, 32);
				asm.d.cmov_r(C.Z, I.r0, I.r1);
				asm.movd_m_r(I.vsp_ptr_m1, I.r0);
			}
			I32_POPCNT => {
				asm.d.popcnt_r_m(I.r0, I.vsp_ptr_m1);
				asm.movd_m_r(I.vsp_ptr_m1, I.r0);
			}
			I32_ADD => genBinopI32_m_r(asm.d.add_m_r);
			I32_SUB => genBinopI32_m_r(asm.d.sub_m_r);
			I32_MUL => {
				asm.movd_r_m(I.r0, I.vsp_ptr_m1);
				asm.d.imul_r_m(I.r0, I.vsp_ptr_m3);
				asm.movd_m_r(I.vsp_ptr_m3, I.r0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			I32_DIV_S => {
				var rax = X86_64Regs.RAX, rdx = X86_64Regs.RDX;
				var div = X86_64Label.new(), neg = X86_64Label.new(), done = X86_64Label.new();
				asm.movd_r_m(I.r0, I.vsp_ptr_m1);
				asm.d.cmp_r_i(I.r0, -1);
				asm.jc_rel_near(C.NZ, div);
				asm.movd_r_m(I.r0, I.vsp_ptr_m3);
				asm.d.cmp_r_i(I.r0, 0x80000000);
				asm.jc_rel_near(C.NZ, neg);
				genAbruptReturn(ExecState.TRAPPED, TrapReason.DIV_UNREPRESENTABLE);
				asm.bind(neg);
				asm.d.neg_m(I.vsp_ptr_m3);
				asm.jmp_rel_near(done);
				asm.bind(div);
				saveReg(rax);
				saveReg(rdx);
				asm.movd_r_m(rax, I.vsp_ptr_m3);
				asm.d.cdq();
				asm.d.idiv_r(I.r0);
				asm.movd_m_r(I.vsp_ptr_m3, rax);
				restoreReg(rax);
				restoreReg(rdx);
				asm.bind(done);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			I32_DIV_U => {
				var rax = X86_64Regs.RAX, rdx = X86_64Regs.RDX;
				asm.movd_r_m(I.r0, I.vsp_ptr_m1);
				saveReg(rax);
				saveReg(rdx);
				asm.movd_r_m(rax, I.vsp_ptr_m3);
				asm.movd_r_i(rdx, 0);
				asm.d.div_r(I.r0);
				asm.movd_m_r(I.vsp_ptr_m3, rax);
				restoreReg(rax);
				restoreReg(rdx);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			I32_REM_S => {
				var rax = X86_64Regs.RAX, rdx = X86_64Regs.RDX;
				var div = X86_64Label.new(), done = X86_64Label.new();
				asm.movd_r_m(I.r0, I.vsp_ptr_m1);
				asm.d.cmp_r_i(I.r0, -1);
				asm.jc_rel_near(C.NZ, div);
				asm.movd_m_i(I.vsp_ptr_m3, 0);
				asm.jmp_rel_near(done);
				asm.bind(div);
				saveReg(rax);
				saveReg(rdx);
				asm.movd_r_m(rax, I.vsp_ptr_m3);
				asm.d.cdq();
				asm.d.idiv_r(I.r0);
				asm.movd_m_r(I.vsp_ptr_m3, rdx);
				restoreReg(rax);
				restoreReg(rdx);
				asm.bind(done);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			I32_REM_U => {
				var rax = X86_64Regs.RAX, rdx = X86_64Regs.RDX;
				asm.movd_r_m(I.r0, I.vsp_ptr_m1);
				saveReg(rax);
				saveReg(rdx);
				asm.movd_r_m(rax, I.vsp_ptr_m3);
				asm.movd_r_i(rdx, 0);
				asm.d.div_r(I.r0);
				asm.movd_m_r(I.vsp_ptr_m3, rdx);
				restoreReg(rax);
				restoreReg(rdx);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			I32_AND => genBinopI32_m_r(asm.d.and_m_r);
			I32_OR => genBinopI32_m_r(asm.d.or_m_r);
			I32_XOR => genBinopI32_m_r(asm.d.xor_m_r);
			I32_SHL => genBinopI32_m_cl(asm.d.shl_m_cl);
			I32_SHR_S => genBinopI32_m_cl(asm.d.sar_m_cl);
			I32_SHR_U => genBinopI32_m_cl(asm.d.shr_m_cl);
			I32_ROTL => genBinopI32_m_cl(asm.d.rol_m_cl);
			I32_ROTR => genBinopI32_m_cl(asm.d.ror_m_cl);

			I64_CLZ => {
				asm.movq_r_i(I.r1, -1);
				asm.q.bsr_r_m(I.r0, I.vsp_ptr_m1);
				asm.q.cmov_r(C.Z, I.r0, I.r1);
				asm.movd_r_i(I.r1, 63);
				asm.q.sub_r_r(I.r1, I.r0);
				asm.movq_m_r(I.vsp_ptr_m1, I.r1);
			}
			I64_CTZ => {
				asm.q.bsf_r_m(I.r0, I.vsp_ptr_m1);
				asm.movd_r_i(I.r1, 64);
				asm.q.cmov_r(C.Z, I.r0, I.r1);
				asm.movq_m_r(I.vsp_ptr_m1, I.r0);
			}
			I64_POPCNT => {
				asm.q.popcnt_r_m(I.r0, I.vsp_ptr_m1);
				asm.movq_m_r(I.vsp_ptr_m1, I.r0);
			}
			I64_ADD => genBinopI64_m_r(asm.q.add_m_r);
			I64_SUB => genBinopI64_m_r(asm.q.sub_m_r);
			I64_MUL => {
				asm.movq_r_m(I.r0, I.vsp_ptr_m1);
				asm.q.imul_r_m(I.r0, I.vsp_ptr_m3);
				asm.movq_m_r(I.vsp_ptr_m3, I.r0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			I64_DIV_S => {
				var rax = X86_64Regs.RAX, rdx = X86_64Regs.RDX;
				var div = X86_64Label.new(), neg = X86_64Label.new(), done = X86_64Label.new();
				asm.movq_r_m(I.r0, I.vsp_ptr_m1);
				asm.cmp_r_i(I.r0, -1);
				asm.jc_rel_near(C.NZ, div);
				asm.movq_r_m(I.r0, I.vsp_ptr_m3);
				asm.movq_r_i(I.r1, 0x80);
				asm.shl_r_i(I.r1, 56);
				asm.cmp_r_r(I.r0, I.r1);
				asm.jc_rel_near(C.NZ, neg);
				genAbruptReturn(ExecState.TRAPPED, TrapReason.DIV_UNREPRESENTABLE);
				asm.bind(neg);
				asm.neg_m(I.vsp_ptr_m3);
				asm.jmp_rel_near(done);
				asm.bind(div);
				saveReg(rax);
				saveReg(rdx);
				asm.movq_r_m(rax, I.vsp_ptr_m3);
				asm.cqo();
				asm.idiv_r(I.r0);
				asm.movq_m_r(I.vsp_ptr_m3, rax);
				restoreReg(rax);
				restoreReg(rdx);
				asm.bind(done);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			I64_DIV_U => {
				var rax = X86_64Regs.RAX, rdx = X86_64Regs.RDX;
				asm.movq_r_m(I.r0, I.vsp_ptr_m1);
				saveReg(rax);
				saveReg(rdx);
				asm.movq_r_m(rax, I.vsp_ptr_m3);
				asm.movd_r_i(rdx, 0);
				asm.div_r(I.r0);
				asm.movq_m_r(I.vsp_ptr_m3, rax);
				restoreReg(rax);
				restoreReg(rdx);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			I64_REM_S => {
				var rax = X86_64Regs.RAX, rdx = X86_64Regs.RDX;
				var div = X86_64Label.new(), done = X86_64Label.new();
				asm.movq_r_m(I.r0, I.vsp_ptr_m1);
				asm.cmp_r_i(I.r0, -1);
				asm.jc_rel_near(C.NZ, div);
				asm.movq_m_i(I.vsp_ptr_m3, 0);
				asm.jmp_rel_near(done);
				asm.bind(div);
				saveReg(rax);
				saveReg(rdx);
				asm.movq_r_m(rax, I.vsp_ptr_m3);
				asm.cqo();
				asm.idiv_r(I.r0);
				asm.movq_m_r(I.vsp_ptr_m3, rdx);
				restoreReg(rax);
				restoreReg(rdx);
				asm.bind(done);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			I64_REM_U => {
				var rax = X86_64Regs.RAX, rdx = X86_64Regs.RDX;
				asm.movq_r_m(I.r0, I.vsp_ptr_m1);
				saveReg(rax);
				saveReg(rdx);
				asm.movq_r_m(rax, I.vsp_ptr_m3);
				asm.movd_r_i(rdx, 0);
				asm.div_r(I.r0);
				asm.movq_m_r(I.vsp_ptr_m3, rdx);
				restoreReg(rax);
				restoreReg(rdx);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			I64_AND => genBinopI64_m_r(asm.q.and_m_r);
			I64_OR => genBinopI64_m_r(asm.q.or_m_r);
			I64_XOR => genBinopI64_m_r(asm.q.xor_m_r);
			I64_SHL => genBinopI64_m_cl(asm.q.shl_m_cl);
			I64_SHR_S => genBinopI64_m_cl(asm.q.sar_m_cl);
			I64_SHR_U => genBinopI64_m_cl(asm.q.shr_m_cl);
			I64_ROTL => genBinopI64_m_cl(asm.q.rol_m_cl);
			I64_ROTR => genBinopI64_m_cl(asm.q.ror_m_cl);
			F32_ABS => {
				asm.d.and_m_i(I.vsp_ptr_m1, 0x7FFFFFFF); // explicit update of upper word
			}
			F32_NEG => {
				asm.d.xor_m_i(I.vsp_ptr_m1, 0x80000000); // explicit update of upper word
			}
			F32_ADD => {
				asm.movss_s_m(I.xmm0, I.vsp_ptr_m3);
				asm.addss_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movss_m_s(I.vsp_ptr_m3, I.xmm0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			F32_SUB => {
				asm.movss_s_m(I.xmm0, I.vsp_ptr_m3);
				asm.subss_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movss_m_s(I.vsp_ptr_m3, I.xmm0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			F32_MUL => {
				asm.movss_s_m(I.xmm0, I.vsp_ptr_m3);
				asm.mulss_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movss_m_s(I.vsp_ptr_m3, I.xmm0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			F32_DIV => {
				asm.movss_s_m(I.xmm0, I.vsp_ptr_m3);
				asm.divss_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movss_m_s(I.vsp_ptr_m3, I.xmm0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			F32_CEIL => genRoundF32(X86_64Rounding.TO_POS_INF);
			F32_FLOOR => genRoundF32(X86_64Rounding.TO_NEG_INF);
			F32_TRUNC => genRoundF32(X86_64Rounding.TO_ZERO);
			F32_NEAREST => genRoundF32(X86_64Rounding.TO_NEAREST);
			F32_SQRT => {
				asm.sqrtss_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movss_m_s(I.vsp_ptr_m1, I.xmm0);
			}
			F32_COPYSIGN => {
				asm.movd_r_m(I.r0, I.vsp_ptr_m3); // XXX: tradeoff between memory operands and extra regs?
				asm.d.and_r_i(I.r0, 0x7FFFFFFF);
				asm.movd_r_m(I.r1, I.vsp_ptr_m1);
				asm.d.and_r_i(I.r1, 0x80000000);
				asm.d.or_r_r(I.r0, I.r1);
				asm.movd_m_r(I.vsp_ptr_m3, I.r0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			F64_ABS => {
				asm.d.and_m_i(I.vsp_ptr_m1u, 0x7FFFFFFF);
			}
			F64_NEG => {
				asm.d.xor_m_i(I.vsp_ptr_m1u, 0x80000000);
			}
			F64_ADD => {
				asm.movsd_s_m(I.xmm0, I.vsp_ptr_m3);
				asm.addsd_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movsd_m_s(I.vsp_ptr_m3, I.xmm0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			F64_SUB => {
				asm.movsd_s_m(I.xmm0, I.vsp_ptr_m3);
				asm.subsd_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movsd_m_s(I.vsp_ptr_m3, I.xmm0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			F64_MUL => {
				asm.movsd_s_m(I.xmm0, I.vsp_ptr_m3);
				asm.mulsd_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movsd_m_s(I.vsp_ptr_m3, I.xmm0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			F64_DIV => {
				asm.movsd_s_m(I.xmm0, I.vsp_ptr_m3);
				asm.divsd_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movsd_m_s(I.vsp_ptr_m3, I.xmm0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			F64_CEIL => genRoundF64(X86_64Rounding.TO_POS_INF);
			F64_FLOOR => genRoundF64(X86_64Rounding.TO_NEG_INF);
			F64_TRUNC => genRoundF64(X86_64Rounding.TO_ZERO);
			F64_NEAREST => genRoundF64(X86_64Rounding.TO_NEAREST);
			F64_SQRT => {
				asm.sqrtsd_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movsd_m_s(I.vsp_ptr_m1, I.xmm0);
			}
			F64_COPYSIGN => {
				asm.movd_r_m(I.r0, I.vsp_ptr_m3u); // XXX: tradeoff between memory operands and extra regs?
				asm.d.and_r_i(I.r0, 0x7FFFFFFF);
				asm.movd_r_m(I.r1, I.vsp_ptr_m1u);
				asm.d.and_r_i(I.r1, 0x80000000);
				asm.d.or_r_r(I.r0, I.r1);
				asm.movd_m_r(I.vsp_ptr_m3u, I.r0);
				asm.q.sub_r_i(I.vsp, SLOT_SIZE);
			}
			I32_WRAP_I64 => {
				genTagUpdate(BpTypecon.I32.code);
			}
			I32_TRUNC_F32_S => return genFloatTrunc(TRUNC_i32_f32_s, false);
			I32_TRUNC_F32_U => return genFloatTrunc(TRUNC_i32_f32_u, false);
			I32_TRUNC_F64_S => return genFloatTrunc(TRUNC_i32_f64_s, false);
			I32_TRUNC_F64_U => return genFloatTrunc(TRUNC_i32_f64_u, false);
			I64_TRUNC_F32_S => return genFloatTrunc(TRUNC_i64_f32_s, false);
			I64_TRUNC_F32_U => return genFloatTrunc(TRUNC_i64_f32_u, false);
			I64_TRUNC_F64_S => return genFloatTrunc(TRUNC_i64_f64_s, false);
			I64_TRUNC_F64_U => return genFloatTrunc(TRUNC_i64_f64_u, false);
			F32_CONVERT_I32_S => {
				genTagUpdate(BpTypecon.F32.code);
				asm.movd_r_m(I.r0, I.vsp_ptr_m1);
				asm.q.shl_r_i(I.r0, 32);
				asm.q.sar_r_i(I.r0, 32); // sign-extend
				asm.cvtsi2ss_s_r(I.xmm0, I.r0);
				asm.movss_m_s(I.vsp_ptr_m1, I.xmm0);
			}
			F32_CONVERT_I32_U => {
				genTagUpdate(BpTypecon.F32.code);
				asm.movd_r_m(I.r0, I.vsp_ptr_m1);
				asm.cvtsi2ss_s_r(I.xmm0, I.r0);
				asm.movss_m_s(I.vsp_ptr_m1, I.xmm0);
			}
			F32_CONVERT_I64_S => {
				genTagUpdate(BpTypecon.F32.code);
				asm.movq_r_m(I.r0, I.vsp_ptr_m1);
				asm.cvtsi2ss_s_r(I.xmm0, I.r0);
				asm.movss_m_s(I.vsp_ptr_m1, I.xmm0);
			}
			F32_CONVERT_I64_U => {
				genTagUpdate(BpTypecon.F32.code);
				asm.movq_r_m(I.r0, I.vsp_ptr_m1);
				asm.q.cvtsi2ss_s_r(I.xmm0, I.r0);
				asm.q.cmp_r_i(I.r0, 0);
				var done = X86_64Label.new();
				asm.jc_rel_near(C.NS, done);
				// input < 0, compute 2.0d * cvt((x >> 1) | (x&1))
				asm.movq_r_r(I.r1, I.r0);
				asm.q.and_r_i(I.r1, 1);
				asm.q.shr_r_i(I.r0, 1);
				asm.q.or_r_r(I.r0, I.r1);
				asm.q.cvtsi2ss_s_r(I.xmm0, I.r0);
				asm.movd_r_i(I.r1, int.view(Floats.f_1p1));
				asm.movd_s_r(I.xmm1, I.r1);
				asm.mulss_s_s(I.xmm0, I.xmm1);
				// done
				asm.bind(done);
				asm.movss_m_s(I.vsp_ptr_m1, I.xmm0);
			}
			F32_DEMOTE_F64 => {
				genTagUpdate(BpTypecon.F32.code);
				asm.movsd_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.cvtsd2ss_s_s(I.xmm0, I.xmm0);
				asm.movss_m_s(I.vsp_ptr_m1, I.xmm0);
			}
			F64_CONVERT_I32_S => {
				genTagUpdate(BpTypecon.F64.code);
				asm.movd_r_m(I.r0, I.vsp_ptr_m1);
				asm.q.shl_r_i(I.r0, 32);
				asm.q.sar_r_i(I.r0, 32); // sign-extend
				asm.cvtsi2sd_s_r(I.xmm0, I.r0);
				asm.movsd_m_s(I.vsp_ptr_m1, I.xmm0);
			}
			F64_CONVERT_I32_U => {
				genTagUpdate(BpTypecon.F64.code);
				asm.movd_r_m(I.r0, I.vsp_ptr_m1);
				asm.cvtsi2sd_s_r(I.xmm0, I.r0);
				asm.movsd_m_s(I.vsp_ptr_m1, I.xmm0);
			}
			F64_CONVERT_I64_S => {
				genTagUpdate(BpTypecon.F64.code);
				asm.movq_r_m(I.r0, I.vsp_ptr_m1);
				asm.cvtsi2sd_s_r(I.xmm0, I.r0);
				asm.movsd_m_s(I.vsp_ptr_m1, I.xmm0);
			}
			F64_CONVERT_I64_U => {
				genTagUpdate(BpTypecon.F64.code);
				asm.movq_r_m(I.r0, I.vsp_ptr_m1);
				asm.q.cvtsi2sd_s_r(I.xmm0, I.r0);
				asm.q.cmp_r_i(I.r0, 0);
				var done = X86_64Label.new();
				asm.jc_rel_near(C.NS, done);
				// input < 0, compute 2.0d * cvt((x >> 1) | (x&1))
				asm.movq_r_r(I.r1, I.r0);
				asm.q.and_r_i(I.r1, 1);
				asm.q.shr_r_i(I.r0, 1);
				asm.q.or_r_r(I.r0, I.r1);
				asm.q.cvtsi2sd_s_r(I.xmm0, I.r0);
				asm.movd_r_i(I.r1, int.view(Floats.d_1p1 >> 32));
				asm.q.shl_r_i(I.r1, 32);
				asm.movq_s_r(I.xmm1, I.r1);
				asm.mulsd_s_s(I.xmm0, I.xmm1);
				// done
				asm.bind(done);
				asm.movsd_m_s(I.vsp_ptr_m1, I.xmm0);
			}
			F64_PROMOTE_F32 => {
				genTagUpdate(BpTypecon.F64.code);
				asm.movss_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.cvtss2sd_s_s(I.xmm0, I.xmm0);
				asm.movsd_m_s(I.vsp_ptr_m1, I.xmm0);
			}
			I32_REINTERPRET_F32 => {
				genTagUpdate(BpTypecon.I32.code);
			}
			I64_REINTERPRET_F64 => {
				genTagUpdate(BpTypecon.I64.code);
			}
			F32_REINTERPRET_I32 => {
				genTagUpdate(BpTypecon.F32.code);
			}
			F64_REINTERPRET_I64 => {
				genTagUpdate(BpTypecon.F64.code);
			}
			I32_EXTEND8_S => {
				asm.d.movbsx_r_m(I.r0, I.vsp_ptr_m1);
				asm.movd_m_r(I.vsp_ptr_m1, I.r0);
			}
			I32_EXTEND16_S => {
				asm.d.movwsx_r_m(I.r0, I.vsp_ptr_m1);
				asm.movd_m_r(I.vsp_ptr_m1, I.r0);
			}
			I64_EXTEND8_S => {
				asm.q.movbsx_r_m(I.r0, I.vsp_ptr_m1);
				asm.movq_m_r(I.vsp_ptr_m1, I.r0);
			}
			I64_EXTEND16_S => {
				asm.q.movwsx_r_m(I.r0, I.vsp_ptr_m1);
				asm.movq_m_r(I.vsp_ptr_m1, I.r0);
			}
			I64_EXTEND_I32_S, I64_EXTEND32_S => { // XXX: share code between these two
				genTagUpdate(BpTypecon.I64.code);
				asm.movd_r_m(I.r0, I.vsp_ptr_m1);
				asm.q.shl_r_i(I.r0, 32);
				asm.q.sar_r_i(I.r0, 32);
				asm.movq_m_r(I.vsp_ptr_m1, I.r0);
			}
			I64_EXTEND_I32_U => {
				genTagUpdate(BpTypecon.I64.code);
				asm.movd_m_i(X86_64Addr.new(IVar.VSP.gpr, null, 1, -4), 0); // zero upper portion
			}
			REF_NULL => {
				asm.movbzx_r_m(I.r0, I.ip_ptr);
				genSkipLeb(); // skip type
				asm.movq_m_r(I.vsp_ptr, I.r0);
				asm.movq_m_i(I.vsp_ptr_p1, 0);
				asm.add_r_i(I.vsp, SLOT_SIZE);
			}
			REF_IS_NULL => {
				asm.d.test_m_i(I.vsp_ptr_m1, -1);
				asm.set_r(C.Z, I.r0);
				asm.movbzx_r_r(I.r0, I.r0);
				asm.movd_m_i(I.vsp_ptr_m2, i7.view(BpTypecon.I32.code));
				asm.movd_m_r(I.vsp_ptr_m1, I.r0);
			}
			REF_FUNC => {
				genReadUleb32(I.r1); // TODO: r0 == ecx, which is overwritten
				asm.movq_r_m(I.r0, X86_64Addr.new(IVar.INSTANCE.gpr, null, 1, offsets.Instance_functions));
				asm.movq_r_m(I.r0, X86_64Addr.new(I.r0, I.r1, offsets.REF_SIZE, offsets.Array_contents));
				asm.movq_m_i(I.vsp_ptr, i7.view(BpTypecon.FUNCREF.code));
				asm.movq_m_r(I.vsp_ptr_p1, I.r0);
				asm.add_r_i(I.vsp, SLOT_SIZE);
			}
			I32_TRUNC_SAT_F32_S =>  return genFloatTrunc(TRUNC_i32_f32_s, true);
			I32_TRUNC_SAT_F32_U => return genFloatTrunc(TRUNC_i32_f32_u, true);
			I32_TRUNC_SAT_F64_S => return genFloatTrunc(TRUNC_i32_f64_s, true);
			I32_TRUNC_SAT_F64_U => return genFloatTrunc(TRUNC_i32_f64_u, true);
			I64_TRUNC_SAT_F32_S => { // TODO: tricky constant
				genTagUpdate(BpTypecon.I64.code);
				asm.movss_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movd_r_i(I.r0, int.view(Floats.f_1p63));
				asm.movd_s_r(I.xmm1, I.r0);
				asm.ucomiss_s_s(I.xmm0, I.xmm1);
				var is_nan = X86_64Label.new(), ovf_pos = X86_64Label.new(), done = X86_64Label.new();
				asm.jc_rel_near(C.P, is_nan);
				asm.jc_rel_near(C.NC, ovf_pos);
				asm.roundss_s_s(I.xmm0, I.xmm0, X86_64Rounding.TO_ZERO);
				asm.q.cvtss2si_r_s(I.r0, I.xmm0);
				asm.bind(done);
				asm.movq_m_r(I.vsp_ptr_m1, I.r0);
				genDispatchOrJumpToDispatch();
				asm.bind(is_nan);
				asm.movd_r_i(I.r0, 0);
				asm.jmp_rel_near(done);
				asm.bind(ovf_pos);
				asm.movq_r_i(I.r0, 0xFFFFFFFE);
				asm.q.ror_r_i(I.r0, 1); // result = 0x7FFFFFFF_FFFFFFFF
				asm.jmp_rel_near(done);
				return HandlerGenResult.END;
			}
			I64_TRUNC_SAT_F32_U => return genFloatTrunc(TRUNC_i64_f32_u, true);
			I64_TRUNC_SAT_F64_S => { // TODO: tricky constant
				genTagUpdate(BpTypecon.I64.code);
				asm.movsd_s_m(I.xmm0, I.vsp_ptr_m1);
				asm.movd_r_i(I.r0, int.view(Floats.d_1p63 >> 32));
				asm.q.shl_r_i(I.r0, 32);
				asm.movq_s_r(I.xmm1, I.r0);
				asm.ucomisd_s_s(I.xmm0, I.xmm1);
				var is_nan = X86_64Label.new(), ovf_pos = X86_64Label.new(), done = X86_64Label.new();
				asm.jc_rel_near(C.P, is_nan);
				asm.jc_rel_near(C.NC, ovf_pos);
				asm.roundsd_s_s(I.xmm0, I.xmm0, X86_64Rounding.TO_ZERO);
				asm.q.cvtsd2si_r_s(I.r0, I.xmm0);
				asm.bind(done);
				asm.movq_m_r(I.vsp_ptr_m1, I.r0);
				genDispatchOrJumpToDispatch();
				asm.bind(is_nan);
				asm.movd_r_i(I.r0, 0);
				asm.jmp_rel_near(done);
				asm.bind(ovf_pos);
				asm.movq_r_i(I.r0, 0xFFFFFFFE);
				asm.q.ror_r_i(I.r0, 1); // result = 0x7FFFFFFF_FFFFFFFF
				asm.jmp_rel_near(done);
				return HandlerGenResult.END;
			}
			I64_TRUNC_SAT_F64_U => return genFloatTrunc(TRUNC_i64_f64_u, true);
			MEMORY_INIT => genCallbackWith2Lebs(refCallback(i.callback_MEMORY_INIT), true);
			DATA_DROP => {
				genReadUleb32(I.r1);
				asm.movq_r_m(I.r0, X86_64Addr.new(IVar.INSTANCE.gpr, null, 1, offsets.Instance_dropped_data));
				asm.movb_m_i(X86_64Addr.new(I.r0, I.r1, 1, offsets.Array_contents), 1);
			}
			MEMORY_COPY => genCallbackWith2Lebs(refCallback(i.callback_MEMORY_COPY), true); // XXX: inline
			MEMORY_FILL => genCallbackWith1Leb(refCallback(i.callback_MEMORY_FILL), true);
			TABLE_INIT => genCallbackWith2Lebs(refCallback(i.callback_TABLE_INIT), true);
			ELEM_DROP => {
				genReadUleb32(I.r1);
				asm.movq_r_m(I.r0, X86_64Addr.new(IVar.INSTANCE.gpr, null, 1, offsets.Instance_dropped_elems));
				asm.movb_m_i(X86_64Addr.new(I.r0, I.r1, 1, offsets.Array_contents), 1);
			}
			TABLE_COPY => genCallbackWith2Lebs(refCallback(i.callback_TABLE_COPY), true);
			TABLE_GROW => genCallbackWith1Leb(refCallback(i.callback_TABLE_GROW), false);
			TABLE_SIZE => {
				genReadUleb32(I.r1); // TODO: r0 == ecx, which is overwritten
				asm.movq_r_m(I.r0, X86_64Addr.new(IVar.INSTANCE.gpr, null, 1, offsets.Instance_tables));
				asm.movq_r_m(I.r0, X86_64Addr.new(I.r0, I.r1, offsets.REF_SIZE, offsets.Array_contents));
				asm.movq_r_m(I.r0, X86_64Addr.new(I.r0, null, 1, offsets.Table_elems));
				asm.movq_r_m(I.r0, X86_64Addr.new(I.r0, null, 1, offsets.Array_length));
				asm.movq_m_i(I.vsp_ptr, i7.view(BpTypecon.I32.code));
				asm.movq_m_r(I.vsp_ptr_p1, I.r0);
				asm.add_r_i(I.vsp, SLOT_SIZE);
			}
			TABLE_FILL => genCallbackWith1Leb(refCallback(i.callback_TABLE_FILL), true);
			_ => return HandlerGenResult.UNHANDLED;
		}
		return HandlerGenResult.HANDLED;
	}
	def genFloatMinAndMax() {
		var ret_b = X86_64Label.new(), ret_a = X86_64Label.new(), is_nan32 = X86_64Label.new(), is_nan64 = X86_64Label.new();
		bindOpcodeHandler(Opcode.F32_MIN);
		asm.movss_s_m(I.xmm0, I.vsp_ptr_m3);
		asm.movss_s_m(I.xmm1, I.vsp_ptr_m1);
		asm.ucomiss_s_s(I.xmm0, I.xmm1);
		asm.jc_rel_far(C.P, is_nan32);
		asm.jc_rel_near(C.C, ret_a);
		asm.jc_rel_near(C.A, ret_b);
		asm.d.cmp_m_i(I.vsp_ptr_m1, 0);
		asm.jc_rel_near(C.S, ret_b); // handle min(-0, 0) == -0
		asm.jmp_rel_near(ret_a);

		bindOpcodeHandler(Opcode.F32_MAX);
		asm.movss_s_m(I.xmm0, I.vsp_ptr_m3);
		asm.movss_s_m(I.xmm1, I.vsp_ptr_m1);
		asm.ucomiss_s_s(I.xmm0, I.xmm1);
		asm.jc_rel_far(C.P, is_nan32);
		asm.jc_rel_near(C.C, ret_b);
		asm.jc_rel_near(C.A, ret_a);
		asm.d.cmp_m_i(I.vsp_ptr_m1, 0);
		asm.jc_rel_near(C.NS, ret_b); // handle max(-0, 0) == 0
		asm.jmp_rel_near(ret_a);

		bindOpcodeHandler(Opcode.F64_MIN);
		asm.movsd_s_m(I.xmm0, I.vsp_ptr_m3);
		asm.movsd_s_m(I.xmm1, I.vsp_ptr_m1);
		asm.ucomisd_s_s(I.xmm0, I.xmm1);
		asm.jc_rel_near(C.P, is_nan64);
		asm.jc_rel_near(C.C, ret_a);
		asm.jc_rel_near(C.A, ret_b);
		asm.d.cmp_m_i(I.vsp_ptr_m1u, 0);
		asm.jc_rel_near(C.S, ret_b); // handle min(-0, 0) == -0
		// fall through to ret_a
		asm.bind(ret_a);
		asm.q.sub_r_i(I.vsp, SLOT_SIZE);
		genDispatchOrJumpToDispatch();

		bindOpcodeHandler(Opcode.F64_MAX);
		asm.movsd_s_m(I.xmm0, I.vsp_ptr_m3);
		asm.movsd_s_m(I.xmm1, I.vsp_ptr_m1);
		asm.ucomisd_s_s(I.xmm0, I.xmm1);
		asm.jc_rel_near(C.P, is_nan64);
		asm.jc_rel_near(C.C, ret_b);
		asm.jc_rel_near(C.A, ret_a);
		asm.d.cmp_m_i(I.vsp_ptr_m1u, 0);
		asm.jc_rel_near(C.S, ret_a); // handle max(-0, 0) == 0
		// fall through to ret_b
		asm.bind(ret_b);
		asm.movsd_m_s(I.vsp_ptr_m3, I.xmm1);
		asm.q.sub_r_i(I.vsp, SLOT_SIZE);
		genDispatchOrJumpToDispatch();

		asm.bind(is_nan32);
		asm.movd_m_i(I.vsp_ptr_m3, int.view(Floats.f_nan));
		asm.jmp_rel_near(ret_a);

		asm.bind(is_nan64);
		asm.movd_m_i(I.vsp_ptr_m3u, int.view(Floats.d_nan >> 32));
		asm.movd_m_i(I.vsp_ptr_m3, 0);
		asm.jmp_rel_near(ret_a);
	}
	def bindOpcodeHandler(opcode: Opcode) {
		patchDispatchTable(opcode, w.atEnd().pos);
	}
	def genFloatTrunc(config: FloatTrunc, saturate: bool) -> HandlerGenResult {
		config.mov_s_m(asm, I.xmm0, I.vsp_ptr_m1);
		config.mov_s_i(asm, I.xmm1, config.maxv);
		config.ucomi_s_s(asm, I.xmm0, I.xmm1);
		var above = X86_64Label.new(), is_nan = X86_64Label.new(), below = X86_64Label.new();
		var ret = X86_64Label.new();
		asm.jc_rel_near(C.P, is_nan);
		asm.jc_rel_near(C.NC, above);
		var not_big = X86_64Label.new();

		if (config.isI64 && !config.isSigned) {
			// handle u64 convert of v > 1p63 <= 1p64
			config.mov_s_i(asm, I.xmm1, if(config.isF64, Floats.d_1p63, Floats.f_1p63));
			config.ucomi_s_s(asm, I.xmm0, I.xmm1);
			asm.jc_rel_near(C.C, not_big);
			config.sub_s_s(asm, I.xmm0, I.xmm1);
			config.round_s_s(asm, I.xmm0, I.xmm0, X86_64Rounding.TO_ZERO);
			config.cvt2si_r_s(asm.q, I.r0, I.xmm0);
			asm.movd_r_i(I.r1, 1);
			asm.ror_r_i(I.r1, 1);
			asm.q.add_r_r(I.r0, I.r1);
			asm.jmp_rel_near(ret);
		}
		asm.bind(not_big);

		if (!saturate || config.isI64 || !config.isSigned) {
			config.mov_s_i(asm, I.xmm1, config.minv);
			config.ucomi_s_s(asm, I.xmm0, I.xmm1);
			asm.jc_rel_near(C.NA, below); // v <= min
		}

		config.round_s_s(asm, I.xmm0, I.xmm0, X86_64Rounding.TO_ZERO);
		if (!config.isI64 && config.isSigned) {
			config.cvt2si_r_s(asm.d, I.r0, I.xmm0);
		} else {
			config.cvt2si_r_s(asm.q, I.r0, I.xmm0);
		}
		asm.bind(ret);
		config.mov_m_r(asm, I.vsp_ptr_m1, I.r0);
		genTagUpdate(config.tag);
		genDispatchOrJumpToDispatch();
		if (saturate) {
			asm.bind(above);
			config.mov_r_i(asm, I.r0, config.ceilv);
			asm.jmp_rel_near(ret);
			asm.bind(is_nan);
			asm.bind(below);
			asm.movd_r_i(I.r0, 0);
			asm.jmp_rel_near(ret);
		} else {  // XXX: share trap code
			asm.bind(above);
			asm.bind(below);
			asm.bind(is_nan);
			genAbruptReturn(ExecState.TRAPPED, TrapReason.FLOAT_UNREPRESENTABLE);
		}
		return HandlerGenResult.END;
	}
	def genCallbackWith1Leb(ref: IcCodeRef, canTrap: bool) { // XXX: load ref and jump to shared routine
		genReadUleb32(I.r0);
		saveCallerIVars(true);
		callV3WithVspp(ref, [IVar.INSTANCE.gpr, I.r0]);
		if (canTrap) genExecStateCheck();
		restoreCallerIVars(true);
	}
	def genCallbackWith2Lebs(ref: IcCodeRef, canTrap: bool) { // XXX: load ref and jump to shared routine
		genReadUleb32(I.r0);
		genReadUleb32(I.r1);
		saveCallerIVars(true);
		callV3WithVspp(ref, [IVar.INSTANCE.gpr, I.r0, I.r1]);
		if (canTrap) genExecStateCheck();
		restoreCallerIVars(true);
	}
	def genExecStateCheck() {
		asm.cmpb_r_i(I.V3_RET_GPRS[0], ExecState.FINISHED.tag);
		asm.jc_rel_addr(C.NZ, abruptRetRef);
	}
	def saveCallerIVars(vsp: bool) {
		saveIVar(IVar.IP);
		saveIVar(IVar.XIP);
		if (vsp) saveIVar(IVar.VSP);
	}
	def restoreCallerIVars(vsp: bool) {
		restoreIVar(IVar.IP);
		restoreIVar(IVar.XIP);
		restoreIVar(IVar.EIP);
		restoreIVar(IVar.INSTANCE);
		restoreIVar(IVar.FUNC_DECL);
		restoreIVar(IVar.MEM0_BASE);
		restoreIVar(IVar.VFP);
		if (vsp) restoreIVar(IVar.VSP);
	}
	def callV3WithVspp(ref: IcCodeRef, regs: Array<X86_64Gpr>) {
		// Generate parallel moves from args into param gprs; assume each src register used only once
		var dst = Array<X86_64Gpr>.new(G.length);
		for (i < regs.length) {
			var sreg = regs[i];
			var dreg = I.V3_PARAM_GPRS[i + 1];
			if (sreg != dreg) dst[sreg.regnum] = dreg;
		}
		var stk = Array<i8>.new(G.length);
		for (i < dst.length) orderMoves(dst, stk, i);
		// load interpreter into first arg register
		asm.movq_r_m(I.V3_PARAM_GPRS[0], R.RSP.plus(IVar.INTERPRETER.frameOffset));
		asm.q.lea(I.V3_PARAM_GPRS[1 + regs.length], R.RSP.plus(IVar.VSP.frameOffset));
		asm.icall_m(ref);
	}
	def orderMoves(dst: Array<X86_64Gpr>, stk: Array<i8>, i: int) {
		var dreg = dst[i];
		if (dreg == null) return;		// no moves here
		if (stk[i] > 0) return;			// this node already done
		stk[i] = -1;				// mark as on stack
		if (stk[dreg.regnum] < 0) {		// destination on stack => cycle
			asm.movq_r_r(I.scratch, dreg);	// save destination first
			stk[dreg.regnum] = -2;		// mark as cycle
		} else {
			orderMoves(dst, stk, dreg.regnum);	// recurse on destination
		}
		asm.movq_r_r(dreg, if(stk[i] == -2, I.scratch, G[i]));	// emit post-order move
		stk[i] = 1;				// mark as done
	}
	def genTagUpdate(tag: byte) {
		asm.movq_m_i(I.vsp_ptr_m2, tag);
	}
	def saveIVar(iv: IVar) {
		asm.movq_m_r(R.RSP.plus(iv.frameOffset), iv.gpr);
	}
	def restoreIVar(iv: IVar) {
		asm.movq_r_m(iv.gpr, R.RSP.plus(iv.frameOffset));
	}
	def saveReg(r: X86_64Gpr) {
		for (iv in IVar) {
			if (iv.gpr == r) {
				if (iv.mutable) asm.movq_m_r(R.RSP.plus(iv.frameOffset), r);
				break;
			}
		}
	}
	def restoreReg(r: X86_64Gpr) {
		for (iv in IVar) {
			if (iv.gpr == r) {
				asm.movq_r_m(r, R.RSP.plus(iv.frameOffset));
				break;
			}
		}
	}
	def genLoad(opcode: Opcode, tag: byte, gen: (X86_64Gpr, X86_64Addr) -> X86_64Assembler) {
		bindOpcodeHandler(opcode);
		asm.q.inc_r(I.ip);				// skip flags byte
		genReadUleb32(I.r0);				// decode offset
		asm.movd_r_m(I.r1, I.vsp_ptr_m1);		// read index
		asm.q.add_r_r(I.r0, I.r1);			// add index + offset
		gen(I.r1, X86_64Addr.new(IVar.MEM0_BASE.gpr, I.r0, 1, 0));
		if (tag != BpTypecon.I32.code) genTagUpdate(tag); // update tag if necessary
		asm.movq_m_r(I.vsp_ptr_m1, I.r1);
		genDispatchOrJumpToDispatch();
	}
	def genStore(gen: (X86_64Addr, X86_64Gpr) -> X86_64Assembler) {
		asm.q.inc_r(I.ip);				// skip flags byte
		genReadUleb32(I.r0);				// decode offset
		asm.movd_r_m(I.r1, I.vsp_ptr_m3);		// read index
		asm.q.add_r_r(I.r0, I.r1);			// add index + offset
		asm.movq_r_m(I.r1, I.vsp_ptr_m1);		// read value
		gen(X86_64Addr.new(IVar.MEM0_BASE.gpr, I.r0, 1, 0), I.r1);
		asm.q.sub_r_i(I.vsp, 2 * SLOT_SIZE);
		genDispatchOrJumpToDispatch();
	}
	def genBinopI32_m_r(gen: (X86_64Addr, X86_64Gpr) -> X86_64Assembler) {
		asm.movd_r_m(I.r0, I.vsp_ptr_m1);
		gen(I.vsp_ptr_m3, I.r0);
		asm.q.sub_r_i(I.vsp, SLOT_SIZE);
	}
	def genBinopI32_m_cl(gen: X86_64Addr -> X86_64Assembler) {
		asm.movd_r_m(I.r0, I.vsp_ptr_m1);
		gen(I.vsp_ptr_m3);
		asm.q.sub_r_i(I.vsp, SLOT_SIZE);
	}
	def genBinopI64_m_r(gen: (X86_64Addr, X86_64Gpr) -> X86_64Assembler) {
		asm.movq_r_m(I.r0, I.vsp_ptr_m1);
		gen(I.vsp_ptr_m3, I.r0);
		asm.q.sub_r_i(I.vsp, SLOT_SIZE);
	}
	def genBinopI64_m_cl(gen: X86_64Addr -> X86_64Assembler) {
		asm.movq_r_m(I.r0, I.vsp_ptr_m1);
		gen(I.vsp_ptr_m3);
		asm.q.sub_r_i(I.vsp, SLOT_SIZE);
	}
	def genCmpI32(cond: X86_64Cond) {
		asm.movd_r_m(I.r0, I.vsp_ptr_m1);
		asm.d.cmp_m_r(I.vsp_ptr_m3, I.r0);
		asm.set_r(cond, I.r0);
		asm.movbzx_r_r(I.r0, I.r0);
		asm.movd_m_r(I.vsp_ptr_m3, I.r0);
		asm.q.sub_r_i(I.vsp, SLOT_SIZE);
	}
	def genCmpI64(cond: X86_64Cond) {
		asm.movq_r_m(I.r0, I.vsp_ptr_m1);
		asm.q.cmp_m_r(I.vsp_ptr_m3, I.r0);
		asm.set_r(cond, I.r0);
		asm.movbzx_r_r(I.r0, I.r0);
		asm.movq_m_r(I.vsp_ptr_m3, I.r0);
		asm.movq_m_i(I.vsp_ptr_m4, BpTypecon.I32.code);
		asm.q.sub_r_i(I.vsp, SLOT_SIZE);
	}
	def genRoundF32(rounding: X86_64Rounding) {
		asm.movss_s_m(I.xmm0, I.vsp_ptr_m1);
		asm.roundss_s_s(I.xmm0, I.xmm0, rounding);
		asm.movss_m_s(I.vsp_ptr_m1, I.xmm0);
	}
	def genRoundF64(rounding: X86_64Rounding) {
		asm.movsd_s_m(I.xmm0, I.vsp_ptr_m1);
		asm.roundsd_s_s(I.xmm0, I.xmm0, rounding);
		asm.movsd_m_s(I.vsp_ptr_m1, I.xmm0);
	}
	def genAbruptReturn(state: ExecState, reason: TrapReason) {
		asm.movd_r_i(I.V3_RET_GPRS[0], state.tag);
		asm.movd_r_i(I.V3_RET_GPRS[1], reason.tag);
		abruptRetRef = IcCodeRef.new(w.pos);
		asm.q.add_r_i(R.RSP, frameSize);
		asm.ret();
	}

	// Generate a read of a 32-bit unsigned LEB.
	def genReadUleb32(dest: X86_64Gpr) {
		var ool_leb = OutOfLineLEB.new(dest);
		oolULeb32Sites.put(ool_leb);
		var asm = this.asm.d;
		asm.movbzx_r_m(dest, I.ip_ptr);		// load first byte
		asm.q.inc_r(I.ip);			// increment pointer
		asm.test_r_i(dest, 0x80);		// test most-significant bit
		asm.jc_rel_addr(C.NZ, ool_leb);
		ool_leb.retOffset = asm.pos();
	}
	// Generate a read of a 32-bit signed LEB.
	def genReadSleb32_inline(dest: X86_64Gpr) {
		var done = X86_64Label.new(), sext = X86_64Label.new(), loop = X86_64Label.new();
		asm.movd_r_i(R.RCX, 0);
		asm.movd_r_i(dest, 0);

		asm.bind(loop);
		asm.movbzx_r_m(I.scratch, I.ip_ptr);	// load byte
		asm.q.inc_r(I.ip);			// increment pointer
		asm.d.test_r_i(I.scratch, 0x80);	// test most-significant bit
		asm.jc_rel_near(C.Z, sext);		// break if not set
		asm.d.and_r_i(I.scratch, 0x7F);		// mask off upper bit
		asm.d.shl_r_cl(I.scratch);		// shift byte into correct bit pos
		asm.d.or_r_r(dest, I.scratch);		// merge byte into val
		asm.d.add_r_i(R.RCX, 7);		// compute next bit pos
		asm.jmp_rel_near(loop);			// loop

		asm.bind(sext);
		asm.d.shl_r_cl(I.scratch);		// shift byte into correct bit pos
		asm.d.or_r_r(dest, I.scratch);		// merge byte into val
		asm.d.sub_r_i(R.RCX, 25);		// compute 32 - shift
		asm.d.neg_r(R.RCX);
		asm.jc_rel_near(C.S, done);		// if shift > 32, done
		asm.d.shl_r_cl(dest);			// sign extension
		asm.d.sar_r_cl(dest);
		asm.bind(done);
	}
	// Generate a read of a 32-bit signed LEB.
	def genReadSleb64_inline(dest: X86_64Gpr) {
		var done = X86_64Label.new(), sext = X86_64Label.new(), loop = X86_64Label.new();
		asm.movd_r_i(R.RCX, 0);
		asm.movd_r_i(dest, 0);

		asm.bind(loop);
		asm.movbzx_r_m(I.scratch, I.ip_ptr);	// load byte
		asm.q.inc_r(I.ip);			// increment pointer
		asm.d.test_r_i(I.scratch, 0x80);	// test most-significant bit
		asm.jc_rel_near(C.Z, sext);		// break if not set
		asm.d.and_r_i(I.scratch, 0x7F);		// mask off upper bit
		asm.q.shl_r_cl(I.scratch);		// shift byte into correct bit pos
		asm.q.or_r_r(dest, I.scratch);		// merge byte into val
		asm.d.add_r_i(R.RCX, 7);		// compute next bit pos
		asm.jmp_rel_near(loop);			// loop

		asm.bind(sext);
		asm.q.shl_r_cl(I.scratch);		// shift byte into correct bit pos
		asm.q.or_r_r(dest, I.scratch);		// merge byte into val
		asm.d.sub_r_i(R.RCX, 57);		// compute 67 - shift
		asm.d.neg_r(R.RCX);
		asm.jc_rel_near(C.S, done);		// if shift > 64, done
		asm.q.shl_r_cl(dest);			// sign extension
		asm.q.sar_r_cl(dest);
		asm.bind(done);
	}
	// Generate code which skips over an LEB.
	def genSkipLeb() {
		var more = X86_64Label.new();
		asm.bind(more);
		asm.movbzx_r_m(I.scratch, I.ip_ptr);	// load first byte
		asm.q.inc_r(I.ip);			// increment pointer
		asm.test_r_i(I.scratch, 0x80);		// test most-significant bit
		asm.jc_rel_near(C.NZ, more);
	}
	// Generate an inline dispatch or a jump to the dispatch loop, depending on config.
	def genDispatchOrJumpToDispatch() {
		if (tuning.threadedDispatch) genDispatch(dispatchTables[0]);
		else asm.jmp_rel(firstDispatchOffset - w.atEnd().pos);
	}
	// Generate a load of the next bytecode and a dispatch through the dispatch table.
	def genDispatch(prefix: int, ref: IcCodeRef) {
		var opcode = I.r0;
		var base = I.r1;
		asm.movbzx_r_m(opcode, I.ip_ptr);
		asm.inc_r(I.ip);
		asm.lea(base, ref); // RIP-relative LEA
		asm.movwsx_r_m(opcode, X86_64Addr.new(base, opcode, 2, 0)); // load 16-bit offset
		asm.add_r_r(base, opcode);
		if (dispatchJmpOffset < 0) dispatchJmpOffset = w.pos;
		asm.ijmp_r(base);
	}
	// Patch the dispatch table for the given opcode to go to the given position.
	def patchDispatchTable(opcode: Opcode, pos: int) {
		for (t in dispatchTables) {
			if (t.0 == opcode.prefix) {
				var ref = t.1;
				var offset = pos - ref.offset;
				w.at(ref.offset + 2 * opcode.code).put_b16(offset);
				w.atEnd();
				return;
			}
		}
		System.error("X86_64InterpreterGen", "no dispatch table found for prefix");
	}
	// Generate the out-of-line LEB decoding code.
	def genOutOfLineLEBs() {
		for (i < oolULeb32Sites.length) {
			var o = oolULeb32Sites[i];
			var pos = w.atEnd().pos;
			w.at(o.pos).put_b32(pos - (o.pos + o.delta));
			w.atEnd();
			// XXX: share code between out-of-line LEB cases
			// generate out-of-line code for > 1 byte LEB cases
			var dest = o.dest, destRcx = dest == R.RCX;
			asm.d.and_r_i(dest, 0x7F);		// mask off upper bit of first byte
			if (destRcx) {
				asm.movd_r_r(I.r3, dest);
				dest = I.r3;
			} else {
				asm.pushq_r(R.RCX);
			}
			asm.movd_r_i(R.RCX, 7);
			var loop = X86_64Label.new(), nomore = X86_64Label.new();
			asm.bind(loop);
			asm.movbzx_r_m(I.scratch, I.ip_ptr);	// load byte
			asm.q.inc_r(I.ip);			// increment pointer
			asm.d.test_r_i(I.scratch, 0x80);	// test most-significant bit
			asm.jc_rel_near(C.Z, nomore);		// break if not set
			asm.d.and_r_i(I.scratch, 0x7F);		// mask off upper bit
			asm.d.shl_r_cl(I.scratch);		// shift byte into correct bit pos
			asm.d.or_r_r(dest, I.scratch);		// merge byte into val
			asm.d.add_r_i(R.RCX, 7);		// compute next bit pos
			asm.jmp_rel_near(loop);			// loop

			asm.bind(nomore);
			asm.d.shl_r_cl(I.scratch);		// shift byte into correct bit pos
			asm.d.or_r_r(dest, I.scratch);		// merge byte into val
			if (destRcx) asm.movd_r_r(R.RCX, dest);
			else asm.popq_r(R.RCX);
			asm.jmp_rel(o.retOffset - w.atEnd().pos);
		}
		oolULeb32Sites = null;
	}
	// Generate trap handler code.
	def genTrapHandlers() {
		w.atEnd();
		ic.oobMemoryHandlerOffset = w.pos;
		genAbruptReturn(ExecState.TRAPPED, TrapReason.MEM_OUT_OF_BOUNDS);

		ic.divZeroHandlerOffset = w.pos;
		genAbruptReturn(ExecState.TRAPPED, TrapReason.DIV_BY_ZERO);

		ic.stackOverflowHandlerOffset = w.pos;
		genAbruptReturn(ExecState.TRAPPED, TrapReason.STACK_OVERFLOW);
	}
	def refCallback<P, R>(f: P -> R) -> IcCodeRef {
		if (callbackTableOffset >= callbackTableLimit) fatal("out of callback ref slots");
		var ptr = CiRuntime.unpackClosure<X86_64Interpreter, P, R>(f).0;
		var ref = IcCodeRef.new(callbackTableOffset);
		w.at(callbackTableOffset).put_b64(ptr - Pointer.NULL);
		callbackTableOffset += 8;
		w.atEnd();
		return ref;
	}
	def reportOom(w: DataWriter, nlength: int) -> DataWriter {
		System.error("InterpreterGenError", "ran out of buffer space");
		return w;
	}
}
type HandlerGenResult {
	case UNHANDLED;
	case HANDLED;
	case Offset(off: int);
	case END;
}

// Assembler patching support for out-of-line LEBs and other code refs.
def ABS_MARKER = 0x55443322;
def REL_MARKER = 0x44332211;
class OutOfLineLEB(dest: X86_64Gpr) extends X86_64Addr {
	var retOffset: int; // where OOB code should "return"
	var pos: int = -1;
	var delta: int;

	new() super(null, null, 1, REL_MARKER) { }
}
class IcCodeRef(var offset: int) extends X86_64Addr {
	new() super(null, null, 1, REL_MARKER) { }
}
class Patcher(w: DataWriter) extends X86_64AddrPatcher {
	new() super(ABS_MARKER, REL_MARKER) { }
	def recordRel32(pos: int, delta: int, addr: X86_64Addr) {
		match (addr) {
			x: OutOfLineLEB => {
				x.pos = pos;
				x.delta = delta;
			}
			x: IcCodeRef => {
				if (x.offset < 0) System.error("InterpreterGen", "unbound forward code ref");
				w.at(pos).put_b32(x.offset - (pos + delta));
				w.atEnd();
			}
		}
	}
}
// A utility that generates constants and picks appropriate instructions for rounding, data movement,
// and conversion in dealing with floating point truncations.
class FloatTrunc(isI64: bool, isF64: bool, isSigned: bool) {
	def round_s_s = if(isF64, X86_64Assembler.roundsd_s_s, X86_64Assembler.roundss_s_s);
	def sub_s_s = if(isF64, X86_64Assembler.subsd_s_s, X86_64Assembler.subss_s_s);
	def ucomi_s_s = if(isF64, X86_64Assembler.ucomisd_s_s, X86_64Assembler.ucomiss_s_s);
	def mov_s_r = if(isF64, X86_64Assembler.movq_s_r, X86_64Assembler.movd_s_r);
	def mov_s_m = if(isF64, X86_64Assembler.movsd_s_m, X86_64Assembler.movss_s_m);
	def mov_m_s = if(isF64, X86_64Assembler.movsd_m_s, X86_64Assembler.movss_m_s);
	def mov_m_r = if(isI64, X86_64Assembler.movq_m_r, X86_64Assembler.movd_m_r);
	def maxv: u64 = if(isI64,
				if(isSigned,
					if(isF64, Floats.d_1p63, Floats.f_1p63),
					if(isF64, Floats.d_1p64, Floats.f_1p64)),
				if(isSigned,
					if(isF64, Floats.d_1p31, Floats.f_1p31),
					if(isF64, Floats.d_1p32, Floats.f_1p32)));
	def minv: u64 = if(isI64, // XXX: share these constants with V3 interpreter
				if(isSigned,
					if(isF64, u64.view(-9.223372036854778E18d), u32.view(-9.223373e18f)),
					if(isF64, u64.view(-1d), u32.view(-1f))),
				if(isSigned,
					if(isF64, u64.view(-2147483649d), u32.view(-2.1474839E9f)),
					if(isF64, u64.view(-1d), u32.view(-1f))));

	def minus1: u64 = if(isF64, Floats.d_minus1, Floats.f_minus1);

	def ceilv: u64 = if(isI64,
				if(isSigned, u63.max, u64.max),
				if(isSigned, u31.max, u32.max));
	def floorv: u64 = if(isSigned,
				if(isI64, u64.view(i63.min), u64.view(i31.min)));

	def tag = if(isI64, BpTypecon.I64, BpTypecon.I32).code;

	def mov_s_i(asm: X86_64Assembler, s: X86_64Xmmr, v: u64) {
		if (isF64) {
			if ((v & u32.max) == 0) {
				asm.movd_r_i(I.scratch, int.view(v >> 32));
				asm.q.shl_r_i(I.scratch, 32);
				asm.movq_s_r(s, I.scratch);
			} else if (int.view(v) > 0) { // no sign extension
				// XXX: load float constants from memory
				asm.movd_r_i(I.scratch, int.view(v >> 32));
				asm.q.shl_r_i(I.scratch, 32);
				asm.q.or_r_i(I.scratch, int.view(v));
				asm.movq_s_r(s, I.scratch);
			} else {
				System.error("FloatTrunc", "tricky 64-bit constant unimplemented");
			}
		} else {
			asm.movd_r_i(I.scratch, int.view(v));
			asm.movd_s_r(s, I.scratch);
		}
	}
	def mov_r_i(asm: X86_64Assembler, r: X86_64Gpr, v: u64) {
		if (isI64) {
			if (i32.view(v) == i64.view(v)) {
				asm.movq_r_i(r, int.view(v));
			} else if (u32.view(v) == u64.view(v)) {
				asm.movd_r_i(r, int.view(v));
			} else {
				System.error("FloatTrunc", "tricky 64-bit constant unimplemented");
			}
		} else {
			asm.movd_r_i(r, int.view(v));
		}
	}
	def cvt2si_r_s = if(isF64, X86_64Assembler.cvtsd2si_r_s, X86_64Assembler.cvtss2si_r_s);
}
def TRUNC_i32_f32_s = FloatTrunc.new(false, false, true);
def TRUNC_i32_f32_u = FloatTrunc.new(false, false, false);
def TRUNC_i32_f64_s = FloatTrunc.new(false, true, true);
def TRUNC_i32_f64_u = FloatTrunc.new(false, true, false);
def TRUNC_i64_f32_s = FloatTrunc.new(true, false, true);
def TRUNC_i64_f32_u = FloatTrunc.new(true, false, false);
def TRUNC_i64_f64_s = FloatTrunc.new(true, true, true);
def TRUNC_i64_f64_u = FloatTrunc.new(true, true, false);
