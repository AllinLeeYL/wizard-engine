// Copyright 2022 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

def G = X86_64Regs2.toGpr, X = X86_64Regs2.toXmmr;
def R: X86_64Regs;
def C: X86_64Conds;
def A(ma: MasmAddr) -> X86_64Addr {
	return X86_64Addr.new(G(ma.base), null, 1, ma.offset);
}

class X86_64MasmLabel extends MasmLabel {
	def label: X86_64Label;

	new(create_pos: int, label) super(create_pos) {	}
}
class X86_64MacroAssembler extends MacroAssembler {
	def w: DataWriter;
	def asm = X86_64Assemblers.create64(w);
	var scratch: X86_64Gpr;
	var jump_tables: Vector<(int, Array<X86_64Label>)>;
	var offsets: V3Offsets;

	new(w, regAlloc: RegAlloc) super(Target.tagging, regAlloc) {
		scratch = G(regAlloc.regConfig.regs.scratch);
	}

	// Called after
	def prepareToCopyInto(asm: X86_64Assembler, addr: u64) {
		if (jump_tables != null) {
			for (i < jump_tables.length) {
				var t = jump_tables[i], offset = t.0, labels = t.1;
				for (j < labels.length) {
					var l = labels[j], target = addr + u64.!(l.pos);
					asm.w.at(offset + j * 8).put_b64(long.view(target));
				}
			}
			jump_tables = null;
		}
	}

	// Label operations
	def newLabel(create_pos: int) -> X86_64MasmLabel {
		return X86_64MasmLabel.new(create_pos, asm.newLabel());
	}
	def bindLabel(l: MasmLabel) {
		if (Trace.compiler) Trace.OUT.put2("    bind label (+%d) -> @%d", l.create_pos, w.end()).outln();
		var label = X86_64MasmLabel.!(l);
		label.offset = w.pos;
		asm.bind(label.label);
	}

	def emit_read_v3_array_r_r(dst: Reg, array: Reg, index: Reg, kind: ValueKind) {
		var a = G(array), i = G(index);
		match (kind) {
			I32 => asm.movd_r_m(G(dst), X86_64Addr.new(a, i, 4, getOffsets().Array_contents));
			F32 => asm.movss_s_m(X(dst), X86_64Addr.new(a, i, 4, getOffsets().Array_contents));
			I64, REF => asm.movq_r_m(G(dst), X86_64Addr.new(a, i, 8, getOffsets().Array_contents));
			F64 => asm.movsd_s_m(X(dst), X86_64Addr.new(a, i, 8, getOffsets().Array_contents));
			ABS, V128 => asm.movdqu_s_m(X(dst), X86_64Addr.new(a, i, 16, getOffsets().Array_contents)); // TODO: can't scale by 16
		}
	}
	def emit_bounds_check_v3_array(array: Reg, index: Reg, oob_label: MasmLabel) {
		asm.d.cmp_r_m(G(index), X86_64Addr.new(G(array), null, 1, getOffsets().Array_length));
		asm.jc_rel_far(X86_64Conds.GE, X86_64MasmLabel.!(oob_label).label);
	}
	def emit_read_v3_mem_base(dst: Reg, memobj: Reg) {
		asm.movq_r_m(G(dst), X86_64Addr.new(G(memobj), null, 1, getOffsets().X86_64Memory_start));
	}

	def emit_loadbsx_r_r_r_i(dst: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		var x = if (kind == ValueKind.I64, asm.q, asm.d).movbsx_r_m(G(dst), X86_64Addr.new(G(base), G(index), 1, int.view(offset)));
	}
	def emit_loadbzx_r_r_r_i(dst: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		var x = if (kind == ValueKind.I64, asm.q, asm.d).movbzx_r_m(G(dst), X86_64Addr.new(G(base), G(index), 1, int.view(offset)));
	}
	def emit_loadwsx_r_r_r_i(dst: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		var x = if (kind == ValueKind.I64, asm.q, asm.d).movwsx_r_m(G(dst), X86_64Addr.new(G(base), G(index), 1, int.view(offset)));
	}
	def emit_loadwzx_r_r_r_i(dst: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		var x = if (kind == ValueKind.I64, asm.q, asm.d).movwzx_r_m(G(dst), X86_64Addr.new(G(base), G(index), 1, int.view(offset)));
	}
	def emit_loaddsx_r_r_r_i(dst: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		var d = G(dst);
		asm.q.movd_r_m(d, X86_64Addr.new(G(base), G(index), 1, int.view(offset)));
		asm.q.shl_r_i(d, 32);
		asm.q.sar_r_i(d, 32);
	}
	def emit_loaddzx_r_r_r_i(dst: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		asm.q.movd_r_m(G(dst), X86_64Addr.new(G(base), G(index), 1, int.view(offset)));
	}
	def emit_load_r_r_r_i(dst: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		var b = G(base), i = G(index), o = int.view(offset);
		match (kind) {
			I32 => asm.movd_r_m(G(dst), X86_64Addr.new(b, i, 1, o));
			REF, I64 => asm.movq_r_m(G(dst), X86_64Addr.new(b, i, 1, o));
			F32 => asm.movss_s_m(X(dst), X86_64Addr.new(b, i, 1, o));
			F64 => asm.movsd_s_m(X(dst), X86_64Addr.new(b, i, 1, o));
			ABS, V128 => asm.movdqu_s_m(X(dst), X86_64Addr.new(b, i, 1, o));
		}
	}

	def emit_storeb_r_r_r_i(val: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		asm.q.movb_m_r(X86_64Addr.new(G(base), G(index), 1, int.view(offset)), G(val));
	}
	def emit_storew_r_r_r_i(val: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		asm.q.movw_m_r(X86_64Addr.new(G(base), G(index), 1, int.view(offset)), G(val));
	}
	def emit_store_r_r_r_i(val: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		var b = G(base), i = G(index), o = int.view(offset);
		match (kind) {
			I32 => asm.movd_m_r(X86_64Addr.new(b, i, 1, o), G(val));
			REF, I64 => asm.movq_m_r(X86_64Addr.new(b, i, 1, o), G(val));
			F32 => asm.movss_m_s(X86_64Addr.new(b, i, 1, o), X(val));
			F64 => asm.movsd_m_s(X86_64Addr.new(b, i, 1, o), X(val));
			ABS, V128 => asm.movdqu_m_s(X86_64Addr.new(b, i, 1, o), X(val));
		}
	}

	def emit_mov_r_r(reg: Reg, reg2: Reg) {
		var rd = G(reg);
		if (rd != null) asm.movq_r_r(rd, G(reg2));
		else asm.movsd_s_s(X(reg), X(reg2)); // TODO: v128
	}
	def emit_mov_r_m(reg: Reg, kind: ValueKind, ma: MasmAddr) {
		var addr = A(ma);
		match (kind) {
			I32 => asm.movd_r_m(G(reg), addr);
			I64, REF => asm.movq_r_m(G(reg), addr);
			F32 => asm.movss_s_m(X(reg), addr);
			F64 => asm.movsd_s_m(X(reg), addr);
			ABS, V128 => asm.movdqu_s_m(X(reg), addr);
		}
	}
	def emit_mov_r_i(reg: Reg, val: int) {
		asm.movd_r_i(G(reg), val);
	}
	def emit_mov_r_trap(reg: Reg, reason: TrapReason) {
		var ptr = Pointer.atObject(Execute.trapObjects[reason.tag]);
		var val = int.view(u32.!(ptr - Pointer.NULL));
		asm.movd_r_i(G(reg), val);
	}

	def emit_mov_m_r(ma: MasmAddr, reg: Reg, kind: ValueKind) {
		var addr = A(ma);
		match (kind) {
			I32 => asm.movd_m_r(addr, G(reg));
			I64, REF => asm.movq_m_r(addr, G(reg));
			F32 => asm.movss_m_s(addr, X(reg));
			F64 => asm.movsd_m_s(addr, X(reg));
			ABS, V128 => asm.movdqu_m_s(addr, X(reg));
		}
	}
	def emit_mov_m_i(ma: MasmAddr, val: int) {
		asm.movd_m_i(A(ma), val);
	}
	def emit_mov_m_l(ma: MasmAddr, val: long) {
		var addr = A(ma);
		if (val == int.view(val)) asm.movq_m_i(addr, int.view(val));
		else {  // XXX: use constant pool?
			asm.movd_m_i(addr, int.view(val));
			var p4 = X86_64Addr.new(addr.base, addr.index, addr.scale, addr.disp + 4);
			asm.movd_m_i(p4, int.view(val >> 32));
		}
	}
	def emit_mov_m_f(ma: MasmAddr, bits: u32) {
		asm.movd_m_i(A(ma), int.view(bits));
	}
	def emit_mov_m_d(ma: MasmAddr, bits: u64) {
		emit_mov_m_l(ma, long.view(bits));
	}
	def emit_mov_m_q(ma: MasmAddr, low: u64, high: u64) {  // XXX: use constant pool?
		emit_mov_m_l(ma, long.view(low));
		emit_mov_m_l(MasmAddr(ma.base, ma.offset + 8), long.view(high));
	}
	def emit_mov_m_m(dst: MasmAddr, src: MasmAddr, kind: ValueKind) {
		match (kind) {
			I32, F32 => {
				asm.movd_r_m(scratch, A(src));
				asm.movd_m_r(A(dst), scratch);
			}
			I64, F64, REF => {
				asm.movq_r_m(scratch, A(src));
				asm.movq_m_r(A(dst), scratch);
			}
			ABS, V128 => {
				var scratch = R.XMM15; // TODO
				asm.movdqu_s_m(scratch, A(src));
				asm.movdqu_m_s(A(dst), scratch);
			}
		}
	}

	def emit_addi_r_r(reg: Reg, reg2: Reg) {
		asm.add_r_r(G(reg), X86_64Regs2.toGpr(reg2));
	}
	def emit_addi_r_i(reg: Reg, val: int) {
		asm.add_r_i(G(reg), val);
	}

	def emit_unop_r(op: Opcode, reg: Reg) {
		var r1 = G(reg);
		match (op) {
			I32_CLZ => {
				asm.movd_r_i(scratch, -1);
				asm.d.bsr_r_r(r1, r1);
				asm.d.cmov_r(C.Z, r1, scratch);
				asm.movd_r_i(scratch, 31);
				asm.d.sub_r_r(scratch, r1);
				asm.movd_r_r(r1, scratch); // XXX: can save an instruction with second output reg
			}
			I32_CTZ => {
				asm.d.bsf_r_r(r1, r1);
				asm.movd_r_i(scratch, 32);
				asm.d.cmov_r(C.Z, r1, scratch);
			}
			I32_POPCNT => asm.d.popcnt_r_r(r1, r1);
			I32_EQZ => emit_cmpd_r_i(C.Z, r1, 0);
			I64_CLZ => {
				asm.movq_r_i(scratch, -1);
				asm.q.bsr_r_r(r1, r1);
				asm.q.cmov_r(C.Z, r1, scratch);
				asm.movq_r_i(scratch, 63);
				asm.q.sub_r_r(scratch, r1);
				asm.movq_r_r(r1, scratch); // XXX: can save an instruction with second output reg
			}
			I64_CTZ =>  {
				asm.q.bsf_r_r(r1, r1);
				asm.movq_r_i(scratch, 64);
				asm.q.cmov_r(C.Z, r1, scratch);
			}
			I32_WRAP_I64 => asm.movd_r_r(r1, r1);
			I64_POPCNT => asm.q.popcnt_r_r(r1, r1);
			REF_IS_NULL,
			I64_EQZ => emit_cmpq_r_i(C.Z, r1, 0);
			I64_EXTEND_I32_U => asm.movd_r_r(r1, r1);
			I32_EXTEND8_S => asm.d.movbsx_r_r(r1, r1);
			I32_EXTEND16_S => asm.d.movwsx_r_r(r1, r1);
			I64_EXTEND8_S => asm.q.movbsx_r_r(r1, r1);
			I64_EXTEND16_S => asm.q.movwsx_r_r(r1, r1);
			I64_EXTEND_I32_S, I64_EXTEND32_S => {
				asm.q.shl_r_i(r1, 32);
				asm.q.sar_r_i(r1, 32);
			}
			_ => unimplemented();
		}
	}

	def emit_binop_r_r(op: Opcode, reg: Reg, reg2: Reg) {
		var r1 = G(reg), r2 = G(reg2);
		match (op) {
			// i32 r_r compares
			I32_EQ => emit_cmpd_r_r(C.Z, r1, r2);
			I32_NE => emit_cmpd_r_r(C.NZ, r1, r2);
			I32_LT_S => emit_cmpd_r_r(C.L, r1, r2);
			I32_LT_U => emit_cmpd_r_r(C.C, r1, r2);
			I32_GT_S => emit_cmpd_r_r(C.G, r1, r2);
			I32_GT_U => emit_cmpd_r_r(C.A, r1, r2);
			I32_LE_S => emit_cmpd_r_r(C.LE, r1, r2);
			I32_LE_U => emit_cmpd_r_r(C.NA, r1, r2);
			I32_GE_S => emit_cmpd_r_r(C.GE, r1, r2);
			I32_GE_U => emit_cmpd_r_r(C.NC, r1, r2);
			// i64 r_r compares
			REF_EQ,
			I64_EQ => emit_cmpq_r_r(C.Z, r1, r2);
			I64_NE => emit_cmpq_r_r(C.NZ, r1, r2);
			I64_LT_S => emit_cmpq_r_r(C.L, r1, r2);
			I64_LT_U => emit_cmpq_r_r(C.C, r1, r2);
			I64_GT_S => emit_cmpq_r_r(C.G, r1, r2);
			I64_GT_U => emit_cmpq_r_r(C.A, r1, r2);
			I64_LE_S => emit_cmpq_r_r(C.LE, r1, r2);
			I64_LE_U => emit_cmpq_r_r(C.NA, r1, r2);
			I64_GE_S => emit_cmpq_r_r(C.GE, r1, r2);
			I64_GE_U => emit_cmpq_r_r(C.NC, r1, r2);
			// i32 r_r binops
			I32_ADD => asm.d.add_r_r(r1, r2);
			I32_SUB => asm.d.sub_r_r(r1, r2);
			I32_MUL => asm.d.imul_r_r(r1, r2);
			I32_DIV_S => {
				var div = X86_64Label.new(), done = X86_64Label.new();
				asm.d.cmp_r_i(r2, -1);
				asm.jc_rel_near(C.NZ, div);
				asm.d.cmp_r_i(r1, 0x80000000);
				asm.jc_rel_far(C.Z, X86_64MasmLabel.!(newTrapLabel(TrapReason.DIV_UNREPRESENTABLE)).label);
				asm.movd_r_r(r1, r2);
				asm.d.neg_r(r1);
				asm.jmp_rel_near(done);
				asm.bind(div);
				var saveRAX = !regAlloc.isFree(X86_64Regs2.RAX);
				var saveRDX = !regAlloc.isFree(X86_64Regs2.RDX);
				var divisor = r2;
				if (r2 == R.RAX) {
					saveRAX = false;
					asm.movd_r_r(scratch, r2);
					divisor = scratch;
				} else if (r2 == R.RDX) {
					saveRDX = false;
					asm.movd_r_r(scratch, r2);
					divisor = scratch;
				}
				if (r1 != R.RAX) {
					if (saveRAX) asm.movq_m_r(R.RSP.plus(scratchStackSlot1), R.RAX);
					if (r1 == R.RDX) saveRDX = false;
					asm.movd_r_r(R.RAX, r1);
				}
				if (saveRDX) {
					asm.movq_m_r(R.RSP.plus(scratchStackSlot2), R.RDX);
				}
				asm.d.cdq();
				asm.d.idiv_r(divisor);
				if (r1 != R.RAX) {
					asm.movd_r_r(r1, R.RAX);
					if (saveRAX) asm.movq_r_m(R.RAX, R.RSP.plus(scratchStackSlot1));
				}
				if (saveRDX) {
					asm.movq_r_m(R.RDX, R.RSP.plus(scratchStackSlot2));
				}
				asm.bind(done);
			}
			I32_DIV_U => {
				var saveRAX = !regAlloc.isFree(X86_64Regs2.RAX);
				var saveRDX = !regAlloc.isFree(X86_64Regs2.RDX);
				var divisor = r2;
				if (r2 == R.RAX) {
					saveRAX = false;
					asm.movd_r_r(scratch, r2);
					divisor = scratch;
				} else if (r2 == R.RDX) {
					saveRDX = false;
					asm.movd_r_r(scratch, r2);
					divisor = scratch;
				}
				if (r1 != R.RAX) {
					if (saveRAX) asm.movq_m_r(R.RSP.plus(scratchStackSlot1), R.RAX);
					if (r1 == R.RDX) saveRDX = false;
					asm.movd_r_r(R.RAX, r1);
				}
				if (saveRDX) {
					asm.movq_m_r(R.RSP.plus(scratchStackSlot2), R.RDX);
				}
				asm.d.movd_r_i(R.RDX, 0);
				asm.d.div_r(divisor);
				if (r1 != R.RAX) {
					asm.movd_r_r(r1, R.RAX);
					if (saveRAX) asm.movq_r_m(R.RAX, R.RSP.plus(scratchStackSlot1));
				}
				if (saveRDX) {
					asm.movq_r_m(R.RDX, R.RSP.plus(scratchStackSlot2));
				}
			}
			I32_REM_S,
			I32_REM_U => unimplemented();
			I32_AND => asm.d.and_r_r(r1, r2);
			I32_OR  => asm.d.or_r_r(r1, r2);
			I32_XOR => asm.d.xor_r_r(r1, r2);
			I32_SHL => emit_shift_r(asm.d.shl_r_cl, r1, r2);
			I32_SHR_S => emit_shift_r(asm.d.sar_r_cl, r1, r2);
			I32_SHR_U => emit_shift_r(asm.d.shr_r_cl, r1, r2);
			I32_ROTL => emit_shift_r(asm.d.rol_r_cl, r1, r2);
			I32_ROTR => emit_shift_r(asm.d.ror_r_cl, r1, r2);
			// i64 r_r binops
			I64_ADD => asm.q.add_r_r(r1, r2);
			I64_SUB => asm.q.sub_r_r(r1, r2);
			I64_MUL => asm.q.imul_r_r(r1, r2);
			I64_DIV_S,
			I64_DIV_U,
			I64_REM_S,
			I64_REM_U => unimplemented();
			I64_AND => asm.q.and_r_r(r1, r2);
			I64_OR  => asm.q.or_r_r(r1, r2);
			I64_XOR => asm.q.xor_r_r(r1, r2);
			I64_SHL => emit_shift_r(asm.q.shl_r_cl, r1, r2);
			I64_SHR_S => emit_shift_r(asm.q.sar_r_cl, r1, r2);
			I64_SHR_U => emit_shift_r(asm.q.shr_r_cl, r1, r2);
			I64_ROTL => emit_shift_r(asm.q.rol_r_cl, r1, r2);
			I64_ROTR => emit_shift_r(asm.q.ror_r_cl, r1, r2);
			_ => unimplemented();
		}
	}
	private def emit_shift_r(f: X86_64Gpr -> X86_64Assembler, r1: X86_64Gpr, r2: X86_64Gpr) {
		if (r2 == R.RCX) {
			// shift r1, rcx
			f(r1);
		} else {
			if (regAlloc.isFree(X86_64Regs2.RCX)) {
				// move shiftor into rcx
				asm.movd_r_r(R.RCX, r2);
				f(r1);
			} else if (r1 == R.RCX) {
				asm.movq_r_r(scratch, r1);
				asm.movd_r_r(R.RCX, r2);
				f(scratch);
				asm.movq_r_r(r1, scratch);
			} else {
				// save rcx before moving shiftor into rcx, then restore
				asm.movq_r_r(scratch, R.RCX);
				asm.movd_r_r(R.RCX, r2);
				f(r1);
				asm.movq_r_r(R.RCX, scratch);
			}
		}
	}
	def emit_binop_r_m(op: Opcode, reg: Reg, ma: MasmAddr) {
		var r1 = G(reg), addr = A(ma);
		match (op) {
			// i32 r_r compares
			I32_EQ => emit_cmpd_r_m(C.Z, r1, addr);
			I32_NE => emit_cmpd_r_m(C.NZ, r1, addr);
			I32_LT_S => emit_cmpd_r_m(C.L, r1, addr);
			I32_LT_U => emit_cmpd_r_m(C.C, r1, addr);
			I32_GT_S => emit_cmpd_r_m(C.G, r1, addr);
			I32_GT_U => emit_cmpd_r_m(C.A, r1, addr);
			I32_LE_S => emit_cmpd_r_m(C.LE, r1, addr);
			I32_LE_U => emit_cmpd_r_m(C.NA, r1, addr);
			I32_GE_S => emit_cmpd_r_m(C.GE, r1, addr);
			I32_GE_U => emit_cmpd_r_m(C.NC, r1, addr);
			// i64 r_r compares
			I64_EQ => emit_cmpq_r_m(C.Z, r1, addr);
			I64_NE => emit_cmpq_r_m(C.NZ, r1, addr);
			I64_LT_S => emit_cmpq_r_m(C.L, r1, addr);
			I64_LT_U => emit_cmpq_r_m(C.C, r1, addr);
			I64_GT_S => emit_cmpq_r_m(C.G, r1, addr);
			I64_GT_U => emit_cmpq_r_m(C.A, r1, addr);
			I64_LE_S => emit_cmpq_r_m(C.LE, r1, addr);
			I64_LE_U => emit_cmpq_r_m(C.NA, r1, addr);
			I64_GE_S => emit_cmpq_r_m(C.GE, r1, addr);
			I64_GE_U => emit_cmpq_r_m(C.NC, r1, addr);
			// i32 r_m binops
			I32_ADD => asm.d.add_r_m(r1, addr);
			I32_SUB => asm.d.sub_r_m(r1, addr);
			I32_MUL => asm.d.imul_r_m(r1, addr);
			I32_DIV_S,
			I32_DIV_U,
			I32_REM_S,
			I32_REM_U => unimplemented();
			I32_AND => asm.d.and_r_m(r1, addr);
			I32_OR  => asm.d.or_r_m(r1, addr);
			I32_XOR => asm.d.xor_r_m(r1, addr);
			I32_SHL => emit_shift_m(asm.d.shl_r_cl, r1, addr, false);
			I32_SHR_S => emit_shift_m(asm.d.sar_r_cl, r1, addr, false);
			I32_SHR_U => emit_shift_m(asm.d.shr_r_cl, r1, addr, false);
			I32_ROTL => emit_shift_m(asm.d.rol_r_cl, r1, addr, false);
			I32_ROTR => emit_shift_m(asm.d.ror_r_cl, r1, addr, false);
			// i64 r_m binops
			I64_ADD => asm.q.add_r_m(r1, addr);
			I64_SUB => asm.q.sub_r_m(r1, addr);
			I64_MUL => asm.q.imul_r_m(r1, addr);
			I64_DIV_S,
			I64_DIV_U,
			I64_REM_S,
			I64_REM_U => unimplemented();
			I64_AND => asm.q.and_r_m(r1, addr);
			I64_OR  => asm.q.or_r_m(r1, addr);
			I64_XOR => asm.q.xor_r_m(r1, addr);
			I64_SHL => emit_shift_m(asm.q.shl_r_cl, r1, addr, true);
			I64_SHR_S => emit_shift_m(asm.q.sar_r_cl, r1, addr, true);
			I64_SHR_U => emit_shift_m(asm.q.shr_r_cl, r1, addr, true);
			I64_ROTL => emit_shift_m(asm.q.rol_r_cl, r1, addr, true);
			I64_ROTR => emit_shift_m(asm.q.ror_r_cl, r1, addr, true);
			_ => unimplemented();
		}
	}
	private def emit_shift_m(f: X86_64Gpr -> X86_64Assembler, r1: X86_64Gpr, addr: X86_64Addr, is64: bool) {
		if (r1 == R.RCX) {
			asm.movq_r_r(scratch, r1);
			if (is64) asm.movq_r_m(R.RCX, addr);
			else asm.movd_r_m(R.RCX, addr);
			f(scratch);
			asm.movq_r_r(r1, scratch);
		} else {
			var save = !regAlloc.isFree(X86_64Regs2.RCX);
			if (save) asm.movq_r_r(scratch, R.RCX);
			if (is64) asm.movq_r_m(R.RCX, addr);
			else asm.movd_r_m(R.RCX, addr);
			f(r1);
			if (save) asm.movq_r_r(R.RCX, scratch);
		}
	}
	def emit_binop_r_i(op: Opcode, reg: Reg, val: int) {
		var r1 = G(reg);
		match (op) {
			// i32 r_r compares
			I32_EQ => emit_cmpd_r_i(C.Z, r1, val);
			I32_NE => emit_cmpd_r_i(C.NZ, r1, val);
			I32_LT_S => emit_cmpd_r_i(C.L, r1, val);
			I32_LT_U => emit_cmpd_r_i(C.C, r1, val);
			I32_GT_S => emit_cmpd_r_i(C.G, r1, val);
			I32_GT_U => emit_cmpd_r_i(C.A, r1, val);
			I32_LE_S => emit_cmpd_r_i(C.LE, r1, val);
			I32_LE_U => emit_cmpd_r_i(C.NA, r1, val);
			I32_GE_S => emit_cmpd_r_i(C.GE, r1, val);
			I32_GE_U => emit_cmpd_r_i(C.NC, r1, val);
			// i64 r_r compares
			I64_EQ => emit_cmpq_r_i(C.Z, r1, val);
			I64_NE => emit_cmpq_r_i(C.NZ, r1, val);
			I64_LT_S => emit_cmpq_r_i(C.L, r1, val);
			I64_LT_U => emit_cmpq_r_i(C.C, r1, val);
			I64_GT_S => emit_cmpq_r_i(C.G, r1, val);
			I64_GT_U => emit_cmpq_r_i(C.A, r1, val);
			I64_LE_S => emit_cmpq_r_i(C.LE, r1, val);
			I64_LE_U => emit_cmpq_r_i(C.NA, r1, val);
			I64_GE_S => emit_cmpq_r_i(C.GE, r1, val);
			I64_GE_U => emit_cmpq_r_i(C.NC, r1, val);
			// i32 r_m binops
			I32_ADD => asm.d.add_r_i(r1, val);
			I32_SUB => asm.d.sub_r_i(r1, val);
			I32_MUL => asm.d.imul_r_i(r1, val);
			I32_DIV_S,
			I32_DIV_U,
			I32_REM_S,
			I32_REM_U => unimplemented();
			I32_AND => asm.d.and_r_i(r1, val);
			I32_OR  => asm.d.or_r_i(r1, val);
			I32_XOR => asm.d.xor_r_i(r1, val);
			I32_SHL => asm.d.shl_r_i(r1, u5.view(val));
			I32_SHR_S => asm.d.sar_r_i(r1, u5.view(val));
			I32_SHR_U => asm.d.shl_r_i(r1, u5.view(val));
			I32_ROTL => asm.d.rol_r_i(r1, u5.view(val));
			I32_ROTR => asm.d.ror_r_i(r1, u5.view(val));
			// i64 r_m binops
			I64_ADD => asm.q.add_r_i(r1, val);
			I64_SUB => asm.q.sub_r_i(r1, val);
			I64_MUL => asm.q.imul_r_i(r1, val);
			I64_DIV_S,
			I64_DIV_U,
			I64_REM_S,
			I64_REM_U => unimplemented();
			I64_AND => asm.q.and_r_i(r1, val);
			I64_OR  => asm.q.or_r_i(r1, val);
			I64_XOR => asm.q.xor_r_i(r1, val);
			I64_SHL => asm.q.shl_r_i(r1, u6.view(val));
			I64_SHR_S => asm.q.sar_r_i(r1, u6.view(val));
			I64_SHR_U => asm.q.shl_r_i(r1, u6.view(val));
			I64_ROTL => asm.q.rol_r_i(r1, u6.view(val));
			I64_ROTR => asm.q.rol_r_i(r1, u6.view(val));

			_ => unimplemented();
		}
	}
	def emit_cmpd_r_i(cond: X86_64Cond, r1: X86_64Gpr, val: int) {
		asm.d.cmp_r_i(r1, val);
		asm.set_r(cond, r1);
		asm.d.movbzx_r_r(r1, r1);
	}
	def emit_cmpq_r_i(cond: X86_64Cond, r1: X86_64Gpr, val: int) {
		asm.q.cmp_r_i(r1, val);
		asm.set_r(cond, r1);
		asm.q.movbzx_r_r(r1, r1);
	}
	def emit_cmpd_r_r(cond: X86_64Cond, r1: X86_64Gpr, r2: X86_64Gpr) {
		asm.d.cmp_r_r(r1, r2);
		asm.set_r(cond, r1);
		asm.d.movbzx_r_r(r1, r1);
	}
	def emit_cmpq_r_r(cond: X86_64Cond, r1: X86_64Gpr, r2: X86_64Gpr) {
		asm.q.cmp_r_r(r1, r2);
		asm.set_r(cond, r1);
		asm.q.movbzx_r_r(r1, r1);
	}
	def emit_cmpd_r_m(cond: X86_64Cond, r1: X86_64Gpr, addr: X86_64Addr) {
		asm.d.cmp_r_m(r1, addr);
		asm.set_r(cond, r1);
		asm.d.movbzx_r_r(r1, r1);
	}
	def emit_cmpq_r_m(cond: X86_64Cond, r1: X86_64Gpr, addr: X86_64Addr) {
		asm.q.cmp_r_m(r1, addr);
		asm.set_r(cond, r1);
		asm.q.movbzx_r_r(r1, r1);
	}
	def emit_ret() {
		asm.ret();
	}
	def emit_nop() {
		asm.q.or_r_r(R.RAX, R.RAX);
	}

	def emit_br(label: MasmLabel) {
		asm.jmp_rel_far(X86_64MasmLabel.!(label).label);
	}
	def emit_br_r(reg: Reg, cond: MasmBrCond, label: MasmLabel) {
		(if(cond.i32, asm.d, asm.q)).cmp_r_i(G(reg), 0);
		var cond = if(cond.zero, X86_64Conds.Z, X86_64Conds.NZ);
		asm.jc_rel_far(cond, X86_64MasmLabel.!(label).label);
	}
	def emit_br_m(addr: MasmAddr, cond: MasmBrCond, label: MasmLabel) {
		(if(cond.i32, asm.d, asm.q)).cmp_m_i(A(addr), 0);
		var cond = if(cond.zero, X86_64Conds.Z, X86_64Conds.NZ);
		asm.jc_rel_far(cond, X86_64MasmLabel.!(label).label);
	}
	def emit_breq_r_i(r: Reg, val: int, label: MasmLabel) {
		asm.d.cmp_r_i(G(r), val);
		asm.jc_rel_far(X86_64Conds.Z, X86_64MasmLabel.!(label).label);
	}
	def emit_breq_r_l(r: Reg, val: int, label: MasmLabel) {
		asm.q.cmp_r_i(G(r), val);
		asm.jc_rel_far(X86_64Conds.Z, X86_64MasmLabel.!(label).label);
	}
	def emit_brne_r_i(r: Reg, val: int, label: MasmLabel) {
		asm.d.cmp_r_i(G(r), val);
		asm.jc_rel_far(X86_64Conds.NZ, X86_64MasmLabel.!(label).label);
	}
	def emit_br_table_r(reg: Reg, labels: Array<MasmLabel>) {
		// XXX: simplify the label patching logic by improving X86_64Assembler
		var r1 = G(reg);
		asm.d.cmp_r_i(r1, labels.length);
		asm.jc_rel_far(C.NC, X86_64MasmLabel.!(labels[labels.length - 1]).label);
		var patcher = X86_64MasmJumpTablePatcher.new();
		asm.q.patcher = patcher;
		asm.q.lea(scratch, X86_64Addr.new(null, null, 1, REL_MARKER));
		asm.q.patcher = null;
		asm.ijmp_m(X86_64Addr.new(scratch, r1, 8, 0));
		w.align(8);
		var jtpos = w.atEnd().pos;
		if (jump_tables == null) jump_tables = Vector.new();
		jump_tables.put(jtpos, Arrays.map(labels, getLabel));
		w.skipN(labels.length * 8);
		w.at(patcher.pos).put_b32(jtpos - (patcher.pos + patcher.delta));
		w.atEnd();
	}
	def emit_call_r(reg: Reg) {
		asm.icall_r(G(reg));
	}
	def emit_call_runtime_callHost(func_arg: Reg) {
		emit_call_runtime(X86_64Interpreter.runtime_callHost);
	}
	def emit_call_runtime_GLOBAL_GET() {
		emit_call_runtime(X86_64Interpreter.runtime_GLOBAL_GET);
	}
	def emit_call_runtime_GLOBAL_SET() {
		emit_call_runtime(X86_64Interpreter.runtime_GLOBAL_SET);
	}
	private def emit_call_runtime<P, R>(closure: P -> R) {
		var ptr = CiRuntime.unpackClosure<X86_64Interpreter, P, R>(closure).0;
		// Do an absolute call into the runtime
		asm.movd_r_i(scratch, int.view(u32.!(ptr - Pointer.NULL))); // XXX: make direct call to runtime if within 2GB
		asm.icall_r(scratch);
	}
	def emit_write_runtime_vsp(vsp: Reg) {
		var offsets = getOffsets();
		asm.movq_r_m(scratch, absPointer(offsets.Interpreter_valueStack));
		asm.movq_m_r(scratch.plus(offsets.ValueStack_sp), G(vsp));
	}
	def getLabel(m: MasmLabel) -> X86_64Label {
		return X86_64MasmLabel.!(m).label;
	}
	def absPointer(ptr: Pointer) -> X86_64Addr {
		return X86_64Addr.new(null, null, 1, int.view(u32.!(ptr - Pointer.NULL)));
	}
	def getOffsets() -> V3Offsets {
		if (offsets == null) offsets = V3Offsets.new();
		return offsets;
	}
}

// XXX: Simplify relative loads for jump table by improving X86_64Assembler
def ABS_MARKER = 0x77665544;
def REL_MARKER = 0x99887766;
class X86_64MasmJumpTablePatcher extends X86_64AddrPatcher {
	var pos: int;
	var delta: int;
	new() super(ABS_MARKER, REL_MARKER) { }
	def recordRel32(pos: int, delta: int, addr: X86_64Addr) {
		this.pos = pos;
		this.delta = delta;
	}
}
