// Copyright 2022 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

def G = X86_64Regs2.toGpr, X = X86_64Regs2.toXmmr;
def R: X86_64Regs;
def C: X86_64Conds;
def A(ma: MasmAddr) -> X86_64Addr {
	return X86_64Addr.new(G(ma.base), null, 1, ma.offset);
}

class X86_64MasmLabel extends MasmLabel {
	def label: X86_64Label;

	new(create_pos: int, label) super(create_pos) {	}
}
class X86_64MacroAssembler extends MacroAssembler {
	def w: DataWriter;
	def asm = X86_64Assemblers.create64(w);
	var scratch: X86_64Gpr;
	var jump_tables: Vector<(int, Array<X86_64Label>)>;
	var offsets: V3Offsets;

	new(w, regAlloc: RegAlloc) super(Target.tagging, regAlloc) {
		scratch = G(regAlloc.regConfig.regs.scratch);
	}

	// Called after
	def prepareToCopyInto(asm: X86_64Assembler, addr: u64) {
		if (jump_tables != null) {
			for (i < jump_tables.length) {
				var t = jump_tables[i], offset = t.0, labels = t.1;
				for (j < labels.length) {
					var l = labels[j], target = addr + u64.!(l.pos);
					asm.w.at(offset + j * 8).put_b64(long.view(target));
				}
			}
			jump_tables = null;
		}
	}

	// Label operations
	def newLabel(create_pos: int) -> X86_64MasmLabel {
		return X86_64MasmLabel.new(create_pos, asm.newLabel());
	}
	def bindLabel(l: MasmLabel) {
		if (Trace.compiler) Trace.OUT.put2("    bind label (+%d) -> @%d", l.create_pos, w.end()).outln();
		var label = X86_64MasmLabel.!(l);
		label.offset = w.pos;
		asm.bind(label.label);
	}

	def emit_read_v3_array_r_r(dst: Reg, array: Reg, index: Reg, kind: ValueKind) {
		var a = G(array), i = G(index);
		match (kind) {
			I32 => asm.movd_r_m(G(dst), X86_64Addr.new(a, i, 4, getOffsets().Array_contents));
			F32 => asm.movss_s_m(X(dst), X86_64Addr.new(a, i, 4, getOffsets().Array_contents));
			I64, REF => asm.movq_r_m(G(dst), X86_64Addr.new(a, i, 8, getOffsets().Array_contents));
			F64 => asm.movsd_s_m(X(dst), X86_64Addr.new(a, i, 8, getOffsets().Array_contents));
			ABS, V128 => asm.movdqu_s_m(X(dst), X86_64Addr.new(a, i, 16, getOffsets().Array_contents)); // TODO: can't scale by 16
		}
	}
	def emit_bounds_check_v3_array(array: Reg, index: Reg, oob_label: MasmLabel) {
		asm.d.cmp_r_m(G(index), X86_64Addr.new(G(array), null, 1, getOffsets().Array_length));
		asm.jc_rel_far(X86_64Conds.GE, X86_64MasmLabel.!(oob_label).label);
	}
	def emit_read_v3_mem_base(dst: Reg, memobj: Reg) {
		asm.movq_r_m(G(dst), X86_64Addr.new(G(memobj), null, 1, getOffsets().X86_64Memory_start));
	}

	def emit_loadbsx_r_r_r_i(dst: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		var x = if (kind == ValueKind.I64, asm.q, asm.d).movbsx_r_m(G(dst), X86_64Addr.new(G(base), G(index), 1, int.view(offset)));
	}
	def emit_loadbzx_r_r_r_i(dst: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		var x = if (kind == ValueKind.I64, asm.q, asm.d).movbzx_r_m(G(dst), X86_64Addr.new(G(base), G(index), 1, int.view(offset)));
	}
	def emit_loadwsx_r_r_r_i(dst: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		var x = if (kind == ValueKind.I64, asm.q, asm.d).movwsx_r_m(G(dst), X86_64Addr.new(G(base), G(index), 1, int.view(offset)));
	}
	def emit_loadwzx_r_r_r_i(dst: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		var x = if (kind == ValueKind.I64, asm.q, asm.d).movwzx_r_m(G(dst), X86_64Addr.new(G(base), G(index), 1, int.view(offset)));
	}
	def emit_loaddsx_r_r_r_i(dst: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		var d = G(dst);
		asm.q.movd_r_m(d, X86_64Addr.new(G(base), G(index), 1, int.view(offset)));
		asm.q.shl_r_i(d, 32);
		asm.q.sar_r_i(d, 32);
	}
	def emit_loaddzx_r_r_r_i(dst: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		asm.q.movd_r_m(G(dst), X86_64Addr.new(G(base), G(index), 1, int.view(offset)));
	}
	def emit_load_r_r_r_i(dst: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		var b = G(base), i = G(index), o = int.view(offset);
		match (kind) {
			I32 => asm.movd_r_m(G(dst), X86_64Addr.new(b, i, 1, o));
			REF, I64 => asm.movq_r_m(G(dst), X86_64Addr.new(b, i, 1, o));
			F32 => asm.movss_s_m(X(dst), X86_64Addr.new(b, i, 1, o));
			F64 => asm.movsd_s_m(X(dst), X86_64Addr.new(b, i, 1, o));
			ABS, V128 => asm.movdqu_s_m(X(dst), X86_64Addr.new(b, i, 1, o));
		}
	}

	def emit_storeb_r_r_r_i(val: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		asm.q.movb_m_r(X86_64Addr.new(G(base), G(index), 1, int.view(offset)), G(val));
	}
	def emit_storew_r_r_r_i(val: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		asm.q.movw_m_r(X86_64Addr.new(G(base), G(index), 1, int.view(offset)), G(val));
	}
	def emit_store_r_r_r_i(val: Reg, base: Reg, index: Reg, offset: u32, kind: ValueKind) {
		var b = G(base), i = G(index), o = int.view(offset);
		match (kind) {
			I32 => asm.movd_m_r(X86_64Addr.new(b, i, 1, o), G(val));
			REF, I64 => asm.movq_m_r(X86_64Addr.new(b, i, 1, o), G(val));
			F32 => asm.movss_m_s(X86_64Addr.new(b, i, 1, o), X(val));
			F64 => asm.movsd_m_s(X86_64Addr.new(b, i, 1, o), X(val));
			ABS, V128 => asm.movdqu_m_s(X86_64Addr.new(b, i, 1, o), X(val));
		}
	}

	def emit_mov_r_r(reg: Reg, reg2: Reg) {
		var rd = G(reg);
		if (rd != null) asm.movq_r_r(rd, G(reg2));
		else asm.movsd_s_s(X(reg), X(reg2)); // TODO: v128
	}
	def emit_mov_r_m(reg: Reg, kind: ValueKind, ma: MasmAddr) {
		var addr = A(ma);
		match (kind) {
			I32 => asm.movd_r_m(G(reg), addr);
			I64, REF => asm.movq_r_m(G(reg), addr);
			F32 => asm.movss_s_m(X(reg), addr);
			F64 => asm.movsd_s_m(X(reg), addr);
			ABS, V128 => asm.movdqu_s_m(X(reg), addr);
		}
	}
	def emit_mov_r_i(reg: Reg, val: int) {
		asm.movd_r_i(G(reg), val);
	}
	def emit_mov_r_li(reg: Reg, val: int) {
		asm.movq_r_i(G(reg), val);
	}
	def emit_mov_r_trap(reg: Reg, reason: TrapReason) {
		var ptr = Pointer.atObject(Execute.trapObjects[reason.tag]);
		var val = int.view(u32.!(ptr - Pointer.NULL));
		asm.movd_r_i(G(reg), val);
	}

	def emit_mov_m_r(ma: MasmAddr, reg: Reg, kind: ValueKind) {
		var addr = A(ma);
		match (kind) {
			I32 => asm.movd_m_r(addr, G(reg));
			I64, REF => asm.movq_m_r(addr, G(reg));
			F32 => asm.movss_m_s(addr, X(reg));
			F64 => asm.movsd_m_s(addr, X(reg));
			ABS, V128 => asm.movdqu_m_s(addr, X(reg));
		}
	}
	def emit_mov_m_i(ma: MasmAddr, val: int) {
		asm.movd_m_i(A(ma), val);
	}
	def emit_mov_m_l(ma: MasmAddr, val: long) {
		var addr = A(ma);
		if (val == int.view(val)) asm.movq_m_i(addr, int.view(val));
		else {  // XXX: use constant pool?
			asm.movd_m_i(addr, int.view(val));
			var p4 = X86_64Addr.new(addr.base, addr.index, addr.scale, addr.disp + 4);
			asm.movd_m_i(p4, int.view(val >> 32));
		}
	}
	def emit_mov_m_f(ma: MasmAddr, bits: u32) {
		asm.movd_m_i(A(ma), int.view(bits));
	}
	def emit_mov_m_d(ma: MasmAddr, bits: u64) {
		emit_mov_m_l(ma, long.view(bits));
	}
	def emit_mov_m_q(ma: MasmAddr, low: u64, high: u64) {  // XXX: use constant pool?
		emit_mov_m_l(ma, long.view(low));
		emit_mov_m_l(MasmAddr(ma.base, ma.offset + 8), long.view(high));
	}
	def emit_mov_m_m(dst: MasmAddr, src: MasmAddr, kind: ValueKind) {
		match (kind) {
			I32, F32 => {
				asm.movd_r_m(scratch, A(src));
				asm.movd_m_r(A(dst), scratch);
			}
			I64, F64, REF => {
				asm.movq_r_m(scratch, A(src));
				asm.movq_m_r(A(dst), scratch);
			}
			ABS, V128 => {
				var scratch = R.XMM15; // TODO
				asm.movdqu_s_m(scratch, A(src));
				asm.movdqu_m_s(A(dst), scratch);
			}
		}
	}

	def emit_addi_r_r(reg: Reg, reg2: Reg) {
		asm.add_r_r(G(reg), G(reg2));
	}
	def emit_addi_r_i(reg: Reg, val: int) {
		asm.add_r_i(G(reg), val);
	}

	def emit_subw_r_i(reg: Reg, val: int) {
		asm.sub_r_i(G(reg), val);
	}
	def emit_subw_r_r(reg: Reg, reg2: Reg) {
		asm.sub_r_r(G(reg), G(reg2));
	}
	def emit_addw_r_i(r1: Reg, val: int) {
		asm.add_r_i(G(r1), val);
	}
	def emit_addw_r_r(r1: Reg, r2: Reg) {
		asm.add_r_r(G(r1), G(r2));
	}
	def emit_shlw_r_i(reg: Reg, imm: u6) {
		asm.shl_r_i(G(reg), imm);
	}
	def emit_shrw_r_i(reg: Reg, imm: u6) {
		asm.shr_r_i(G(reg), imm);
	}

	def emit_binop_r_r(op: Opcode, reg: Reg, reg2: Reg) {
		var r1 = G(reg), r2 = G(reg2);
		match (op) {
			// i64 r_r compares
			I32_DIV_S => {
				var div = X86_64Label.new(), done = X86_64Label.new();
				asm.d.cmp_r_i(r2, -1);
				asm.jc_rel_near(C.NZ, div);
				asm.d.cmp_r_i(r1, 0x80000000);
				asm.jc_rel_far(C.Z, X86_64MasmLabel.!(newTrapLabel(TrapReason.DIV_UNREPRESENTABLE)).label);
				asm.movd_r_r(r1, r2);
				asm.d.neg_r(r1);
				asm.jmp_rel_near(done);
				asm.bind(div);
				var saveRAX = !regAlloc.isFree(X86_64Regs2.RAX);
				var saveRDX = !regAlloc.isFree(X86_64Regs2.RDX);
				var divisor = r2;
				if (r2 == R.RAX) {
					saveRAX = false;
					asm.movd_r_r(scratch, r2);
					divisor = scratch;
				} else if (r2 == R.RDX) {
					saveRDX = false;
					asm.movd_r_r(scratch, r2);
					divisor = scratch;
				}
				if (r1 != R.RAX) {
					if (saveRAX) asm.movq_m_r(R.RSP.plus(scratchStackSlot1), R.RAX);
					if (r1 == R.RDX) saveRDX = false;
					asm.movd_r_r(R.RAX, r1);
				}
				if (saveRDX) {
					asm.movq_m_r(R.RSP.plus(scratchStackSlot2), R.RDX);
				}
				asm.d.cdq();
				asm.d.idiv_r(divisor);
				if (r1 != R.RAX) {
					asm.movd_r_r(r1, R.RAX);
					if (saveRAX) asm.movq_r_m(R.RAX, R.RSP.plus(scratchStackSlot1));
				}
				if (saveRDX) {
					asm.movq_r_m(R.RDX, R.RSP.plus(scratchStackSlot2));
				}
				asm.bind(done);
			}
			I32_DIV_U => {
				var saveRAX = !regAlloc.isFree(X86_64Regs2.RAX);
				var saveRDX = !regAlloc.isFree(X86_64Regs2.RDX);
				var divisor = r2;
				if (r2 == R.RAX) {
					saveRAX = false;
					asm.movd_r_r(scratch, r2);
					divisor = scratch;
				} else if (r2 == R.RDX) {
					saveRDX = false;
					asm.movd_r_r(scratch, r2);
					divisor = scratch;
				}
				if (r1 != R.RAX) {
					if (saveRAX) asm.movq_m_r(R.RSP.plus(scratchStackSlot1), R.RAX);
					if (r1 == R.RDX) saveRDX = false;
					asm.movd_r_r(R.RAX, r1);
				}
				if (saveRDX) {
					asm.movq_m_r(R.RSP.plus(scratchStackSlot2), R.RDX);
				}
				asm.d.movd_r_i(R.RDX, 0);
				asm.d.div_r(divisor);
				if (r1 != R.RAX) {
					asm.movd_r_r(r1, R.RAX);
					if (saveRAX) asm.movq_r_m(R.RAX, R.RSP.plus(scratchStackSlot1));
				}
				if (saveRDX) {
					asm.movq_r_m(R.RDX, R.RSP.plus(scratchStackSlot2));
				}
			}
			_ => unimplemented();
		}
	}
	def emit_binop_r_m(op: Opcode, reg: Reg, ma: MasmAddr) {
		unimplemented();
	}
	def emit_binop_r_i(op: Opcode, reg: Reg, val: int) {
		unimplemented();
	}
	def emit_cmpq_r_i(cond: X86_64Cond, r1: X86_64Gpr, val: int) {
		asm.q.cmp_r_i(r1, val);
		asm.set_r(cond, r1);
		asm.q.movbzx_r_r(r1, r1);
	}
	def emit_cmpq_r_r_i(cond: X86_64Cond, r1: X86_64Gpr, r2: X86_64Gpr, val: int) {
		asm.q.cmp_r_i(r2, val);
		asm.set_r(cond, r1);
		asm.d.movbzx_r_r(r1, r1);
	}
	def emit_cmpq_r_r_r(cond: X86_64Cond, r1: X86_64Gpr, r2: X86_64Gpr, r3: X86_64Gpr) {
		asm.q.cmp_r_r(r2, r3);
		asm.set_r(cond, r1);
		asm.d.movbzx_r_r(r1, r1);
	}
	def emit_cmpq_r_r(cond: X86_64Cond, r1: X86_64Gpr, r2: X86_64Gpr) {
		asm.q.cmp_r_r(r1, r2);
		asm.set_r(cond, r1);
		asm.q.movbzx_r_r(r1, r1);
	}
	def emit_cmpq_r_m(cond: X86_64Cond, r1: X86_64Gpr, addr: X86_64Addr) {
		asm.q.cmp_r_m(r1, addr);
		asm.set_r(cond, r1);
		asm.q.movbzx_r_r(r1, r1);
	}
	def emit_ret() {
		asm.ret();
	}
	def emit_nop() {
		asm.q.or_r_r(R.RAX, R.RAX);
	}

	def emit_iNN_trunc_fNN(config: FloatTrunc, dst: X86_64Gpr, xmm0: X86_64Xmmr, xmm1: X86_64Xmmr) {
		emit_i_trunc_f(config, false, dst, xmm0, xmm1);
	}
	def emit_iNN_trunc_fNN_sat(config: FloatTrunc, dst: X86_64Gpr, xmm0: X86_64Xmmr, xmm1: X86_64Xmmr) {
		emit_i_trunc_f(config, true, dst, xmm0, xmm1);
	}
	private def emit_i_trunc_f(config: FloatTrunc, saturate: bool, dst: X86_64Gpr, xmm0: X86_64Xmmr, xmm1: X86_64Xmmr) {
		config.mov_s_i(asm, xmm1, config.maxv, scratch);
		config.ucomi_s_s(asm, xmm0, xmm1);
		var trap = if(!saturate, X86_64MasmLabel.!(newTrapLabel(TrapReason.FLOAT_UNREPRESENTABLE)).label);
		var above = X86_64Label.new(), is_nan = X86_64Label.new(), below = X86_64Label.new();
		var done = X86_64Label.new();
		if (saturate) asm.jc_rel_near(C.P, is_nan);
		else asm.jc_rel_far(C.P, trap);
		if (saturate) asm.jc_rel_near(C.NC, above);
		else asm.jc_rel_far(C.NC, trap);
		var not_big = X86_64Label.new();

		if (config.isI64 && !config.isSigned) {
			// handle u64 convert of 1p63 < v <= 1p64
			config.mov_s_i(asm, xmm1, if(config.isF64, Floats.d_1p63, Floats.f_1p63), scratch);
			config.ucomi_s_s(asm, xmm0, xmm1);
			asm.jc_rel_near(C.C, not_big);
			config.sub_s_s(asm, xmm0, xmm1);
			config.round_s_s(asm, xmm0, xmm0, X86_64Rounding.TO_ZERO);
			config.cvt2si_r_s(asm.q, dst, xmm0);
			asm.movd_r_i(scratch, 1);
			asm.ror_r_i(scratch, 1);
			asm.q.add_r_r(dst, scratch);
			asm.jmp_rel_near(done);
		}
		asm.bind(not_big);

		if (!saturate || config.isI64 || !config.isSigned) {
			config.mov_s_i(asm, xmm1, config.minv, scratch);
			config.ucomi_s_s(asm, xmm0, xmm1);
			if (saturate) asm.jc_rel_near(C.NA, below); // v <= min
			else asm.jc_rel_far(C.NA, trap); // v <= min
		}

		config.round_s_s(asm, xmm0, xmm0, X86_64Rounding.TO_ZERO);
		if (!config.isI64 && config.isSigned) {
			config.cvt2si_r_s(asm.d, dst, xmm0);
		} else {
			config.cvt2si_r_s(asm.q, dst, xmm0);
		}
		if (saturate) {
			asm.jmp_rel_near(done);
			asm.bind(above);
			config.mov_r_i(asm, dst, config.ceilv);
			asm.jmp_rel_near(done);
			asm.bind(is_nan);
			asm.bind(below);
			asm.movd_r_i(dst, 0);
		}
		asm.bind(done);
	}

	def emit_br(label: MasmLabel) {
		asm.jmp_rel_far(X86_64MasmLabel.!(label).label);
	}
	def emit_br_r(reg: Reg, cond: MasmBrCond, label: MasmLabel) {
		(if(cond.i32, asm.d, asm.q)).cmp_r_i(G(reg), 0);
		var cond = if(cond.zero, X86_64Conds.Z, X86_64Conds.NZ);
		asm.jc_rel_far(cond, X86_64MasmLabel.!(label).label);
	}
	def emit_br_m(addr: MasmAddr, cond: MasmBrCond, label: MasmLabel) {
		(if(cond.i32, asm.d, asm.q)).cmp_m_i(A(addr), 0);
		var cond = if(cond.zero, X86_64Conds.Z, X86_64Conds.NZ);
		asm.jc_rel_far(cond, X86_64MasmLabel.!(label).label);
	}
	def emit_breq_r_i(r: Reg, val: int, label: MasmLabel) {
		asm.d.cmp_r_i(G(r), val);
		asm.jc_rel_far(X86_64Conds.Z, X86_64MasmLabel.!(label).label);
	}
	def emit_breq_r_l(r: Reg, val: int, label: MasmLabel) {
		asm.q.cmp_r_i(G(r), val);
		asm.jc_rel_far(X86_64Conds.Z, X86_64MasmLabel.!(label).label);
	}
	def emit_brne_r_i(r: Reg, val: int, label: MasmLabel) {
		asm.d.cmp_r_i(G(r), val);
		asm.jc_rel_far(X86_64Conds.NZ, X86_64MasmLabel.!(label).label);
	}
	def emit_br_table_r(reg: Reg, labels: Array<MasmLabel>) {
		// XXX: simplify the label patching logic by improving X86_64Assembler
		var r1 = G(reg);
		asm.d.cmp_r_i(r1, labels.length);
		asm.jc_rel_far(C.NC, X86_64MasmLabel.!(labels[labels.length - 1]).label);
		var patcher = X86_64MasmJumpTablePatcher.new();
		asm.q.patcher = patcher;
		asm.q.lea(scratch, X86_64Addr.new(null, null, 1, REL_MARKER));
		asm.q.patcher = null;
		asm.ijmp_m(X86_64Addr.new(scratch, r1, 8, 0));
		w.align(8);
		var jtpos = w.atEnd().pos;
		if (jump_tables == null) jump_tables = Vector.new();
		jump_tables.put(jtpos, Arrays.map(labels, getLabel));
		w.skipN(labels.length * 8);
		w.at(patcher.pos).put_b32(jtpos - (patcher.pos + patcher.delta));
		w.atEnd();
	}
	def emit_call_r(reg: Reg) {
		asm.icall_r(G(reg));
	}
	def emit_call_runtime_callHost(func_arg: Reg) {
		emit_call_runtime(X86_64Interpreter.runtime_callHost);
	}
	def emit_call_runtime_GLOBAL_GET() {
		emit_call_runtime(X86_64Interpreter.runtime_GLOBAL_GET);
	}
	def emit_call_runtime_GLOBAL_SET() {
		emit_call_runtime(X86_64Interpreter.runtime_GLOBAL_SET);
	}
	private def emit_call_runtime<P, R>(closure: P -> R) {
		var ptr = CiRuntime.unpackClosure<X86_64Interpreter, P, R>(closure).0;
		// Do an absolute call into the runtime
		asm.movd_r_i(scratch, int.view(u32.!(ptr - Pointer.NULL))); // XXX: make direct call to runtime if within 2GB
		asm.icall_r(scratch);
	}
	def emit_write_runtime_vsp(vsp: Reg) {
		var offsets = getOffsets();
		asm.movq_r_m(scratch, absPointer(offsets.Interpreter_valueStack));
		asm.movq_m_r(scratch.plus(offsets.ValueStack_sp), G(vsp));
	}
	def emit_i32_clz_r_r(r: X86_64Gpr, s: X86_64Gpr) {
		asm.movd_r_i(scratch, -1);
		asm.d.bsr_r_r(r, s);
		asm.d.cmov_r(C.Z, r, scratch);
		asm.movd_r_i(scratch, 31);
		asm.d.sub_r_r(scratch, r);
		asm.movd_r_r(r, scratch); // XXX: can save an instruction here?
	}
	def emit_i32_ctz_r_r(r: X86_64Gpr, s: X86_64Gpr) {
		asm.d.bsf_r_r(r, s);
		asm.movd_r_i(scratch, 32);
		asm.d.cmov_r(C.Z, r, scratch);
	}
	def emit_i64_clz_r_r(r: X86_64Gpr, s: X86_64Gpr) {
		asm.movq_r_i(scratch, -1);
		asm.q.bsr_r_r(r, s);
		asm.q.cmov_r(C.Z, r, scratch);
		asm.movq_r_i(scratch, 63);
		asm.q.sub_r_r(scratch, r);
		asm.movq_r_r(r, scratch); // XXX: can save an instruction with second output reg
	}
	def emit_i64_ctz_r_r(r: X86_64Gpr, s: X86_64Gpr) {
		asm.q.bsf_r_r(r, s);
		asm.movq_r_i(scratch, 64);
		asm.q.cmov_r(C.Z, r, scratch);
	}
	def emit_i64_extend_i32_s(r: X86_64Gpr) {
		asm.q.shl_r_i(r, 32);
		asm.q.sar_r_i(r, 32);
	}
	def emit_i64_extend_i32_u(r: X86_64Gpr) {
		asm.movd_r_r(r, r);
	}
	def getLabel(m: MasmLabel) -> X86_64Label {
		return X86_64MasmLabel.!(m).label;
	}
	def absPointer(ptr: Pointer) -> X86_64Addr {
		return X86_64Addr.new(null, null, 1, int.view(u32.!(ptr - Pointer.NULL)));
	}
	def getOffsets() -> V3Offsets {
		if (offsets == null) offsets = V3Offsets.new();
		return offsets;
	}
}

// XXX: Simplify relative loads for jump table by improving X86_64Assembler
def ABS_MARKER = 0x77665544;
def REL_MARKER = 0x99887766;
class X86_64MasmJumpTablePatcher extends X86_64AddrPatcher {
	var pos: int;
	var delta: int;
	new() super(ABS_MARKER, REL_MARKER) { }
	def recordRel32(pos: int, delta: int, addr: X86_64Addr) {
		this.pos = pos;
		this.delta = delta;
	}
}
