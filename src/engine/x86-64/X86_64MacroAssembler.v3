// Copyright 2022 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

def R: X86_64Regs2;
def G = R.toGpr, X = R.toXmmr;
def C: X86_64Conds;
def A(ma: MasmAddr) -> X86_64Addr {
	return X86_64Addr.new(G(ma.base), null, 1, ma.offset);
}

class X86_64MasmLabel extends MasmLabel {
	def label: X86_64Label;

	new(create_pos: int, label) super(create_pos) {	}
}
class X86_64MacroAssembler extends MacroAssembler {
	def w: DataWriter;
	def asm = X86_64Assemblers.create64(w);
	var scratch: X86_64Gpr;
	var jump_tables: Vector<(int, Array<X86_64Label>)>;

	new(w, regAlloc: RegAlloc) super(Target.tagging, regAlloc) {
		scratch = G(regAlloc.regConfig.regs.scratch);
	}

	// Called after
	def prepareToCopyInto(asm: X86_64Assembler, addr: u64) {
		if (jump_tables == null) return;
		for (i < jump_tables.length) {
			var t = jump_tables[i], offset = t.0, labels = t.1;
			for (j < labels.length) {
				var l = labels[j], target = addr + u64.!(l.pos);
				asm.w.at(offset + j * 8).put_b64(long.view(target));
			}
		}
		jump_tables = null;
	}

	// Label operations
	def newLabel(create_pos: int) -> X86_64MasmLabel {
		return X86_64MasmLabel.new(create_pos, asm.newLabel());
	}
	def bindLabel(l: MasmLabel) {
		if (Trace.compiler) Trace.OUT.put2("    bind label (+%d) -> @%d", l.create_pos, w.end()).outln();
		var label = X86_64MasmLabel.!(l);
		asm.bind(label.label);
	}
	def emit_mov_r_r(reg: Reg, reg2: Reg) {
		var rd = G(reg);
		if (rd != null) asm.movq_r_r(rd, G(reg2));
		else asm.movsd_s_s(X(reg), X(reg2)); // TODO: v128
	}
	def emit_mov_r_m(reg: Reg, kind: ValueKind, ma: MasmAddr) {
		var addr = A(ma);
		match (kind) {
			I32 => asm.movd_r_m(G(reg), addr);
			I64, REF => asm.movq_r_m(G(reg), addr);
			F32 => asm.movss_s_m(X(reg), addr);
			F64 => asm.movsd_s_m(X(reg), addr);
			V128 => asm.movdqu_s_m(X(reg), addr);
		}
	}
	def emit_mov_r_i(reg: Reg, val: int) {
		asm.movd_r_i(R.toGpr(reg), val);
	}

	def emit_mov_m_r(ma: MasmAddr, reg: Reg, kind: ValueKind) {
		var addr = A(ma);
		match (kind) {
			I32 => asm.movd_m_r(addr, G(reg));
			I64, REF => asm.movq_m_r(addr, G(reg));
			F32 => asm.movss_m_s(addr, X(reg));
			F64 => asm.movsd_m_s(addr, X(reg));
			V128 => asm.movdqu_m_s(addr, X(reg));
		}
	}
	def emit_mov_m_i(ma: MasmAddr, val: int) {
		asm.movd_m_i(A(ma), val);
	}
	def emit_mov_m_l(ma: MasmAddr, val: long) {
		var addr = A(ma);
		if (val == int.view(val)) asm.movq_m_i(addr, int.view(val));
		else {  // XXX: use constant pool?
			asm.movd_m_i(addr, int.view(val));
			var p4 = X86_64Addr.new(addr.base, addr.index, addr.scale, addr.disp + 4);
			asm.movd_m_i(p4, int.view(val >> 32));
		}
	}
	def emit_mov_m_f(ma: MasmAddr, bits: u32) {
		asm.movd_m_i(A(ma), int.view(bits));
	}
	def emit_mov_m_d(ma: MasmAddr, bits: u64) {
		emit_mov_m_l(ma, long.view(bits));
	}
	def emit_mov_m_q(ma: MasmAddr, low: u64, high: u64) {  // XXX: use constant pool?
		emit_mov_m_l(ma, long.view(low));
		emit_mov_m_l(MasmAddr(ma.base, ma.offset + 8), long.view(high));
	}
	def emit_mov_m_m(dst: MasmAddr, src: MasmAddr, kind: ValueKind) {
		match (kind) {
			I32, F32 => {
				asm.movd_r_m(scratch, A(src));
				asm.movd_m_r(A(dst), scratch);
			}
			I64, F64, REF => {
				asm.movq_r_m(scratch, A(src));
				asm.movq_m_r(A(dst), scratch);
			}
			V128 => {
				var scratch = X86_64Regs.XMM15; // TODO
				asm.movdqu_s_m(scratch, A(src));
				asm.movdqu_m_s(A(dst), scratch);
			}
		}
	}

	def emit_addi_r_r(reg: Reg, reg2: Reg) {
		asm.add_r_r(R.toGpr(reg), X86_64Regs2.toGpr(reg2));
	}
	def emit_addi_r_i(reg: Reg, val: int) {
		asm.add_r_i(R.toGpr(reg), val);
	}

	def emit_unop_r(op: Opcode, reg: Reg) {
		var r1 = G(reg);
		match (op) {
			I32_CLZ => {
				asm.movd_r_i(scratch, -1);
				asm.d.bsr_r_r(r1, r1);
				asm.d.cmov_r(C.Z, r1, scratch);
				asm.movd_r_i(scratch, 31);
				asm.d.sub_r_r(scratch, r1);
				asm.movd_r_r(r1, scratch); // XXX: can save an instruction with second output reg
			}
			I32_CTZ => {
				asm.d.bsf_r_r(r1, r1);
				asm.movd_r_i(scratch, 32);
				asm.d.cmov_r(C.Z, r1, scratch);
			}
			I32_POPCNT => asm.d.popcnt_r_r(r1, r1);
			I32_EQZ => emit_cmpd_r_i(C.Z, r1, 0);
			I64_CLZ => {
				asm.movq_r_i(scratch, -1);
				asm.q.bsr_r_r(r1, r1);
				asm.q.cmov_r(C.Z, r1, scratch);
				asm.movq_r_i(scratch, 63);
				asm.q.sub_r_r(scratch, r1);
				asm.movq_r_r(r1, scratch); // XXX: can save an instruction with second output reg
			}
			I64_CTZ =>  {
				asm.q.bsf_r_r(r1, r1);
				asm.movq_r_i(scratch, 64);
				asm.q.cmov_r(C.Z, r1, scratch);
			}
			I32_WRAP_I64 => asm.movd_r_r(r1, r1);
			I64_POPCNT => asm.q.popcnt_r_r(r1, r1);
			REF_IS_NULL,
			I64_EQZ => emit_cmpq_r_i(C.Z, r1, 0);
			I64_EXTEND_I32_U => asm.movd_r_r(r1, r1);
			I32_EXTEND8_S => asm.d.movbsx_r_r(r1, r1);
			I32_EXTEND16_S => asm.d.movwsx_r_r(r1, r1);
			I64_EXTEND8_S => asm.q.movbsx_r_r(r1, r1);
			I64_EXTEND16_S => asm.q.movwsx_r_r(r1, r1);
			I64_EXTEND_I32_S, I64_EXTEND32_S => {
				asm.q.shl_r_i(r1, 32);
				asm.q.sar_r_i(r1, 32);
			}
			_ => unimplemented();
		}
	}

	def emit_binop_r_r(op: Opcode, reg: Reg, reg2: Reg) {
		var r1 = G(reg), r2 = G(reg2);
		match (op) {
			// i32 r_r compares
			I32_EQ => emit_cmpd_r_r(C.Z, r1, r2);
			I32_NE => emit_cmpd_r_r(C.NZ, r1, r2);
			I32_LT_S => emit_cmpd_r_r(C.L, r1, r2);
			I32_LT_U => emit_cmpd_r_r(C.C, r1, r2);
			I32_GT_S => emit_cmpd_r_r(C.G, r1, r2);
			I32_GT_U => emit_cmpd_r_r(C.A, r1, r2);
			I32_LE_S => emit_cmpd_r_r(C.LE, r1, r2);
			I32_LE_U => emit_cmpd_r_r(C.NA, r1, r2);
			I32_GE_S => emit_cmpd_r_r(C.GE, r1, r2);
			I32_GE_U => emit_cmpd_r_r(C.NC, r1, r2);
			// i64 r_r compares
			REF_EQ,
			I64_EQ => emit_cmpq_r_r(C.Z, r1, r2);
			I64_NE => emit_cmpq_r_r(C.NZ, r1, r2);
			I64_LT_S => emit_cmpq_r_r(C.L, r1, r2);
			I64_LT_U => emit_cmpq_r_r(C.C, r1, r2);
			I64_GT_S => emit_cmpq_r_r(C.G, r1, r2);
			I64_GT_U => emit_cmpq_r_r(C.A, r1, r2);
			I64_LE_S => emit_cmpq_r_r(C.LE, r1, r2);
			I64_LE_U => emit_cmpq_r_r(C.NA, r1, r2);
			I64_GE_S => emit_cmpq_r_r(C.GE, r1, r2);
			I64_GE_U => emit_cmpq_r_r(C.NC, r1, r2);
			// i32 r_r binops
			I32_ADD => asm.d.add_r_r(r1, r2);
			I32_SUB => asm.d.sub_r_r(r1, r2);
			I32_MUL => asm.d.imul_r_r(r1, r2);
			I32_DIV_S,
			I32_DIV_U,
			I32_REM_S,
			I32_REM_U => unimplemented();
			I32_AND => asm.d.and_r_r(r1, r2);
			I32_OR  => asm.d.or_r_r(r1, r2);
			I32_XOR => asm.d.xor_r_r(r1, r2);
			I32_SHL => emit_shift(asm.d.shl_r_cl, r1, r2);
			I32_SHR_S => emit_shift(asm.d.sar_r_cl, r1, r2);
			I32_SHR_U => emit_shift(asm.d.shr_r_cl, r1, r2);
			I32_ROTL => emit_shift(asm.d.rol_r_cl, r1, r2);
			I32_ROTR => emit_shift(asm.d.ror_r_cl, r1, r2);
			// i64 r_r binops
			I64_ADD => asm.q.add_r_r(r1, r2);
			I64_SUB => asm.q.sub_r_r(r1, r2);
			I64_MUL => asm.q.imul_r_r(r1, r2);
			I64_DIV_S,
			I64_DIV_U,
			I64_REM_S,
			I64_REM_U => unimplemented();
			I64_AND => asm.q.and_r_r(r1, r2);
			I64_OR  => asm.q.or_r_r(r1, r2);
			I64_XOR => asm.q.xor_r_r(r1, r2);
			I64_SHL => emit_shift(asm.q.shl_r_cl, r1, r2);
			I64_SHR_S => emit_shift(asm.q.sar_r_cl, r1, r2);
			I64_SHR_U => emit_shift(asm.q.shr_r_cl, r1, r2);
			I64_ROTL => emit_shift(asm.q.rol_r_cl, r1, r2);
			I64_ROTR => emit_shift(asm.q.ror_r_cl, r1, r2);
			_ => unimplemented();
		}
	}
	private def emit_shift(f: X86_64Gpr -> X86_64Assembler, r1: X86_64Gpr, r2: X86_64Gpr) {
		if (r1 == X86_64Regs.RCX) {
			if (r2 == X86_64Regs.RCX) {
				f(r1); // shift rcx, rcx
			} else {
				// shift rcx, r1  => swap
				asm.xchg_r_r(r1, r2);
				f(r1);
				asm.xchg_r_r(r1, r2);
			}
			return;
		}
		if (r2 == X86_64Regs.RCX) {
			// shift r1, rcx
			f(r1);
		} else {
			if (regAlloc.isFree(X86_64Regs2.RCX)) {
				// move shiftor into rcx
				asm.movd_r_r(X86_64Regs.RCX, r2);
				f(r1);
			} else {
				// save rcx before moving shiftor into rcx, then restore
				asm.movq_r_r(scratch, r2);
				asm.movd_r_r(X86_64Regs.RCX, r2);
				f(r1);
				asm.movq_r_r(r2, scratch);
			}
		}
	}
	def emit_binop_r_m(op: Opcode, reg: Reg, ma: MasmAddr) {
		var r1 = G(reg), addr = A(ma);
		match (op) {
			// i32 r_r compares
			I32_EQ => emit_cmpd_r_m(C.Z, r1, addr);
			I32_NE => emit_cmpd_r_m(C.NZ, r1, addr);
			I32_LT_S => emit_cmpd_r_m(C.L, r1, addr);
			I32_LT_U => emit_cmpd_r_m(C.C, r1, addr);
			I32_GT_S => emit_cmpd_r_m(C.G, r1, addr);
			I32_GT_U => emit_cmpd_r_m(C.A, r1, addr);
			I32_LE_S => emit_cmpd_r_m(C.LE, r1, addr);
			I32_LE_U => emit_cmpd_r_m(C.NA, r1, addr);
			I32_GE_S => emit_cmpd_r_m(C.GE, r1, addr);
			I32_GE_U => emit_cmpd_r_m(C.NC, r1, addr);
			// i64 r_r compares
			I64_EQ => emit_cmpq_r_m(C.Z, r1, addr);
			I64_NE => emit_cmpq_r_m(C.NZ, r1, addr);
			I64_LT_S => emit_cmpq_r_m(C.L, r1, addr);
			I64_LT_U => emit_cmpq_r_m(C.C, r1, addr);
			I64_GT_S => emit_cmpq_r_m(C.G, r1, addr);
			I64_GT_U => emit_cmpq_r_m(C.A, r1, addr);
			I64_LE_S => emit_cmpq_r_m(C.LE, r1, addr);
			I64_LE_U => emit_cmpq_r_m(C.NA, r1, addr);
			I64_GE_S => emit_cmpq_r_m(C.GE, r1, addr);
			I64_GE_U => emit_cmpq_r_m(C.NC, r1, addr);
			// i32 r_m binops
			I32_ADD => asm.d.add_r_m(r1, addr);
			I32_SUB => asm.d.sub_r_m(r1, addr);
			I32_MUL => asm.d.imul_r_m(r1, addr);
			I32_DIV_S,
			I32_DIV_U,
			I32_REM_S,
			I32_REM_U => unimplemented();
			I32_AND => asm.d.and_r_m(r1, addr);
			I32_OR  => asm.d.or_r_m(r1, addr);
			I32_XOR => asm.d.xor_r_m(r1, addr);
			I32_SHL,
			I32_SHR_S,
			I32_SHR_U,
			I32_ROTL,
			I32_ROTR => unimplemented();
			// i64 r_m binops
			I64_ADD => asm.q.add_r_m(r1, addr);
			I64_SUB => asm.q.sub_r_m(r1, addr);
			I64_MUL => asm.q.imul_r_m(r1, addr);
			I64_DIV_S,
			I64_DIV_U,
			I64_REM_S,
			I64_REM_U => unimplemented();
			I64_AND => asm.q.and_r_m(r1, addr);
			I64_OR  => asm.q.or_r_m(r1, addr);
			I64_XOR => asm.q.xor_r_m(r1, addr);
			I64_SHL,
			I64_SHR_S,
			I64_SHR_U,
			I64_ROTL,
			I64_ROTR => unimplemented();
			_ => unimplemented();
		}
	}
	def emit_binop_r_i(op: Opcode, reg: Reg, val: int) {
		var r1 = G(reg);
		match (op) {
			// i32 r_r compares
			I32_EQ => emit_cmpd_r_i(C.Z, r1, val);
			I32_NE => emit_cmpd_r_i(C.NZ, r1, val);
			I32_LT_S => emit_cmpd_r_i(C.L, r1, val);
			I32_LT_U => emit_cmpd_r_i(C.C, r1, val);
			I32_GT_S => emit_cmpd_r_i(C.G, r1, val);
			I32_GT_U => emit_cmpd_r_i(C.A, r1, val);
			I32_LE_S => emit_cmpd_r_i(C.LE, r1, val);
			I32_LE_U => emit_cmpd_r_i(C.NA, r1, val);
			I32_GE_S => emit_cmpd_r_i(C.GE, r1, val);
			I32_GE_U => emit_cmpd_r_i(C.NC, r1, val);
			// i64 r_r compares
			I64_EQ => emit_cmpq_r_i(C.Z, r1, val);
			I64_NE => emit_cmpq_r_i(C.NZ, r1, val);
			I64_LT_S => emit_cmpq_r_i(C.L, r1, val);
			I64_LT_U => emit_cmpq_r_i(C.C, r1, val);
			I64_GT_S => emit_cmpq_r_i(C.G, r1, val);
			I64_GT_U => emit_cmpq_r_i(C.A, r1, val);
			I64_LE_S => emit_cmpq_r_i(C.LE, r1, val);
			I64_LE_U => emit_cmpq_r_i(C.NA, r1, val);
			I64_GE_S => emit_cmpq_r_i(C.GE, r1, val);
			I64_GE_U => emit_cmpq_r_i(C.NC, r1, val);
			// i32 r_m binops
			I32_ADD => asm.d.add_r_i(r1, val);
			I32_SUB => asm.d.sub_r_i(r1, val);
			I32_MUL => asm.d.imul_r_i(r1, val);
			I32_DIV_S,
			I32_DIV_U,
			I32_REM_S,
			I32_REM_U => unimplemented();
			I32_AND => asm.d.and_r_i(r1, val);
			I32_OR  => asm.d.or_r_i(r1, val);
			I32_XOR => asm.d.xor_r_i(r1, val);
			I32_SHL => asm.d.shl_r_i(r1, u5.view(val));
			I32_SHR_S => asm.d.sar_r_i(r1, u5.view(val));
			I32_SHR_U => asm.d.shl_r_i(r1, u5.view(val));
			I32_ROTL => asm.d.rol_r_i(r1, u5.view(val));
			I32_ROTR => asm.d.ror_r_i(r1, u5.view(val));
			// i64 r_m binops
			I64_ADD => asm.q.add_r_i(r1, val);
			I64_SUB => asm.q.sub_r_i(r1, val);
			I64_MUL => asm.q.imul_r_i(r1, val);
			I64_DIV_S,
			I64_DIV_U,
			I64_REM_S,
			I64_REM_U => unimplemented();
			I64_AND => asm.q.and_r_i(r1, val);
			I64_OR  => asm.q.or_r_i(r1, val);
			I64_XOR => asm.q.xor_r_i(r1, val);
			I64_SHL => asm.q.shl_r_i(r1, u6.view(val));
			I64_SHR_S => asm.q.sar_r_i(r1, u6.view(val));
			I64_SHR_U => asm.q.shl_r_i(r1, u6.view(val));
			I64_ROTL => asm.q.rol_r_i(r1, u6.view(val));
			I64_ROTR => asm.q.rol_r_i(r1, u6.view(val));

			_ => unimplemented();
		}
	}
	def emit_cmpd_r_i(cond: X86_64Cond, r1: X86_64Gpr, val: int) {
		asm.d.cmp_r_i(r1, val);
		asm.set_r(cond, r1);
		asm.d.movbzx_r_r(r1, r1);
	}
	def emit_cmpq_r_i(cond: X86_64Cond, r1: X86_64Gpr, val: int) {
		asm.q.cmp_r_i(r1, val);
		asm.set_r(cond, r1);
		asm.q.movbzx_r_r(r1, r1);
	}
	def emit_cmpd_r_r(cond: X86_64Cond, r1: X86_64Gpr, r2: X86_64Gpr) {
		asm.d.cmp_r_r(r1, r2);
		asm.set_r(cond, r1);
		asm.d.movbzx_r_r(r1, r1);
	}
	def emit_cmpq_r_r(cond: X86_64Cond, r1: X86_64Gpr, r2: X86_64Gpr) {
		asm.q.cmp_r_r(r1, r2);
		asm.set_r(cond, r1);
		asm.q.movbzx_r_r(r1, r1);
	}
	def emit_cmpd_r_m(cond: X86_64Cond, r1: X86_64Gpr, addr: X86_64Addr) {
		asm.d.cmp_r_m(r1, addr);
		asm.set_r(cond, r1);
		asm.d.movbzx_r_r(r1, r1);
	}
	def emit_cmpq_r_m(cond: X86_64Cond, r1: X86_64Gpr, addr: X86_64Addr) {
		asm.q.cmp_r_m(r1, addr);
		asm.set_r(cond, r1);
		asm.q.movbzx_r_r(r1, r1);
	}
	def emit_ret() {
		asm.ret();
	}
	def emit_nop() {
		asm.q.or_r_r(X86_64Regs.RAX, X86_64Regs.RAX);
	}

	def emit_br(label: MasmLabel) {
		asm.jmp_rel_far(X86_64MasmLabel.!(label).label);
	}
	def emit_br_r(reg: Reg, nonzero: bool, label: MasmLabel) {
		asm.d.cmp_r_i(G(reg), 0);
		var cond = if(nonzero, X86_64Conds.NZ, X86_64Conds.Z);
		asm.jc_rel_far(cond, X86_64MasmLabel.!(label).label);
	}
	def emit_br_m(addr: MasmAddr, nonzero: bool, label: MasmLabel) {
		asm.d.cmp_m_i(A(addr), 0);
		var cond = if(nonzero, X86_64Conds.NZ, X86_64Conds.Z);
		asm.jc_rel_far(cond, X86_64MasmLabel.!(label).label);
	}
	def emit_br_table_r(reg: Reg, labels: Array<MasmLabel>) {
		// XXX: simplify the label patching logic by improving X86_64Assembler
		var r1 = G(reg);
		asm.d.cmp_r_i(r1, labels.length);
		asm.jc_rel_far(C.NC, X86_64MasmLabel.!(labels[labels.length - 1]).label);
		var patcher = X86_64MasmJumpTablePatcher.new();
		asm.q.patcher = patcher;
		asm.q.lea(scratch, X86_64Addr.new(null, null, 1, REL_MARKER));
		asm.q.patcher = null;
		asm.ijmp_m(X86_64Addr.new(scratch, r1, 8, 0));
		w.align(8);
		var jtpos = w.atEnd().pos;
		if (jump_tables == null) jump_tables = Vector.new();
		jump_tables.put(jtpos, Arrays.map(labels, getLabel));
		w.skipN(labels.length * 8);
		w.at(patcher.pos).put_b32(jtpos - (patcher.pos + patcher.delta));
		w.atEnd();
	}
	def getLabel(m: MasmLabel) -> X86_64Label {
		return X86_64MasmLabel.!(m).label;
	}
}

// XXX: Simplify relative loads for jump table by improving X86_64Assembler
def ABS_MARKER = 0x77665544;
def REL_MARKER = 0x99887766;
class X86_64MasmJumpTablePatcher extends X86_64AddrPatcher {
	var pos: int;
	var delta: int;
	new() super(ABS_MARKER, REL_MARKER) { }
	def recordRel32(pos: int, delta: int, addr: X86_64Addr) {
		this.pos = pos;
		this.delta = delta;
	}
}