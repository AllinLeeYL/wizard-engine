// Copyright 2022 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

// XXX: reduce duplication with MacroAssembler
def G = X86_64Regs2.toGpr, X = X86_64Regs2.toXmmr;
def R: X86_64Regs;
def C: X86_64Conds;
def A(ma: MasmAddr) -> X86_64Addr {
	return X86_64Addr.new(G(ma.base), null, 1, ma.offset);
}

// Shorten constants inside this file.
def NO_REG = SpcConsts.NO_REG;
def IS_STORED = SpcConsts.IS_STORED;
def IS_CONST = SpcConsts.IS_CONST;
def IN_REG = SpcConsts.IN_REG;
def TAG_STORED = SpcConsts.TAG_STORED;
def KIND_MASK = SpcConsts.KIND_MASK;
def KIND_I32 = SpcConsts.KIND_I32;
def KIND_I64 = SpcConsts.KIND_I64;
def KIND_F32 = SpcConsts.KIND_F32;
def KIND_F64 = SpcConsts.KIND_F64;
def KIND_V128 = SpcConsts.KIND_V128;
def KIND_REF = SpcConsts.KIND_REF;
def KIND_ABS = SpcConsts.KIND_ABS;

class X86_64SinglePassCompiler extends SinglePassCompiler {
	def w = DataWriter.new();
	def mmasm = X86_64MacroAssembler.new(w, X86_64Regs2.SPC_ALLOC.copy());
	def asm = mmasm.asm;

	new(extensions: Extension.set, limits: Limits, config: RegConfig, module: Module)
		super(mmasm, extensions, limits, module) {
	}

	private def visitCompareI(asm: X86_64Assembler, cond: X86_64Cond) -> bool {
		var b = state.pop(), a = popReg();
		if (b.isConst()) asm.cmp_r_i(G(a.reg), b.const);
		else if (b.inReg()) asm.cmp_r_r(G(a.reg), G(b.reg));
		else asm.cmp_r_m(G(a.reg), A(masm.slotAddr(state.sp + 1)));
		freeVal(b);
		freeVal(a);
		var reg = allocReg(ValueKind.I32).reg, r1 = G(reg);
		asm.set_r(cond, r1);
		asm.movbzx_r_r(r1, r1);
		state.push(KIND_I32 | IN_REG, reg, 0);
		return true;
	}
	def visit_I32_EQZ() {
		state.push(KIND_I32 | IS_CONST, NO_REG, 0);
		visit_I32_EQ();
	}
	def visit_I32_EQ()   { var x = tryFold_uu_z(V3Eval.I32_EQ)   || visitCompareI(asm.d, C.Z); }
	def visit_I32_NE()   { var x = tryFold_uu_z(V3Eval.I32_NE)   || visitCompareI(asm.d, C.NZ); }
	def visit_I32_LT_S() { var x = tryFold_ii_z(V3Eval.I32_LT_S) || visitCompareI(asm.d, C.L); }
	def visit_I32_LT_U() { var x = tryFold_uu_z(V3Eval.I32_LT_U) || visitCompareI(asm.d, C.C); }
	def visit_I32_GT_S() { var x = tryFold_ii_z(V3Eval.I32_GT_S) || visitCompareI(asm.d, C.G); }
	def visit_I32_GT_U() { var x = tryFold_uu_z(V3Eval.I32_GT_U) || visitCompareI(asm.d, C.A); }
	def visit_I32_LE_S() { var x = tryFold_ii_z(V3Eval.I32_LE_S) || visitCompareI(asm.d, C.LE); }
	def visit_I32_LE_U() { var x = tryFold_uu_z(V3Eval.I32_LE_U) || visitCompareI(asm.d, C.NA); }
	def visit_I32_GE_S() { var x = tryFold_ii_z(V3Eval.I32_GE_S) || visitCompareI(asm.d, C.GE); }
	def visit_I32_GE_U() { var x = tryFold_uu_z(V3Eval.I32_GE_U) || visitCompareI(asm.d, C.NC); }

	def visit_I64_EQZ() {
		state.push(KIND_I64 | IS_CONST, NO_REG, 0);
		visit_I64_EQ();
	}
	def visit_I64_EQ()   { var x = tryFold_qq_z(V3Eval.I64_EQ)   || visitCompareI(asm.q, C.Z); }
	def visit_I64_NE()   { var x = tryFold_qq_z(V3Eval.I64_NE)   || visitCompareI(asm.q, C.NZ); }
	def visit_I64_LT_S() { var x = tryFold_ll_z(V3Eval.I64_LT_S) || visitCompareI(asm.q, C.L); }
	def visit_I64_LT_U() { var x = tryFold_qq_z(V3Eval.I64_LT_U) || visitCompareI(asm.q, C.C); }
	def visit_I64_GT_S() { var x = tryFold_ll_z(V3Eval.I64_GT_S) || visitCompareI(asm.q, C.G); }
	def visit_I64_GT_U() { var x = tryFold_qq_z(V3Eval.I64_GT_U) || visitCompareI(asm.q, C.A); }
	def visit_I64_LE_S() { var x = tryFold_ll_z(V3Eval.I64_LE_S) || visitCompareI(asm.q, C.LE); }
	def visit_I64_LE_U() { var x = tryFold_qq_z(V3Eval.I64_LE_U) || visitCompareI(asm.q, C.NA); }
	def visit_I64_GE_S() { var x = tryFold_ll_z(V3Eval.I64_GE_S) || visitCompareI(asm.q, C.GE); }
	def visit_I64_GE_U() { var x = tryFold_qq_z(V3Eval.I64_GE_U) || visitCompareI(asm.q, C.NC); }

	def visit_REF_IS_NULL() {
		state.push(KIND_REF | IS_CONST, NO_REG, 0);
		visit_I64_EQ();
	}
/*
	private def visitCompareF(cond: X86_64Cond);
	private def visitCompareD(cond: X86_64Cond);
	def visit_F32_EQ() { visitCompareF(C.NZ); }
	def visit_F32_NE() { visitCompareF(C.Z); }
	def visit_F32_LT() { visitCompareF(C.NC); }
	def visit_F32_GT() { visitCompareF(C.NA); }
	def visit_F32_LE() { visitCompareF(C.A); }
	def visit_F32_GE() { visitCompareF(C.C); }
	def visit_F64_EQ() { visitCompareD(C.NZ); }
	def visit_F64_NE() { visitCompareD(C.Z); }
	def visit_F64_LT() { visitCompareD(C.NC); }
	def visit_F64_GT() { visitCompareD(C.NA); }
	def visit_F64_LE() { visitCompareD(C.A); }
	def visit_F64_GE() { visitCompareD(C.C); }
*/
	def visit_I32_CLZ() {
		var x = tryFold_u_u(V3Eval.I32_CLZ)
		|| do_op1_r_r(ValueKind.I32, mmasm.emit_i32_clz);
	}
	def visit_I32_CTZ() {
		var x = tryFold_u_u(V3Eval.I32_CTZ)
			|| do_op1_r_r(ValueKind.I32, mmasm.emit_i32_ctz);
	}
	def visit_I32_POPCNT() {
		var x = tryFold_u_u(V3Eval.I32_POPCNT)
			|| do_op1_r_r(ValueKind.I32, asm.d.popcnt_r_r);
	}

	private def visitSimpleOp2_uu_u(fold: (u32, u32) -> u32,
		emit_r_i: (X86_64Gpr, int) -> X86_64Assembler,
		emit_r_m: (X86_64Gpr, X86_64Addr) -> X86_64Assembler,
		emit_r_r: (X86_64Gpr, X86_64Gpr) -> X86_64Assembler) {
		var x = tryFold_uu_u(fold)
			|| do_op2_r_i(ValueKind.I32, emit_r_i)
			|| do_op2_r_m(ValueKind.I32, emit_r_m)
			|| do_op2_r_r(ValueKind.I32, emit_r_r);
	}
	def visit_I32_ADD() { visitSimpleOp2_uu_u(V3Eval.I32_ADD, asm.d.add_r_i, asm.d.add_r_m, asm.d.add_r_r); }
	def visit_I32_SUB() { visitSimpleOp2_uu_u(V3Eval.I32_SUB, asm.d.sub_r_i, asm.d.sub_r_m, asm.d.sub_r_r); }
	def visit_I32_MUL() { visitSimpleOp2_uu_u(V3Eval.I32_MUL, asm.d.imul_r_i, asm.d.imul_r_m, asm.d.imul_r_r); }
	def visit_I32_AND() { visitSimpleOp2_uu_u(V3Eval.I32_AND, asm.d.and_r_i, asm.d.and_r_m, asm.d.and_r_r); }
	def visit_I32_OR()  { visitSimpleOp2_uu_u(V3Eval.I32_OR, asm.d.or_r_i, asm.d.or_r_m, asm.d.or_r_r); }
	def visit_I32_XOR() { visitSimpleOp2_uu_u(V3Eval.I32_XOR, asm.d.xor_r_i, asm.d.xor_r_m, asm.d.xor_r_r); }

	private def visitSimpleOp2_qq_q(fold: (u64, u64) -> u64,
		emit_r_i: (X86_64Gpr, int) -> X86_64Assembler,
		emit_r_m: (X86_64Gpr, X86_64Addr) -> X86_64Assembler,
		emit_r_r: (X86_64Gpr, X86_64Gpr) -> X86_64Assembler) {
		var x = tryFold_qq_q(fold)
			|| do_op2_r_i(ValueKind.I64, emit_r_i)
			|| do_op2_r_m(ValueKind.I64, emit_r_m)
			|| do_op2_r_r(ValueKind.I64, emit_r_r);
	}
	def visit_I64_ADD() { visitSimpleOp2_qq_q(V3Eval.I64_ADD, asm.q.add_r_i, asm.q.add_r_m, asm.q.add_r_r); }
	def visit_I64_SUB() { visitSimpleOp2_qq_q(V3Eval.I64_SUB, asm.q.sub_r_i, asm.q.sub_r_m, asm.q.sub_r_r); }
	def visit_I64_MUL() { visitSimpleOp2_qq_q(V3Eval.I64_MUL, asm.q.imul_r_i, asm.q.imul_r_m, asm.q.imul_r_r); }
	def visit_I64_AND() { visitSimpleOp2_qq_q(V3Eval.I64_AND, asm.q.and_r_i, asm.q.and_r_m, asm.q.and_r_r); }
	def visit_I64_OR()  { visitSimpleOp2_qq_q(V3Eval.I64_OR, asm.q.or_r_i, asm.q.or_r_m, asm.q.or_r_r); }
	def visit_I64_XOR() { visitSimpleOp2_qq_q(V3Eval.I64_XOR, asm.q.xor_r_i, asm.q.xor_r_m, asm.q.xor_r_r); }

	def visit_F32_SQRT() { do_op1_x_x(ValueKind.F32, asm.sqrtss_s_s); } // XXX: try s_m addressing mode
	def visit_F32_ADD() { do_op2_x_x(ValueKind.F32, asm.addss_s_s); } // XXX: try s_m addressing mode
	def visit_F32_SUB() { do_op2_x_x(ValueKind.F32, asm.subss_s_s); } // XXX: try s_m addressing mode
	def visit_F32_MUL() { do_op2_x_x(ValueKind.F32, asm.mulss_s_s); } // XXX: try s_m addressing mode
	def visit_F32_DIV() { do_op2_x_x(ValueKind.F32, asm.divss_s_s); } // XXX: try s_m addressing mode

	def visit_F64_SQRT() { do_op1_x_x(ValueKind.F64, asm.sqrtsd_s_s); } // XXX: try s_m addressing mode
	def visit_F64_ADD() { do_op2_x_x(ValueKind.F64, asm.addsd_s_s); } // XXX: try s_m addressing mode
	def visit_F64_SUB() { do_op2_x_x(ValueKind.F64, asm.subsd_s_s); } // XXX: try s_m addressing mode
	def visit_F64_MUL() { do_op2_x_x(ValueKind.F64, asm.mulsd_s_s); } // XXX: try s_m addressing mode
	def visit_F64_DIV() { do_op2_x_x(ValueKind.F64, asm.divsd_s_s); } // XXX: try s_m addressing mode

	def visitReinterpret(kind: ValueKind) {
		var sv = state.pop(), flags = SpcConsts.kindToFlags(kind);
		if (sv.isConst()) {
			state.push(flags | IS_CONST, NO_REG, sv.const);
		} else if (sv.inReg()) {
			var d = allocReg(kind);
			match (kind) {
				I32 => asm.movd_r_s(G(d.reg), X(sv.reg));
				I64 => asm.movq_r_s(G(d.reg), X(sv.reg));
				F32 => asm.movd_s_r(X(d.reg), G(sv.reg));
				F64 => asm.movq_s_r(X(d.reg), G(sv.reg));
				_ => bailout("unexpected value kind");
			}
			state.push(flags | IN_REG, d.reg, 0);
			freeVal(sv);
		} else {
			state.push(flags | IS_STORED, NO_REG, 0);
		}
	}
	def visit_I32_REINTERPRET_F32() { visitReinterpret(ValueKind.I32); }
	def visit_I64_REINTERPRET_F64() { visitReinterpret(ValueKind.I64); }
	def visit_F32_REINTERPRET_I32() { visitReinterpret(ValueKind.F32); }
	def visit_F64_REINTERPRET_I64() { visitReinterpret(ValueKind.F64); }
	def visit_I64_EXTEND_I32_S() {
		var x = tryFold_i_l(i64.view<i32>)
			|| do_op1_r(ValueKind.I64, mmasm.emit_i64_extend_i32_s);
	}
	def visit_I64_EXTEND_I32_U() {
		var x = tryFold_u_l(i64.view<u32>)
			|| do_op1_r(ValueKind.I64, mmasm.emit_i64_extend_i32_u);
	}
	def visit_F32_DEMOTE_F64() { do_op1_x_x(ValueKind.F32, asm.cvtsd2ss_s_s); } // XXX: try s_m addr mode
	def visit_F64_PROMOTE_F32() { do_op1_x_x(ValueKind.F64, asm.cvtss2sd_s_s); } // XXX: try s_m addr mode

	// r1 = op(r1)
	private def do_op1_r<T>(kind: ValueKind, emit: (X86_64Gpr -> T)) -> bool {
		var sv = popRegToOverwrite(), r = G(sv.reg);
		emit(r);
		state.push(sv.kindFlagsMatching(kind, IN_REG), sv.reg, 0);
		return true;
	}
	// r1 = op(r2)
	private def do_op1_r_r<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr) -> T) -> bool {
		var sv = popReg(), r = G(sv.reg);
		var d = allocReg(kind);
		emit(G(d.reg), r);
		state.push(sv.kindFlagsMatching(kind, IN_REG), d.reg, 0);
		freeVal(sv);
		return true;
	}
	// r1 = op(r1, r2)
	private def do_op2_r_r<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr) -> T) -> bool {
		var b = popReg();
		var a = popRegToOverwrite();
		emit(G(a.reg), G(b.reg));
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		freeVal(b);
		return true;
	}
	// r1 = op(r1, m2)
	private def do_op2_r_m<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Addr) -> T) -> bool {
		var b = state.peek();
		if (b.inReg() || b.isConst()) return false;
		state.pop();
		var addr = masm.slotAddr(state.sp);
		var a = popRegToOverwrite();
		emit(G(a.reg), A(addr));
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		return true;
	}
	// r1 = op(r1, imm)
	private def do_op2_r_i<T>(kind: ValueKind, emit: (X86_64Gpr, int) -> T) -> bool {
		var b = state.peek();
		if (!b.isConst()) return false;
		state.pop();
		var a = popRegToOverwrite();
		emit(G(a.reg), b.const);
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		return true;
	}
	// r1 = op(r2, imm)
	private def do_op2_r_r_i<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr, int) -> T) -> bool;
	// r1 = op(r2, m3)
	private def do_op2_r_r_m<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr, X86_64Addr) -> T) -> bool;
	// r1 = op(r2, r3)
	private def do_op2_r_r_r<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr, X86_64Gpr) -> T) -> bool;
	// r1 = op(r2)
	private def do_op1_x_x<T>(kind: ValueKind, emit: (X86_64Xmmr, X86_64Xmmr) -> T) -> bool {
		var sv = popReg(), r = X(sv.reg);
		var d = allocReg(kind);
		emit(X(d.reg), r);
		state.push(sv.kindFlagsMatching(kind, IN_REG), d.reg, 0);
		freeVal(sv);
		return true;
	}
	// x1 = op(x1, x2)
	private def do_op2_x_x<T>(kind: ValueKind, emit: (X86_64Xmmr, X86_64Xmmr) -> T) -> bool {
		var b = popReg();
		var a = popRegToOverwrite();
		emit(X(a.reg), X(b.reg));
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		freeVal(b);
		return true;
	}
}

def ucontext_rip_offset = 168;
def ucontext_rsp_offset = 160;
def SIGFPE  = 8;
def SIGBUS  = 10;
def SIGSEGV = 11;

// Implements the RiUserCode interface to add generated machine code to the V3 runtime.
// Handles stackwalking and signals in JITed code.
class X86_64SpcCode extends RiUserCode {
	def mapping: Mapping;
	def frameSize = IVarConfig.frameSize;
	var oobMemoryHandlerOffset: int;	// handler for signals caused by OOB memory access
	var divZeroHandlerOffset: int;		// handler for signals caused by divide by zero
	var stackOverflowHandlerOffset: int;	// handler for signals caused by (value- or call-) stack overflow
	var buf = StringBuilder.new().grow(128);  // avoid allocations when describing frames

	new(mapping) super(mapping.range.start, mapping.range.end) { }

	// Called from V3 runtime upon fatal errors to describe a frame for a stacktrace.
	def describeFrame(ip: Pointer, sp: Pointer, out: (Array<byte>, int, int) -> ()) {
		var msg = "\tin [spc-jit] ";
		out(msg, 0, msg.length);
		var instance = (sp + IVarConfig.frame.INSTANCE.disp).load<Instance>();
		var func = (sp + IVarConfig.frame.FUNC_DECL.disp).load<FuncDecl>();
		// TODO: lazy parse of names section may allocate; must avoid this in OOM situation
		func.render(instance.module.names, buf);
		buf.ln().out(out);
		buf.reset();
	}

	// Called from V3 runtime for a frame where {ip} is in interpreter code.
	def nextFrame(ip: Pointer, sp: Pointer) -> (Pointer, Pointer) {
		sp += frameSize;	 // assume frame is allocated
		ip = sp.load<Pointer>(); // return address on stack
		return (ip + -1, sp + Pointer.SIZE); // XXX: V3 quirk with -1 (use RiOs?)
	}

	// Called from V3 runtime when the garbage collector needs to scan a JIT stack frame.
	def scanFrame(ip: Pointer, sp: Pointer) {
		// Handle other roots in the frame
		RiGc.scanRoot(sp + IVarConfig.frame.FUNC_DECL.disp);
		RiGc.scanRoot(sp + IVarConfig.frame.INSTANCE.disp);
	}

	// Called from V3 runtime to handle an OS-level signal that occurred while {ip} was in JIT code.
	def handleSignal(signum: int, siginfo: Pointer, ucontext: Pointer, ip: Pointer, sp: Pointer) -> bool {
		var pip = ucontext + ucontext_rip_offset;
		var ip = pip.load<Pointer>();
		if (Trace.interpreter) {
			Trace.OUT.put2("  !signal %d in JIT code @ 0x%x", signum, ip - Pointer.NULL).outln();
		}
		match (signum) {
			SIGFPE => {
				// presume divide/modulus by zero
				pip.store<Pointer>(start + divZeroHandlerOffset);
				return true;
			}
			SIGBUS, SIGSEGV => {
				var addr = RiOs.getAccessAddress(siginfo, ucontext);
				if (RedZones.isInRedZone(addr)) {
					pip.store<Pointer>(start + stackOverflowHandlerOffset);
					return true;
				}
				pip.store<Pointer>(start + oobMemoryHandlerOffset);
				return true;
			}
		}
		return true;
	}
	def keepAlive() { // XXX: need to trick V3 whole-program optimizer to not delete these fields
		if (mapping == null) System.error(null, null);
		if (mapping.range == null) System.error(null, null);
	}
}
