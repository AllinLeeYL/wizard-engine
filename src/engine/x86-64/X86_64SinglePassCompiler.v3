// Copyright 2022 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

// XXX: reduce duplication with MacroAssembler
def G = X86_64MasmRegs.toGpr, X = X86_64MasmRegs.toXmmr;
def R: X86_64Regs;
def C: X86_64Conds;
def IVAR_FRAME = X86_64MasmRegs.IVAR_FRAME;
def A(ma: MasmAddr) -> X86_64Addr {
	return X86_64Addr.new(G(ma.base), null, 1, ma.offset);
}

// Shorten constants inside this file.
def NO_REG = SpcConsts.NO_REG;
def IS_STORED = SpcConsts.IS_STORED;
def IS_CONST = SpcConsts.IS_CONST;
def IN_REG = SpcConsts.IN_REG;
def TAG_STORED = SpcConsts.TAG_STORED;
def KIND_MASK = SpcConsts.KIND_MASK;
def KIND_I32 = SpcConsts.KIND_I32;
def KIND_I64 = SpcConsts.KIND_I64;
def KIND_F32 = SpcConsts.KIND_F32;
def KIND_F64 = SpcConsts.KIND_F64;
def KIND_V128 = SpcConsts.KIND_V128;
def KIND_REF = SpcConsts.KIND_REF;
def KIND_ABS = SpcConsts.KIND_ABS;

// Implements the target-specific parts of the single-pass compiler for X86-64.
class X86_64SinglePassCompiler extends SinglePassCompiler {
	def w = DataWriter.new();
	def mmasm = X86_64MacroAssembler.new(w, X86_64MasmRegs.CONFIG);
	def asm = mmasm.asm;

	new(extensions: Extension.set, limits: Limits, config: RegConfig)
		super(X86_64MasmRegs.SPC_EXEC_ENV, mmasm, X86_64MasmRegs.SPC_ALLOC.copy(), extensions, limits) {
		mmasm.trap_stubs = X86_64PreGenStubs.getSpcTrapsStub();
	}

	private def visitCompareI(asm: X86_64Assembler, cond: X86_64Cond) -> bool {
		var b = pop(), a = popReg();
		if (b.isConst()) asm.cmp_r_i(G(a.reg), b.const);
		else if (b.inReg()) asm.cmp_r_r(G(a.reg), G(b.reg));
		else asm.cmp_r_m(G(a.reg), A(masm.slotAddr(state.sp + 1)));
		var d = allocRegTos(ValueKind.I32), r1 = G(d);
		asm.set_r(cond, r1);
		asm.movbzx_r_r(r1, r1);
		state.push(KIND_I32 | IN_REG, d, 0);
		return true;
	}
	def visit_I32_EQZ() {
		state.push(KIND_I32 | IS_CONST, NO_REG, 0);
		visit_I32_EQ();
	}
	def visit_I32_EQ()   { void(tryFold_uu_z(V3Eval.I32_EQ)   || visitCompareI(asm.d, C.Z)); }
	def visit_I32_NE()   { void(tryFold_uu_z(V3Eval.I32_NE)   || visitCompareI(asm.d, C.NZ)); }
	def visit_I32_LT_S() { void(tryFold_ii_z(V3Eval.I32_LT_S) || visitCompareI(asm.d, C.L)); }
	def visit_I32_LT_U() { void(tryFold_uu_z(V3Eval.I32_LT_U) || visitCompareI(asm.d, C.C)); }
	def visit_I32_GT_S() { void(tryFold_ii_z(V3Eval.I32_GT_S) || visitCompareI(asm.d, C.G)); }
	def visit_I32_GT_U() { void(tryFold_uu_z(V3Eval.I32_GT_U) || visitCompareI(asm.d, C.A)); }
	def visit_I32_LE_S() { void(tryFold_ii_z(V3Eval.I32_LE_S) || visitCompareI(asm.d, C.LE)); }
	def visit_I32_LE_U() { void(tryFold_uu_z(V3Eval.I32_LE_U) || visitCompareI(asm.d, C.NA)); }
	def visit_I32_GE_S() { void(tryFold_ii_z(V3Eval.I32_GE_S) || visitCompareI(asm.d, C.GE)); }
	def visit_I32_GE_U() { void(tryFold_uu_z(V3Eval.I32_GE_U) || visitCompareI(asm.d, C.NC)); }

	def visit_I64_EQZ() {
		state.push(KIND_I64 | IS_CONST, NO_REG, 0);
		visit_I64_EQ();
	}
	def visit_I64_EQ()   { void(tryFold_qq_z(V3Eval.I64_EQ)   || visitCompareI(asm.q, C.Z)); }
	def visit_I64_NE()   { void(tryFold_qq_z(V3Eval.I64_NE)   || visitCompareI(asm.q, C.NZ)); }
	def visit_I64_LT_S() { void(tryFold_ll_z(V3Eval.I64_LT_S) || visitCompareI(asm.q, C.L)); }
	def visit_I64_LT_U() { void(tryFold_qq_z(V3Eval.I64_LT_U) || visitCompareI(asm.q, C.C)); }
	def visit_I64_GT_S() { void(tryFold_ll_z(V3Eval.I64_GT_S) || visitCompareI(asm.q, C.G)); }
	def visit_I64_GT_U() { void(tryFold_qq_z(V3Eval.I64_GT_U) || visitCompareI(asm.q, C.A)); }
	def visit_I64_LE_S() { void(tryFold_ll_z(V3Eval.I64_LE_S) || visitCompareI(asm.q, C.LE)); }
	def visit_I64_LE_U() { void(tryFold_qq_z(V3Eval.I64_LE_U) || visitCompareI(asm.q, C.NA)); }
	def visit_I64_GE_S() { void(tryFold_ll_z(V3Eval.I64_GE_S) || visitCompareI(asm.q, C.GE)); }
	def visit_I64_GE_U() { void(tryFold_qq_z(V3Eval.I64_GE_U) || visitCompareI(asm.q, C.NC)); }

	private def visitFloatCmp(emit_cmp: (X86_64Xmmr, X86_64Xmmr) -> X86_64Assembler, unordered_true: bool, cond: X86_64Cond) {
		var b = popReg(), a = popReg();
		var ret_zero = X86_64Label.new(), ret_one = X86_64Label.new(), done = X86_64Label.new();
		emit_cmp(X(a.reg), X(b.reg));
		asm.jc_rel_near(C.P, if(unordered_true, ret_one, ret_zero));
		asm.jc_rel_near(cond, ret_zero);
		var d = allocRegTos(ValueKind.I32);
		asm.bind(ret_one);
		asm.movd_r_i(G(d), 1);
		asm.jmp_rel_near(done);
		asm.bind(ret_zero);
		asm.movd_r_i(G(d), 0);
		asm.bind(done);
		state.push(KIND_I32 | IN_REG, d, 0);
	}
	def visit_F32_EQ() { visitFloatCmp(asm.ucomiss_s_s, false, C.NZ); }
	def visit_F32_NE() { visitFloatCmp(asm.ucomiss_s_s, true, C.Z); }
	def visit_F32_LT() { visitFloatCmp(asm.ucomiss_s_s, false, C.NC); }
	def visit_F32_GT() { visitFloatCmp(asm.ucomiss_s_s, false, C.NA); }
	def visit_F32_LE() { visitFloatCmp(asm.ucomiss_s_s, false, C.A); }
	def visit_F32_GE() { visitFloatCmp(asm.ucomiss_s_s, false, C.C); }

	def visit_F64_EQ() { visitFloatCmp(asm.ucomisd_s_s, false, C.NZ); }
	def visit_F64_NE() { visitFloatCmp(asm.ucomisd_s_s, true, C.Z); }
	def visit_F64_LT() { visitFloatCmp(asm.ucomisd_s_s, false, C.NC); }
	def visit_F64_GT() { visitFloatCmp(asm.ucomisd_s_s, false, C.NA); }
	def visit_F64_LE() { visitFloatCmp(asm.ucomisd_s_s, false, C.A); }
	def visit_F64_GE() { visitFloatCmp(asm.ucomisd_s_s, false, C.C); }

	def visit_REF_IS_NULL() {
		state.push(KIND_REF | IS_CONST, NO_REG, 0);
		visit_REF_EQ();
	}
	def visit_I32_CLZ() {
		void(tryFold_u_u(V3Eval.I32_CLZ)
			|| do_op1_r_r(ValueKind.I32, mmasm.emit_i32_clz_r_r));
	}
	def visit_I32_CTZ() {
		void(tryFold_u_u(V3Eval.I32_CTZ)
			|| do_op1_r_r(ValueKind.I32, mmasm.emit_i32_ctz_r_r));
	}
	def visit_I32_POPCNT() {
		void(tryFold_u_u(V3Eval.I32_POPCNT)
			|| do_op1_r_r(ValueKind.I32, asm.d.popcnt_r_r));
	}

	private def visitSimpleOp2_uu_u(fold: (u32, u32) -> u32,
		emit_r_i: (X86_64Gpr, int) -> X86_64Assembler,
		emit_r_m: (X86_64Gpr, X86_64Addr) -> X86_64Assembler,
		emit_r_r: (X86_64Gpr, X86_64Gpr) -> X86_64Assembler) {
		void(tryFold_uu_u(fold)
			|| do_op2_r_i(ValueKind.I32, emit_r_i)
			|| do_op2_r_m(ValueKind.I32, emit_r_m)
			|| do_op2_r_r(ValueKind.I32, emit_r_r));
	}
	def visit_I32_ADD() { visitSimpleOp2_uu_u(V3Eval.I32_ADD, asm.d.add_r_i, asm.d.add_r_m, asm.d.add_r_r); }
	def visit_I32_SUB() { visitSimpleOp2_uu_u(V3Eval.I32_SUB, asm.d.sub_r_i, asm.d.sub_r_m, asm.d.sub_r_r); }
	def visit_I32_MUL() { visitSimpleOp2_uu_u(V3Eval.I32_MUL, asm.d.imul_r_i, asm.d.imul_r_m, asm.d.imul_r_r); }

	private def visitIDivRem(dst: Reg, emit: X86_64Gpr -> void) {
		var b = popFixedReg(X86_64MasmRegs.RCX); // XXX: use any reg except RAX or RDX
		var a = popFixedReg(X86_64MasmRegs.RAX);
		spillReg(X86_64MasmRegs.RAX);
		spillReg(X86_64MasmRegs.RDX);
		emit(G(b.reg));
		regAlloc.assign(dst, int.!(state.sp));
		state.push(a.kindFlagsAndTag(IN_REG), dst, 0);
	}
	def visit_I32_DIV_S() { visitIDivRem(X86_64MasmRegs.RAX, mmasm.emit_i32_div_s); } // XXX: fold potentially-trapping div/rem
	def visit_I32_DIV_U() { visitIDivRem(X86_64MasmRegs.RAX, mmasm.emit_i32_div_u); }
	def visit_I32_REM_S() { visitIDivRem(X86_64MasmRegs.RDX, mmasm.emit_i32_rem_s); }
	def visit_I32_REM_U() { visitIDivRem(X86_64MasmRegs.RDX, mmasm.emit_i32_rem_u); }

	private def visitShift(kind: ValueKind, emit_r_i: (X86_64Gpr, u6) -> X86_64Assembler, emit_r_cl: X86_64Gpr -> X86_64Assembler) -> bool {
		var b = state.peek(), a: SpcVal;
		if (b.isConst()) {
			pop();
			a = popRegToOverwrite();
			emit_r_i(G(a.reg), u6.view(b.const));
		} else {
			b = popFixedReg(X86_64MasmRegs.RCX);
			a = popRegToOverwrite();
			emit_r_cl(G(a.reg));
		}
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, a.reg, 0); // XXX: tag preservation
		return true;
	}
	def visit_I32_SHL()   { void(tryFold_ii_i(V3Eval.I32_SHL) || visitShift(ValueKind.I32, asm.d.shl_r_i, asm.d.shl_r_cl)); }
	def visit_I32_SHR_S() { void(tryFold_ii_i(V3Eval.I32_SHR_S) || visitShift(ValueKind.I32, asm.d.sar_r_i, asm.d.sar_r_cl)); }
	def visit_I32_SHR_U() { void(tryFold_ii_i(V3Eval.I32_SHR_U) || visitShift(ValueKind.I32, asm.d.shr_r_i, asm.d.shr_r_cl)); }
	def visit_I32_ROTL()  { void(tryFold_uu_u(V3Eval.I32_ROTL) || visitShift(ValueKind.I32, asm.d.rol_r_i, asm.d.rol_r_cl)); }
	def visit_I32_ROTR()  { void(tryFold_uu_u(V3Eval.I32_ROTR) || visitShift(ValueKind.I32, asm.d.ror_r_i, asm.d.ror_r_cl)); }
	def visit_I32_AND() { visitSimpleOp2_uu_u(V3Eval.I32_AND, asm.d.and_r_i, asm.d.and_r_m, asm.d.and_r_r); }
	def visit_I32_OR()  { visitSimpleOp2_uu_u(V3Eval.I32_OR, asm.d.or_r_i, asm.d.or_r_m, asm.d.or_r_r); }
	def visit_I32_XOR() { visitSimpleOp2_uu_u(V3Eval.I32_XOR, asm.d.xor_r_i, asm.d.xor_r_m, asm.d.xor_r_r); }

	def visit_I64_CLZ() {
		void(tryFold_q_q(V3Eval.I64_CLZ)
			|| do_op1_r_r(ValueKind.I64, mmasm.emit_i64_clz_r_r));
	}
	def visit_I64_CTZ() {
		void(tryFold_q_q(V3Eval.I64_CTZ)
			|| do_op1_r_r(ValueKind.I64, mmasm.emit_i64_ctz_r_r));
	}
	def visit_I64_POPCNT() {
		void(tryFold_q_q(V3Eval.I64_POPCNT)
			|| do_op1_r_r(ValueKind.I64, asm.q.popcnt_r_r));
	}
	private def visitSimpleOp2_qq_q(fold: (u64, u64) -> u64,
		emit_r_i: (X86_64Gpr, int) -> X86_64Assembler,
		emit_r_m: (X86_64Gpr, X86_64Addr) -> X86_64Assembler,
		emit_r_r: (X86_64Gpr, X86_64Gpr) -> X86_64Assembler) {
		void(tryFold_qq_q(fold)
			|| do_op2_r_i(ValueKind.I64, emit_r_i)
			|| do_op2_r_m(ValueKind.I64, emit_r_m)
			|| do_op2_r_r(ValueKind.I64, emit_r_r));
	}
	def visit_I64_ADD() { visitSimpleOp2_qq_q(V3Eval.I64_ADD, asm.q.add_r_i, asm.q.add_r_m, asm.q.add_r_r); }
	def visit_I64_SUB() { visitSimpleOp2_qq_q(V3Eval.I64_SUB, asm.q.sub_r_i, asm.q.sub_r_m, asm.q.sub_r_r); }
	def visit_I64_MUL() { visitSimpleOp2_qq_q(V3Eval.I64_MUL, asm.q.imul_r_i, asm.q.imul_r_m, asm.q.imul_r_r); }
	def visit_I64_DIV_S() { visitIDivRem(X86_64MasmRegs.RAX, mmasm.emit_i64_div_s); }
	def visit_I64_DIV_U() { visitIDivRem(X86_64MasmRegs.RAX, mmasm.emit_i64_div_u); }
	def visit_I64_REM_S() { visitIDivRem(X86_64MasmRegs.RDX, mmasm.emit_i64_rem_s); }
	def visit_I64_REM_U() { visitIDivRem(X86_64MasmRegs.RDX, mmasm.emit_i64_rem_u); }
	def visit_I64_SHL()   { void(tryFold_qq_q(V3Eval.I64_SHL) || visitShift(ValueKind.I64, asm.q.shl_r_i, asm.q.shl_r_cl)); }
	def visit_I64_SHR_S() { void(tryFold_ll_l(V3Eval.I64_SHR_S) || visitShift(ValueKind.I64, asm.q.sar_r_i, asm.q.sar_r_cl)); }
	def visit_I64_SHR_U() { void(tryFold_qq_q(V3Eval.I64_SHR_U) || visitShift(ValueKind.I64, asm.q.shr_r_i, asm.q.shr_r_cl)); }
	def visit_I64_ROTL()  { void(tryFold_qq_q(V3Eval.I64_ROTL) || visitShift(ValueKind.I64, asm.q.rol_r_i, asm.q.rol_r_cl)); }
	def visit_I64_ROTR()  { void(tryFold_qq_q(V3Eval.I64_ROTR) || visitShift(ValueKind.I64, asm.q.ror_r_i, asm.q.ror_r_cl)); }
	def visit_I64_AND() { visitSimpleOp2_qq_q(V3Eval.I64_AND, asm.q.and_r_i, asm.q.and_r_m, asm.q.and_r_r); }
	def visit_I64_OR()  { visitSimpleOp2_qq_q(V3Eval.I64_OR, asm.q.or_r_i, asm.q.or_r_m, asm.q.or_r_r); }
	def visit_I64_XOR() { visitSimpleOp2_qq_q(V3Eval.I64_XOR, asm.q.xor_r_i, asm.q.xor_r_m, asm.q.xor_r_r); }

	// XXX: try s_m addressing mode for floating point ops
	def visit_F32_ABS() {
		var sv = popReg(), r = X(sv.reg), scratch = mmasm.scratch;
		var d = allocRegTos(ValueKind.F32);
		asm.movd_r_s(scratch, X(sv.reg));
		asm.d.and_r_i(scratch, 0x7FFFFFFF);
		asm.movd_s_r(X(d), scratch);
		state.push(sv.kindFlagsAndTag(IN_REG), d, 0);
	}
	def visit_F32_NEG() {
		var sv = popReg(), r = X(sv.reg), scratch = mmasm.scratch;
		var d = allocRegTos(ValueKind.F32);
		asm.movd_r_s(scratch, X(sv.reg));
		asm.d.xor_r_i(scratch, 0x80000000);
		asm.movd_s_r(X(d), scratch);
		state.push(sv.kindFlagsAndTag(IN_REG), d, 0);
	}
	def visit_F32_CEIL() { do_op1_x_x(ValueKind.F32, asm.roundss_s_s(_, _, X86_64Rounding.TO_POS_INF)); }
	def visit_F32_FLOOR() { do_op1_x_x(ValueKind.F32, asm.roundss_s_s(_, _, X86_64Rounding.TO_NEG_INF)); }
	def visit_F32_TRUNC() { do_op1_x_x(ValueKind.F32, asm.roundss_s_s(_, _, X86_64Rounding.TO_ZERO)); }
	def visit_F32_NEAREST() { do_op1_x_x(ValueKind.F32, asm.roundss_s_s(_, _, X86_64Rounding.TO_NEAREST)); }
	def visit_F32_SQRT() { do_op1_x_x(ValueKind.F32, asm.sqrtss_s_s); }
	def visit_F32_ADD() { do_op2_x_x(ValueKind.F32, asm.addss_s_s); }
	def visit_F32_SUB() { do_op2_x_x(ValueKind.F32, asm.subss_s_s); }
	def visit_F32_MUL() { do_op2_x_x(ValueKind.F32, asm.mulss_s_s); }
	def visit_F32_DIV() { do_op2_x_x(ValueKind.F32, asm.divss_s_s); }

	def visitFloatMinOrMax(is64: bool, isMin: bool) { // XXX: move to macro assembler?
		var b = popReg(), a = popRegToOverwrite();
		var ret_a = X86_64Label.new(), ret_b = X86_64Label.new(), is_nan = X86_64Label.new();
		var done = X86_64Label.new();
		var xmmA = X(a.reg), xmmB = X(b.reg);

		if (is64) asm.ucomisd_s_s(xmmA, xmmB);
		else asm.ucomiss_s_s(xmmA, xmmB);
		asm.jc_rel_far(C.P, is_nan);
		asm.jc_rel_near(C.C, if(isMin, ret_a, ret_b));
		asm.jc_rel_near(C.A, if(isMin, ret_b, ret_a));
		asm.movq_r_s(mmasm.scratch, xmmB); // XXX: does a 32-bit move save anything?
		if (is64) asm.q.cmp_r_i(mmasm.scratch, 0);
		else asm.d.cmp_r_i(mmasm.scratch, 0);
		asm.jc_rel_near(if(isMin, C.S, C.NS), ret_b); // handle min(-0, 0) == -0
		asm.jmp_rel_near(ret_a);

		asm.bind(is_nan);
		if (is64) {
			masm.emit_mov_r_d64(a.reg, Floats.d_nan);
		} else {
			masm.emit_mov_r_f32(a.reg, Floats.f_nan);
		}
		asm.jmp_rel_near(done);

		asm.bind(ret_b);
		if (is64) asm.movsd_s_s(xmmA, xmmB); // fallthru
		else asm.movss_s_s(xmmA, xmmB); // fallthru
		asm.bind(ret_a); // nop
		asm.bind(done);

		state.push(a.kindFlagsAndTag(IN_REG), a.reg, 0);
	}
	def visit_F32_MIN() { visitFloatMinOrMax(false, true); }
	def visit_F32_MAX() { visitFloatMinOrMax(false, false); }
	def visit_F32_COPYSIGN() {
		var sv = popReg(), d = popRegToOverwrite(); // XXX: constant-fold and strength reduce
		var t1 = allocTmp(ValueKind.I32), t2 = allocTmp(ValueKind.I32);
		mmasm.emit_f32_copysign(X(d.reg), X(sv.reg), G(t1), G(t2));
		state.push(d.kindFlagsAndTag(IN_REG), d.reg, 0);
	}

	// XXX: try s_m addressing mode for floating point ops
	def visit_F64_ABS() {
		var sv = popRegToOverwrite();
		var t = allocTmp(ValueKind.I64), r1 = G(t);
		asm.movq_r_s(r1, X(sv.reg));
		asm.q.rol_r_i(r1, 1);
		asm.q.and_r_i(r1, -2);
		asm.q.ror_r_i(r1, 1);
		asm.movq_s_r(X(sv.reg), G(t));
		state.push(sv.kindFlagsAndTag(IN_REG), sv.reg, 0);
	}
	def visit_F64_NEG() {
		var sv = popRegToOverwrite();
		var t = allocTmp(ValueKind.I64), r1 = G(t);
		asm.movq_r_s(r1, X(sv.reg));
		asm.q.rol_r_i(r1, 1);
		asm.q.xor_r_i(r1, 1);
		asm.q.ror_r_i(r1, 1);
		asm.movq_s_r(X(sv.reg), G(t));
		state.push(sv.kindFlagsAndTag(IN_REG), sv.reg, 0);
	}
	def visit_F64_CEIL() { do_op1_x_x(ValueKind.F64, asm.roundsd_s_s(_, _, X86_64Rounding.TO_POS_INF)); }
	def visit_F64_FLOOR() { do_op1_x_x(ValueKind.F64, asm.roundsd_s_s(_, _, X86_64Rounding.TO_NEG_INF)); }
	def visit_F64_TRUNC() { do_op1_x_x(ValueKind.F64, asm.roundsd_s_s(_, _, X86_64Rounding.TO_ZERO)); }
	def visit_F64_NEAREST() { do_op1_x_x(ValueKind.F64, asm.roundsd_s_s(_, _, X86_64Rounding.TO_NEAREST)); }
	def visit_F64_SQRT() { do_op1_x_x(ValueKind.F64, asm.sqrtsd_s_s); }
	def visit_F64_ADD() { do_op2_x_x(ValueKind.F64, asm.addsd_s_s); }
	def visit_F64_SUB() { do_op2_x_x(ValueKind.F64, asm.subsd_s_s); }
	def visit_F64_MUL() { do_op2_x_x(ValueKind.F64, asm.mulsd_s_s); }
	def visit_F64_DIV() { do_op2_x_x(ValueKind.F64, asm.divsd_s_s); }
	def visit_F64_MIN() { visitFloatMinOrMax(true, true); }
	def visit_F64_MAX() { visitFloatMinOrMax(true, false); }
	def visit_F64_COPYSIGN() {
		var sv = popReg(), d = popRegToOverwrite(); // XXX: constant-fold and strength reduce
		var t1 = allocTmp(ValueKind.I64), t2 = allocTmp(ValueKind.I64);
		mmasm.emit_f64_copysign(X(d.reg), X(sv.reg), G(t1), G(t2));
		state.push(d.kindFlagsAndTag(IN_REG), d.reg, 0);
	}

	private def visitITruncF(opcode: Opcode) {
		var x1 = popReg();
		spillReg(x1.reg); // XXX: macro assembler routine overwrites x1
		var xkind = if(opcode.sig.params[0] == ValueType.F32, ValueKind.F32, ValueKind.F64);
		var dkind = if(opcode.sig.results[0] == ValueType.I32, ValueKind.I32, ValueKind.I64);
		var xscratch = allocTmp(xkind);
		var d = allocRegTos(dkind);
		mmasm.emit_i_trunc_f(opcode, G(d), X(x1.reg), X(xscratch));
		state.push(SpcConsts.kindToFlags(dkind) | IN_REG, d, 0);
	}

	def visit_I32_WRAP_I64() {
		void(tryFold_l_i(i32.view<i64>)
			|| do_op1_r_r(ValueKind.I32, asm.movd_r_r));
	}
	def visit_I32_TRUNC_F32_S() { visitITruncF(Opcode.I32_TRUNC_F32_S); }
	def visit_I32_TRUNC_F32_U() { visitITruncF(Opcode.I32_TRUNC_F32_U); }
	def visit_I32_TRUNC_F64_S() { visitITruncF(Opcode.I32_TRUNC_F64_S); }
	def visit_I32_TRUNC_F64_U() { visitITruncF(Opcode.I32_TRUNC_F64_U); }
	def visit_I64_EXTEND_I32_S() {
		void(tryFold_i_l(i64.view<i32>)
			|| do_op1_r(ValueKind.I64, mmasm.emit_i64_extend_i32_s));
	}
	def visit_I64_TRUNC_F32_S() { visitITruncF(Opcode.I64_TRUNC_F32_S); }
	def visit_I64_TRUNC_F32_U() { visitITruncF(Opcode.I64_TRUNC_F32_U); }
	def visit_I64_TRUNC_F64_S() { visitITruncF(Opcode.I64_TRUNC_F64_S); }
	def visit_I64_TRUNC_F64_U() { visitITruncF(Opcode.I64_TRUNC_F64_U); }
	def visit_I64_EXTEND_I32_U() {
		void(tryFold_u_l(i64.view<u32>)
			|| do_op1_r(ValueKind.I64, mmasm.emit_i64_extend_i32_u));
	}
	def visitReinterpret(kind: ValueKind) {
		var sv = pop(), flags = SpcConsts.kindToFlags(kind);
		if (sv.isConst()) {
			state.push(flags | IS_CONST, NO_REG, sv.const);
		} else if (sv.inReg()) {
			var d = allocRegTos(kind);
			match (kind) {
				I32 => asm.movd_r_s(G(d), X(sv.reg));
				I64 => asm.movq_r_s(G(d), X(sv.reg));
				F32 => asm.movd_s_r(X(d), G(sv.reg));
				F64 => asm.movq_s_r(X(d), G(sv.reg));
				_ => bailout("unexpected value kind");
			}
			state.push(flags | IN_REG, d, 0);
		} else {
			state.push(flags | IS_STORED, NO_REG, 0);
		}
	}
	def visit_I32_REINTERPRET_F32() { visitReinterpret(ValueKind.I32); }
	def visit_I64_REINTERPRET_F64() { visitReinterpret(ValueKind.I64); }
	def visit_F32_REINTERPRET_I32() { visitReinterpret(ValueKind.F32); }
	def visit_F64_REINTERPRET_I64() { visitReinterpret(ValueKind.F64); }

	def visit_I32_EXTEND8_S() {
		void(tryFold_i_i(V3Eval.I32_EXTEND8_S)
			|| do_op1_r_r(ValueKind.I32, asm.d.movbsx_r_r));
	}
	def visit_I32_EXTEND16_S() {
		void(tryFold_i_i(V3Eval.I32_EXTEND16_S)
			|| do_op1_r_r(ValueKind.I32, asm.d.movwsx_r_r));
	}
	def visit_I64_EXTEND8_S() {
		void(tryFold_l_l(V3Eval.I64_EXTEND8_S)
			|| do_op1_r_r(ValueKind.I64, asm.q.movbsx_r_r));
	}
	def visit_I64_EXTEND16_S() {
		void(tryFold_l_l(V3Eval.I64_EXTEND16_S)
			|| do_op1_r_r(ValueKind.I64, asm.q.movwsx_r_r));
	}
	def visit_I64_EXTEND32_S() {
		void(tryFold_l_l(V3Eval.I64_EXTEND32_S)
			|| do_op1_r(ValueKind.I64, mmasm.emit_i64_extend_i32_s));
	}
	def visit_REF_EQ()   { visitCompareI(asm.q, C.Z); }

	def visit_I32_TRUNC_SAT_F32_S() { visitITruncF(Opcode.I32_TRUNC_SAT_F32_S); }
	def visit_I32_TRUNC_SAT_F32_U() { visitITruncF(Opcode.I32_TRUNC_SAT_F32_U); }
	def visit_I32_TRUNC_SAT_F64_S() { visitITruncF(Opcode.I32_TRUNC_SAT_F64_S); }
	def visit_I32_TRUNC_SAT_F64_U() { visitITruncF(Opcode.I32_TRUNC_SAT_F64_U); }
	def visit_I64_TRUNC_SAT_F32_S() { visitITruncF(Opcode.I64_TRUNC_SAT_F32_S); }
	def visit_I64_TRUNC_SAT_F32_U() { visitITruncF(Opcode.I64_TRUNC_SAT_F32_U); }
	def visit_I64_TRUNC_SAT_F64_S() { visitITruncF(Opcode.I64_TRUNC_SAT_F64_S); }
	def visit_I64_TRUNC_SAT_F64_U() { visitITruncF(Opcode.I64_TRUNC_SAT_F64_U); }

	private def visitFConvertI32S(kind: ValueKind, emit_cvt: (X86_64Xmmr, X86_64Gpr) -> X86_64Assembler) {
		var sv = popReg(), r1 = G(sv.reg); // TODO: register killed
		var d = allocRegTos(kind);
		asm.q.shl_r_i(r1, 32);
		asm.q.sar_r_i(r1, 32); // sign-extend
		emit_cvt(X(d), r1);
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}
	def visit_F32_CONVERT_I32_S() { visitFConvertI32S(ValueKind.F32, asm.cvtsi2ss_s_r); }
	def visit_F64_CONVERT_I32_S() { visitFConvertI32S(ValueKind.F64, asm.cvtsi2sd_s_r); }
	private def visitFConvertI32U(kind: ValueKind, emit_cvt: (X86_64Xmmr, X86_64Gpr) -> X86_64Assembler) {
		var sv = popReg(), r1 = G(sv.reg);
		var d = allocRegTos(kind);
		asm.movd_r_r(r1, r1); // zero-extend
		emit_cvt(X(d), r1);
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}
	def visit_F32_CONVERT_I32_U() { visitFConvertI32U(ValueKind.F32, asm.cvtsi2ss_s_r); }
	def visit_F64_CONVERT_I32_U() { visitFConvertI32U(ValueKind.F64, asm.cvtsi2sd_s_r); }
	private def visitFConvertI64S(kind: ValueKind, emit_cvt: (X86_64Xmmr, X86_64Gpr) -> X86_64Assembler) {
		var sv = popReg(), r1 = G(sv.reg);
		var d = allocRegTos(kind);
		emit_cvt(X(d), r1);
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}
	private def visitFConvertI64U(kind: ValueKind, emit_cvt: (X86_64Xmmr, X86_64Gpr, X86_64Xmmr, X86_64Gpr) -> void) {
		var sv = popReg(), r1 = G(sv.reg);
		var d = allocRegTos(kind);
		var xscratch = allocTmp(kind);
		emit_cvt(X(d), r1, X(xscratch), mmasm.scratch);
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}
	def visit_F32_CONVERT_I64_S() { visitFConvertI64S(ValueKind.F32, asm.cvtsi2ss_s_r); }
	def visit_F32_CONVERT_I64_U() { visitFConvertI64U(ValueKind.F32, mmasm.emit_f32_convert_i64_u); }
	def visit_F64_CONVERT_I64_S() { visitFConvertI64S(ValueKind.F64, asm.cvtsi2sd_s_r); }
	def visit_F64_CONVERT_I64_U() { visitFConvertI64U(ValueKind.F64, mmasm.emit_f64_convert_i64_u); }

	def visit_F32_DEMOTE_F64() { do_op1_x_x(ValueKind.F32, asm.cvtsd2ss_s_s); } // XXX: try s_m addr mode
	def visit_F64_PROMOTE_F32() { do_op1_x_x(ValueKind.F64, asm.cvtss2sd_s_s); } // XXX: try s_m addr mode

	def visit_V128_AND() {do_op2_x_x(ValueKind.V128, asm.andps_s_s); }
	def visit_V128_OR() { do_op2_x_x(ValueKind.V128, asm.orps_s_s); }
	def visit_V128_XOR() { do_op2_x_x(ValueKind.V128, asm.xorps_s_s); }
	
	def visit_V128_NOT() {
		var sv = popRegToOverwrite(), r = X(sv.reg);
		var tmp = allocTmp(ValueKind.V128), t = X(tmp);
		mmasm.emit_v128_not(r, t);
		state.push(sv.kindFlagsMatching(ValueKind.V128, IN_REG), sv.reg, 0);
	}

	def visit_V128_ANDNOT() {
		var b = popRegToOverwrite();
		var a = popReg();	
		asm.andnps_s_s(X(b.reg), X(a.reg));
		state.push(b.kindFlagsMatching(ValueKind.V128, IN_REG), b.reg, 0);
	}

	def visit_I8X16_ADD() { do_op2_x_x(ValueKind.V128, asm.paddb_s_s); }
	def visit_I8X16_SUB() { do_op2_x_x(ValueKind.V128, asm.psubb_s_s); }
	def visit_I8X16_NEG() { visit_V128_I_NEG(mmasm.emit_i8x16_neg); }
	def visit_I8X16_EQ() { do_op2_x_x(ValueKind.V128, asm.pcmpeqb_s_s); }
	def visit_I8X16_NE() { do_op2_x_x(ValueKind.V128, mmasm.emit_i8x16_ne); }
	def visit_I8X16_GT_S() { do_op2_x_x(ValueKind.V128, asm.pcmpgtb_s_s); }
	def visit_I8X16_GT_U() { do_op2_x_x(ValueKind.V128, mmasm.emit_i8x16_gt_u(_, _, X(allocTmp(ValueKind.V128)))); }
	def visit_I8X16_LT_S() { do_c_op2_x_x(ValueKind.V128, asm.pcmpgtb_s_s); }
	def visit_I8X16_LT_U() { do_c_op2_x_x(ValueKind.V128, mmasm.emit_i8x16_gt_u(_, _, X(allocTmp(ValueKind.V128)))); }

	def visit_I16X8_ADD() { do_op2_x_x(ValueKind.V128, asm.paddw_s_s); }
	def visit_I16X8_SUB() { do_op2_x_x(ValueKind.V128, asm.psubw_s_s); }
	def visit_I16X8_MUL() { do_op2_x_x(ValueKind.V128, asm.pmullw_s_s); }
	def visit_I16X8_NEG() { visit_V128_I_NEG(mmasm.emit_i16x8_neg); }

	def visit_I32X4_ADD() { do_op2_x_x(ValueKind.V128, asm.paddd_s_s); }
	def visit_I32X4_SUB() { do_op2_x_x(ValueKind.V128, asm.psubd_s_s); }
	def visit_I32X4_MUL() { do_op2_x_x(ValueKind.V128, asm.pmulld_s_s); }
	def visit_I32X4_NEG() { visit_V128_I_NEG(mmasm.emit_i32x4_neg); }

	def visit_I64X2_ADD() { do_op2_x_x(ValueKind.V128, asm.paddq_s_s); }
	def visit_I64X2_SUB() { do_op2_x_x(ValueKind.V128, asm.psubq_s_s); }
	def visit_I64X2_MUL() {
		var b = popReg();
		var a = popRegToOverwrite();
		var t1 = allocTmp(ValueKind.V128);
		var t2 = allocTmp(ValueKind.V128);
		mmasm.emit_i64x2_mul(X(a.reg), X(b.reg), X(t1), X(t2));
		state.push(a.kindFlagsMatching(ValueKind.V128, IN_REG), a.reg, 0);
	}
	def visit_I64X2_NEG() { visit_V128_I_NEG(mmasm.emit_i64x2_neg); }

	def visit_F32X4_ADD() { do_op2_x_x(ValueKind.V128, asm.addps_s_s); }
	def visit_F32X4_SUB() { do_op2_x_x(ValueKind.V128, asm.subps_s_s); }
	def visit_F32X4_MUL() { do_op2_x_x(ValueKind.V128, asm.mulps_s_s); }
	def visit_F32X4_DIV() { do_op2_x_x(ValueKind.V128, asm.divps_s_s); }
	def visit_F32X4_NEG() { visit_V128_F_NEG_ABS(mmasm.emit_v128_negps); }
	def visit_F32X4_SQRT() { do_op1_x_x(ValueKind.V128, asm.sqrtps_s_s); }

	def visit_F64X2_ADD() { do_op2_x_x(ValueKind.V128, asm.addpd_s_s); }
	def visit_F64X2_SUB() { do_op2_x_x(ValueKind.V128, asm.subpd_s_s); }
	def visit_F64X2_MUL() { do_op2_x_x(ValueKind.V128, asm.mulpd_s_s); }
	def visit_F64X2_DIV() { do_op2_x_x(ValueKind.V128, asm.divpd_s_s); }
	def visit_F64X2_NEG() { visit_V128_F_NEG_ABS(mmasm.emit_v128_negpd); }
	def visit_F64X2_SQRT() { do_op1_x_x(ValueKind.V128, asm.sqrtpd_s_s); }

	def visit_V128_BITSELECT() {
		var c = popReg();
		var b = popReg();
		var a = popRegToOverwrite();
		var t = allocTmp(ValueKind.V128);
		mmasm.emit_v128_bitselect(X(a.reg), X(b.reg), X(c.reg), X(t));
		state.push(a.kindFlagsMatching(ValueKind.V128, IN_REG), a.reg, 0);
	}

	private def visit_V128_F_NEG_ABS<T>(emit: (X86_64Xmmr, X86_64Gpr, X86_64Xmmr) -> T) -> void {
		var sv = popRegToOverwrite();
		var tmp = allocTmp(ValueKind.I64);
		var mask = allocTmp(ValueKind.V128);
		emit(X(sv.reg), G(tmp), X(mask));
		state.push(sv.kindFlagsMatching(ValueKind.V128, IN_REG), sv.reg, 0);
	}

	private def visit_V128_I_NEG<T>(emit: (X86_64Xmmr, X86_64Xmmr) -> T) -> void {
		var sv = popReg(), r = X(sv.reg);
		var tmp = allocTmp(ValueKind.V128), t = X(tmp);
		emit(r, t);
		state.push(sv.kindFlagsMatching(ValueKind.V128, IN_REG), sv.reg, 0);
	}

	// r1 = op(r1)
	private def do_op1_r<T>(kind: ValueKind, emit: (X86_64Gpr -> T)) -> bool {
		var sv = popRegToOverwrite(), r = G(sv.reg);
		emit(r);
		state.push(sv.kindFlagsMatching(kind, IN_REG), sv.reg, 0);
		return true;
	}
	// r1 = op(r2)
	private def do_op1_r_r<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr) -> T) -> bool {
		var sv = popReg(), r = G(sv.reg);
		var d = allocRegTos(kind);
		emit(G(d), r);
		state.push(sv.kindFlagsMatching(kind, IN_REG), d, 0);
		return true;
	}
	// r1 = op(r1, r2)
	private def do_op2_r_r<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr) -> T) -> bool {
		var b = popReg();
		var a = popRegToOverwrite();
		emit(G(a.reg), G(b.reg));
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		return true;
	}
	// r1 = op(r1, m2)
	private def do_op2_r_m<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Addr) -> T) -> bool {
		var b = state.peek();
		if (b.inReg() || b.isConst()) return false;
		state.pop();
		var addr = masm.slotAddr(state.sp);
		var a = popRegToOverwrite();
		emit(G(a.reg), A(addr));
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		return true;
	}
	// r1 = op(r1, imm)
	private def do_op2_r_i<T>(kind: ValueKind, emit: (X86_64Gpr, int) -> T) -> bool {
		var b = state.peek();
		if (!b.isConst()) return false;
		pop();
		var a = popRegToOverwrite();
		emit(G(a.reg), b.const);
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		return true;
	}
	// r1 = op(r2, imm)
	private def do_op2_r_r_i<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr, int) -> T) -> bool;
	// r1 = op(r2, m3)
	private def do_op2_r_r_m<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr, X86_64Addr) -> T) -> bool;
	// r1 = op(r2, r3)
	private def do_op2_r_r_r<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr, X86_64Gpr) -> T) -> bool;
	// r1 = op(r2)
	private def do_op1_x_x<T>(kind: ValueKind, emit: (X86_64Xmmr, X86_64Xmmr) -> T) -> bool {
		var sv = popReg(), r = X(sv.reg);
		var d = allocRegTos(kind);
		emit(X(d), r);
		state.push(sv.kindFlagsMatching(kind, IN_REG), d, 0);
		return true;
	}
	// x1 = op(x1, x2)
	private def do_op2_x_x<T>(kind: ValueKind, emit: (X86_64Xmmr, X86_64Xmmr) -> T) -> bool {
		var b = popReg();
		var a = popRegToOverwrite();
		emit(X(a.reg), X(b.reg));
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		return true;
	}
	// x2 = op(x2, x1), commuted version of do_op2_x_x
	private def do_c_op2_x_x<T>(kind: ValueKind, emit: (X86_64Xmmr, X86_64Xmmr) -> T) -> bool {
		var b = popRegToOverwrite();
		var a = popReg();
		emit(X(b.reg), X(a.reg));
		state.push(b.kindFlagsMatching(kind, IN_REG), b.reg, 0);
		return true;
	}
}

def ucontext_rip_offset = 168;
def SIGFPE  = 8;
def SIGBUS  = 10;
def SIGSEGV = 11;
def globalBuf = StringBuilder.new().grow(256);  // avoid allocations when describing frames

// Represents SPC code that uses complete frames (both trapping code and module code).
// Implements the RiUserCode interface to add generated machine code to the V3 runtime.
// Handles stackwalking and signals in JITed code.
class X86_64SpcCode extends RiUserCode {
	def name: string;

	new(name, start: Pointer, end: Pointer) super(start, end) { }

	// V3-runtime callback: upon fatal errors to describe a frame for a stacktrace.
	def describeFrame(ip: Pointer, sp: Pointer, out: (Array<byte>, int, int) -> ()) {
		var msg: string;
		msg = "\tin [";
		out(msg, 0, msg.length);
		msg = name;
		out(msg, 0, msg.length);
		msg = "] ";
		out(msg, 0, msg.length);
		var instance = (sp + IVAR_FRAME.INSTANCE.disp).load<Instance>();
		var wf = (sp + IVAR_FRAME.WASM_FUNC.disp).load<WasmFunction>();
		// TODO: lazy parse of names section may allocate; must avoid this in OOM situation
		var buf = globalBuf;
		wf.decl.render(instance.module.names, buf);
		buf.ln().out(out);
		buf.reset();
	}
	// V3-runtime callback: to advance over this frame.
	def nextFrame(ip: Pointer, sp: Pointer) -> (Pointer, Pointer) {
		sp += IVAR_FRAME.size;	 // assume frame is allocated
		ip = sp.load<Pointer>(); // return address on stack
		return (ip + -1, sp + Pointer.SIZE); // XXX: V3 quirk with -1 (use RiOs?)
	}
	// V3-runtime callback: when the garbage collector needs to scan a JIT stack frame.
	def scanFrame(ip: Pointer, sp: Pointer) {
		// Handle other roots in the frame
		RiGc.scanRoot(sp + IVAR_FRAME.WASM_FUNC.disp);
		RiGc.scanRoot(sp + IVAR_FRAME.INSTANCE.disp);
		RiGc.scanRoot(sp + IVAR_FRAME.ACCESSOR.disp);
	}
}

// Represents the JITed code for an entire module.
class X86_64SpcModuleCode extends X86_64SpcCode {
	def mapping: Mapping;
	var codeEnd: int;			// for dynamically adding code to the end
	var sourcePcs: Vector<(int, int)>;
	var embeddedRefOffsets: Vector<int>;

	new(mapping) super("spc-module", mapping.range.start, mapping.range.end) {
		RiGc.registerScanner(this, X86_64SpcModuleCode.scan);
	}

	// XXX: Keeps the whole-program optimizer from deleting fields needed for rooting the range.
	def keepAlive() {
		if (mapping == null) System.error(null, null);
		if (mapping.range == null) System.error(null, null);
	}
	// V3-runtime callback: handle an OS-level signal that occurred while {ip} was in JIT code.
	def handleSignal(signum: int, siginfo: Pointer, ucontext: Pointer, ip: Pointer, sp: Pointer) -> bool {
		if (Debug.runtime) {
			Trace.OUT.put2("  !signal %d in SPC code @ 0x%x", signum, RiOs.getIp(ucontext) - Pointer.NULL).outln();
		}
		match (signum) {
			SIGFPE => {
				// presume divide/modulus by zero
				updateUContextToTrapsStub(ucontext, TrapReason.DIV_BY_ZERO);
				return true;
			}
			SIGBUS, SIGSEGV => {
				var addr = RiOs.getAccessAddress(siginfo, ucontext);
				var reason: TrapReason;
				if (RedZones.isInRedZone(addr)) {
					reason = TrapReason.STACK_OVERFLOW;
				} else {
					reason = TrapReason.MEM_OUT_OF_BOUNDS;
				}
				updateUContextToTrapsStub(ucontext, reason);
				return true;
			}
		}
		return false;
	}
	// Updates the siginfo's {ucontext} to set the handler %rip and to write the PC of the fault location
	// into the stack frame for the handler.
	private def updateUContextToTrapsStub(ucontext: Pointer, reason: TrapReason) {
		var p_rip = ucontext + ucontext_rip_offset;
		var p_rsp = RiOs.getSp(ucontext);
		if (!RiRuntime.inStackRedZone(p_rsp)) {
			// Update the current PC in the JIT frame, if it is accessible.
			var ip = p_rip.load<Pointer>();
			var pc = lookupPc(ip, false);
			(p_rsp + IVAR_FRAME.CURPC.disp).store<int>(pc);
		}

		var stubs = X86_64PreGenStubs.getSpcTrapsStub();
		var handler_ip = stubs.getIpForReason(reason);
		(p_rip).store<Pointer>(handler_ip);
	}
	// Look up the source {pc} of a location {i} in this code. Returns {-1} if no exact entry is found.
	// Return addresses are treated differently than other addresses in the code.
	def lookupPc(ip: Pointer, isRetAddr: bool) -> int {
		if (Trace.compiler) Trace.OUT.put2("SpcCode.lookupPc(0x%x, ret=%z)", (ip - Pointer.NULL), isRetAddr).outln();
		if (sourcePcs == null) return -1;
		if (!mapping.range.contains(ip)) return -1;
		var offset = ip - mapping.range.start - if(isRetAddr, 1);
		// XXX: use binary search for looking up source PCs in SPC code
		for (i < sourcePcs.length) {
			var entry = sourcePcs[i];
			if (Trace.compiler) Trace.OUT.put2("  (offset=%d, pc=%d)", entry.0, entry.1).outln();
			if (offset == entry.0) return entry.1;
		}
		return -1;
	}
	// Appends code to the end of this module.
	def appendCode(masm: X86_64MacroAssembler) -> Pointer {
		var range = mapping.range;
		var startOffset = codeEnd;
		var entrypoint = range.start + startOffset;
		masm.setTargetAddress(u64.view(entrypoint - Pointer.NULL));
		codeEnd = Target.copyInto(range, startOffset, masm.w);
		if (masm.source_locs != null) {
			if (sourcePcs == null) sourcePcs = Vector.new();
			for (i < masm.source_locs.length) {
				var entry = masm.source_locs[i];
				sourcePcs.put(entry.0 + startOffset, entry.1);
			}
		}
		if (masm.embeddedRefOffsets != null) {
			if (embeddedRefOffsets == null) embeddedRefOffsets = Vector.new();
			for (i < masm.embeddedRefOffsets.length) {
				embeddedRefOffsets.put(masm.embeddedRefOffsets[i] + startOffset);
			}
		}
		return entrypoint;
	}
	// Callback for the GC to custom-scan roots in the module code, e.g. embedded object
	// constants like probe objects.
	private def scan() {
		if (embeddedRefOffsets == null) return;
		var codeStart = mapping.range.start;
		for (i < embeddedRefOffsets.length) {
			var ref_loc = codeStart + embeddedRefOffsets[i];
			RiGc.scanRoot(ref_loc);
		}
	}
}

// Represents (shared) code that calls the runtime to generate a trap.
// This is jumped to conditionally by SPC code or via the signal handler for the
// {X86_64SpcModuleCode}.
class X86_64SpcTrapsStub extends X86_64SpcCode {
	def HANDLER_SIZE = 32;

	new(start: Pointer, end: Pointer) super("spc-trap-stubs", start, end) { }

	// Get the program counter in the frame during trapping.
	def getPcFromFrame(ip: Pointer, sp: Pointer) -> int {
		return (sp + IVAR_FRAME.CURPC.disp).load<int>();
	}
	// Get the instruction pointer for the stub that will call the runtime for the given reason.
	def getIpForReason(reason: TrapReason) -> Pointer {
		return start + reason.tag * HANDLER_SIZE;
	}
}

// The lazy-compile stub needs special handling in the Virgil runtime because it has
// a frame that stores the function being compiled.
class X86_64SpcCompileStub extends RiUserCode {
	def stubName: string;
	def frameSize = Pointer.SIZE;

	new(stubName, start: Pointer, end: Pointer) super(start, end) { }

	// Called from V3 runtime upon fatal errors to describe a frame for a stacktrace.
	def describeFrame(ip: Pointer, sp: Pointer, out: (Array<byte>, int, int) -> ()) {
		var msg = "\tin [spc-";
		out(msg, 0, msg.length);
		out(stubName, 0, stubName.length);
		msg = "-compile-stub] ";
		out(msg, 0, msg.length);
		var wf = (sp + 0).load<WasmFunction>();
		// TODO: lazy parse of names section may allocate; must avoid this in OOM situation
		var buf = globalBuf;
		wf.decl.render(wf.instance.module.names, buf);
		buf.ln().out(out);
		buf.reset();
	}
	// Called from V3 runtime for a frame where {ip} is in the stub code.
	def nextFrame(ip: Pointer, sp: Pointer) -> (Pointer, Pointer) {
		sp += frameSize;	 // assume frame is allocated
		ip = sp.load<Pointer>(); // return address on stack
		return (ip + -1, sp + Pointer.SIZE); // XXX: V3 quirk with -1 (use RiOs?)
	}
	// Called from V3 runtime when the garbage collector needs to scan a JIT stack frame.
	def scanFrame(ip: Pointer, sp: Pointer) {
		// The WasmFunction is stored in the frame for debugging.
		RiGc.scanRoot(sp + 0);
	}
}

type SpcResultForStub(wf: WasmFunction, entrypoint: Pointer, thrown: Throwable) #unboxed { }

// Global functionality associated with the single-pass compiler for X86-64.
component X86_64Spc {
	// Generate the SPC entry stub.
	def genEntryStub(w: DataWriter) {
		var masm = X86_64MacroAssembler.new(w, X86_64MasmRegs.CONFIG);
		var asm = X86_64Assembler.!(masm.asm);
		var regs = X86_64MasmRegs.SPC_EXEC_ENV;
		var func_arg = G(regs.func_arg);
		if (func_arg == Target.V3_PARAM_GPRS[2]) {
			// XXX: use macro assembler to order moves
			var scratch = masm.scratch;
			asm.movq_r_r(scratch, func_arg);
			asm.movq_r_r(func_arg, Target.V3_PARAM_GPRS[1]);	// function
			asm.movq_r_r(G(regs.vsp), scratch);			// vsp
		} else {
			asm.movq_r_r(func_arg, Target.V3_PARAM_GPRS[1]);	// function
			asm.movq_r_r(G(regs.vsp), Target.V3_PARAM_GPRS[2]);	// vsp
		}
		asm.ijmp_r(Target.V3_PARAM_GPRS[3]);				// tail-call entrypoint
		asm.invalid();
	}
	def genLazyCompileStub(w: DataWriter) {
		var masm = X86_64MacroAssembler.new(w, X86_64MasmRegs.CONFIG);
		var asm = X86_64Assembler.!(masm.asm);
		var regs = X86_64MasmRegs.SPC_EXEC_ENV;
		var func_arg = G(regs.func_arg);
		asm.pushq_r(G(regs.func_arg));				// push function onto stack
		masm.emit_store_runtime_vsp(regs.vsp);
		asm.movq_r_r(Target.V3_PARAM_GPRS[1], G(regs.func_arg));	// function
		// Load {null} for the receiver.
		asm.movq_r_i(Target.V3_PARAM_GPRS[0], 0);
		// Call {X86_64Spc.lazyCompile} directly.
		masm.emit_call_abs(codePointer(X86_64Spc.lazyCompile));
		asm.q.add_r_i(R.RSP, Pointer.SIZE);				// pop function off stack
		// Check for non-null abrupt return.
		var unwind = X86_64Label.new();
		asm.q.cmp_r_i(Target.V3_RET_GPRS[2], 0);
		asm.jc_rel_near(C.NZ, unwind);
		// Tail-call the result of the compile.
		var scratch = X86_64Regs.R15;
		asm.movq_r_r(scratch, Target.V3_RET_GPRS[1]);			// entrypoint
		asm.movq_r_r(G(regs.func_arg), Target.V3_RET_GPRS[0]);	// function
		masm.emit_load_runtime_vsp(regs.vsp);
		asm.ijmp_r(scratch);						// jump to entrypoint
		asm.invalid();
		// Simply return the {Throwable} object.
		asm.bind(unwind);
		asm.movq_r_r(Target.V3_RET_GPRS[0], Target.V3_RET_GPRS[2]);
		asm.ret();
	}
	def genTierUpCompileStub(w: DataWriter, ic: X86_64InterpreterCode) {
		var masm = X86_64MacroAssembler.new(w, X86_64MasmRegs.CONFIG);
		var asm = X86_64Assembler.!(masm.asm);
		var regs = X86_64MasmRegs.SPC_EXEC_ENV;
		var func_arg = G(regs.func_arg);
		// Decrement execution counter by 1 and compile if <= 0
		var scratch = X86_64Regs.R15; // XXX: regs.func_arg == scratch!
		var offsets = V3Offsets.new();
		asm.movq_r_m(scratch, G(regs.func_arg).plus(offsets.WasmFunction_decl));
		asm.q.sub_m_i(scratch.plus(offsets.FuncDecl_tierup_trigger), FastIntTuning.entryTierUpDecrement);
		var interpreter_entry = X86_64Label.new();
		interpreter_entry.pos = ic.intSpcEntryOffset;
		asm.jc_rel_near(C.A, interpreter_entry);			// jump to interpreter if >= 0
		// Call compiler
		asm.pushq_r(G(regs.func_arg));					// push function onto stack
		masm.emit_store_runtime_vsp(regs.vsp);
		asm.movq_r_r(Target.V3_PARAM_GPRS[1], G(regs.func_arg));	// function
		// Load {null} for the receiver.
		asm.movq_r_i(Target.V3_PARAM_GPRS[0], 0);
		// Call {X86_64Spc.tierupCompile} directly.
		masm.emit_call_abs(codePointer(X86_64Spc.tierupCompile));
		asm.q.add_r_i(R.RSP, Pointer.SIZE);				// pop function off stack
		// Tail-call the result of the compile (which could be interpreter entry).
		asm.movq_r_r(scratch, Target.V3_RET_GPRS[1]);			// entrypoint
		asm.movq_r_r(G(regs.func_arg), Target.V3_RET_GPRS[0]);		// function
		masm.emit_load_runtime_vsp(regs.vsp);
		asm.ijmp_r(scratch);						// jump to entrypoint
		asm.invalid();
	}
	def genTrapsStub(w: DataWriter, stub: X86_64SpcTrapsStub) {
		var masm = X86_64MacroAssembler.new(w, X86_64MasmRegs.CONFIG);
		var asm = X86_64Assembler.!(masm.asm);
		for (reason in TrapReason) {
			var start = w.atEnd().pos;
			if (reason == TrapReason.STACK_OVERFLOW) {
				masm.emit_mov_r_trap(X86_64MasmRegs.SPC_EXEC_ENV.ret_throw, TrapReason.STACK_OVERFLOW);
			} else {
				asm.movq_r_m(Target.V3_PARAM_GPRS[1], IVAR_FRAME.WASM_FUNC);	// wasm_func
				asm.movq_r_i(Target.V3_PARAM_GPRS[2], -1);			// pc
				asm.movd_r_i(Target.V3_PARAM_GPRS[3], reason.tag);		// reason
				masm.emit_call_runtime_TRAP();
			}
			asm.q.add_r_i(R.RSP, IVAR_FRAME.size); // pop caller frame
			asm.ret();
			var skip = stub.HANDLER_SIZE - (w.atEnd().pos - start);
			if (skip < 0) System.error("TrapsStubError", "handler size too big");
			w.skipN(skip);
		}
	}

	// A handy chokepoint for entering JIT code from V3.
	def invoke(wf: WasmFunction, sp: Pointer) -> Throwable {
		return X86_64PreGenStubs.getSpcV3Entry()(wf, sp, wf.decl.target_code.spc_entry);
	}
	def setLazyCompileFor(module: Module, decl: FuncDecl) {
		if (Debug.runtime) Trace.OUT.put1("setLazyCompile %q", decl.render(module.names, _)).outln();
		decl.target_code = TargetCode(X86_64PreGenStubs.getSpcLazyCompileStub().start);
	}
	def setTierUpFor(module: Module, decl: FuncDecl) {
		if (Debug.runtime) Trace.OUT.put1("setTierUp %q", decl.render(module.names, _)).outln();
		decl.target_code = TargetCode(X86_64PreGenStubs.getSpcTierUpCompileStub().start);
		decl.tierup_trigger = SpcTuning.fastIntTierUpThreshold;
	}
	def setInterpreterFallback(decl: FuncDecl) -> Pointer {
		var addr = X86_64PreGenStubs.getSpcIntEntry();
		decl.target_code = TargetCode(addr);
		decl.tierup_trigger = int.max;
		return addr;
	}
	def estimateCodeSizeFor(decl: FuncDecl) -> int {
		return 60 + decl.orig_bytecode.length * 20; // TODO: huge overestimate
	}
	private def codePointer<P, R>(f: P -> R) -> Pointer {
		return CiRuntime.unpackClosure<X86_64Spc, P, R>(f).0;
	}
	private def lazyCompile(wf: WasmFunction) -> (WasmFunction, Pointer, Throwable) {
		// The global stub simply consults the execution strategy.
		var result = X86_64SpcStrategy.!(Execute.tiering).lazyCompile(wf);
		return (result.wf, result.entrypoint, result.thrown);
	}
	private def tierupCompile(wf: WasmFunction) -> (WasmFunction, Pointer, Throwable) {
		// The global stub simply consults the execution strategy.
		var result = X86_64SpcStrategy.!(Execute.tiering).tierupCompile(wf);
		return (result.wf, result.entrypoint, result.thrown);
	}
}
